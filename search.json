[
  {
    "objectID": "the-ds-workflow-iv.html",
    "href": "the-ds-workflow-iv.html",
    "title": "The Data Science Workflow IV",
    "section": "",
    "text": "Download the PDF version here.",
    "crumbs": [
      "Slides",
      "The Data Science Workflow IV"
    ]
  },
  {
    "objectID": "exercises/exercise-solutions-ii.html",
    "href": "exercises/exercise-solutions-ii.html",
    "title": "Exercises II",
    "section": "",
    "text": "The Data Science Workflow – Import\n\nOpen RStudio and run an R command in the console to determine your working directory. Set your working directory to your Downloads folder.\n\ngetwd()\nsetwd(\"/home/julian/Downloads\")\n\nDownload the Wine Quality data set from the UCI Machine Learning Repository. It contains physicochemical properties of red and white vinho verde wine samples, from the north of Portugal. The data set comprises two csv files, one for red and one for white wine samples. Inspect the file with a text editor to determine the correct import function and the arguments to be used. Then read both files into R using the appropriate function call and save the resulting data.frames in objects named white_wines and red_wines, respectively. Inspect the first 10 rows of the two data.frames using head and verify that the columns have the correct data types.\n\nwhite_wine_path &lt;- file.path(\"../data\", \"winequality-white.csv\")\nred_wine_path &lt;- file.path(\"../data\", \"winequality-red.csv\")\nreadLines(white_wine_path, 5)\n\n[1] \"\\\"fixed acidity\\\";\\\"volatile acidity\\\";\\\"citric acid\\\";\\\"residual sugar\\\";\\\"chlorides\\\";\\\"free sulfur dioxide\\\";\\\"total sulfur dioxide\\\";\\\"density\\\";\\\"pH\\\";\\\"sulphates\\\";\\\"alcohol\\\";\\\"quality\\\"\"\n[2] \"7;0.27;0.36;20.7;0.045;45;170;1.001;3;0.45;8.8;6\"                                                                                                                                                 \n[3] \"6.3;0.3;0.34;1.6;0.049;14;132;0.994;3.3;0.49;9.5;6\"                                                                                                                                               \n[4] \"8.1;0.28;0.4;6.9;0.05;30;97;0.9951;3.26;0.44;10.1;6\"                                                                                                                                              \n[5] \"7.2;0.23;0.32;8.5;0.058;47;186;0.9956;3.19;0.4;9.9;6\"                                                                                                                                             \n\nreadLines(red_wine_path, 5)\n\n[1] \"\\\"fixed acidity\\\";\\\"volatile acidity\\\";\\\"citric acid\\\";\\\"residual sugar\\\";\\\"chlorides\\\";\\\"free sulfur dioxide\\\";\\\"total sulfur dioxide\\\";\\\"density\\\";\\\"pH\\\";\\\"sulphates\\\";\\\"alcohol\\\";\\\"quality\\\"\"\n[2] \"7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5\"                                                                                                                                                 \n[3] \"7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5\"                                                                                                                                                 \n[4] \"7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;9.8;5\"                                                                                                                                              \n[5] \"11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58;9.8;6\"                                                                                                                                             \n\n# The files are semicolon-delimited and use a decimal point.\n# So we use read.csv2, but have to specify the decimal point:\nwhite_wines &lt;- read.csv2(white_wine_path, dec = \".\")\nred_wines &lt;- read.csv2(red_wine_path, dec = \".\")\nhead(white_wines)\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.0             0.27        0.36           20.7     0.045\n2           6.3             0.30        0.34            1.6     0.049\n3           8.1             0.28        0.40            6.9     0.050\n4           7.2             0.23        0.32            8.5     0.058\n5           7.2             0.23        0.32            8.5     0.058\n6           8.1             0.28        0.40            6.9     0.050\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  45                  170  1.0010 3.00      0.45     8.8\n2                  14                  132  0.9940 3.30      0.49     9.5\n3                  30                   97  0.9951 3.26      0.44    10.1\n4                  47                  186  0.9956 3.19      0.40     9.9\n5                  47                  186  0.9956 3.19      0.40     9.9\n6                  30                   97  0.9951 3.26      0.44    10.1\n  quality\n1       6\n2       6\n3       6\n4       6\n5       6\n6       6\n\nhead(red_wines)\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.4             0.70        0.00            1.9     0.076\n2           7.8             0.88        0.00            2.6     0.098\n3           7.8             0.76        0.04            2.3     0.092\n4          11.2             0.28        0.56            1.9     0.075\n5           7.4             0.70        0.00            1.9     0.076\n6           7.4             0.66        0.00            1.8     0.075\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  11                   34  0.9978 3.51      0.56     9.4\n2                  25                   67  0.9968 3.20      0.68     9.8\n3                  15                   54  0.9970 3.26      0.65     9.8\n4                  17                   60  0.9980 3.16      0.58     9.8\n5                  11                   34  0.9978 3.51      0.56     9.4\n6                  13                   40  0.9978 3.51      0.56     9.4\n  quality\n1       5\n2       5\n3       5\n4       6\n5       5\n6       5\n\nunlist(lapply(white_wines, typeof))\n\n       fixed.acidity     volatile.acidity          citric.acid \n            \"double\"             \"double\"             \"double\" \n      residual.sugar            chlorides  free.sulfur.dioxide \n            \"double\"             \"double\"             \"double\" \ntotal.sulfur.dioxide              density                   pH \n            \"double\"             \"double\"             \"double\" \n           sulphates              alcohol              quality \n            \"double\"             \"double\"            \"integer\" \n\nunlist(lapply(red_wines, typeof))\n\n       fixed.acidity     volatile.acidity          citric.acid \n            \"double\"             \"double\"             \"double\" \n      residual.sugar            chlorides  free.sulfur.dioxide \n            \"double\"             \"double\"             \"double\" \ntotal.sulfur.dioxide              density                   pH \n            \"double\"             \"double\"             \"double\" \n           sulphates              alcohol              quality \n            \"double\"             \"double\"            \"integer\" \n\n\nThe file native.csv contains the names and favourite foods of three individuals. Inspect the file using a text editor and read the data into R using the appropriate function. What do you observe? How can you resolve this issue? Implement the necessary adaptations to read in the data without the occurring issue.\n\nnative_path &lt;- file.path(\"../data\", \"native.csv\")\nnative &lt;- read.csv(native_path) # using read.csv as the file is comma-delimited\nnative # Clearly, there is an encoding issue...\n\n     name   favourite_food\n1 Alienor         Cr\\xeape\n2    Hans       Sp\\xe4tzle\n3  Magnus Sm\\xf8rrebr\\xf8d\n\n# Let's try to guess the encoding:\nlibrary(readr)\nas.data.frame(guess_encoding(native_path))\n\n    encoding confidence\n1 ISO-8859-1       0.22\n\n# Seems to be ISO-8859-1, let's try:\nnative &lt;- read.csv(native_path, fileEncoding = \"ISO-8859-1\")\nnative # It worked!\n\n     name favourite_food\n1 Alienor          Crêpe\n2    Hans        Spätzle\n3  Magnus     Smørrebrød\n\n# Note: The fileEncoding \"Windows-1252\" would also work.\n\nThe Excel spreadsheet datasets.xls contains four worksheets, each with a different data set. Read all four work sheets into a list, such that each list element contains the data.frame holding the data of one work sheet. Inspect the head of each of the four data.frames to verify that the data has been read in correctly. Hint: you could use the excel_sheets function of the readxl package to identify the names of the four sheets and then read them into R using the appropriate function in a for loop.\n\nlibrary(readxl)\n\nexcel_path &lt;- file.path(\"../data\", \"datasets.xls\")\nsheet_names &lt;- excel_sheets(excel_path)\nexcel_dfs &lt;- list()\n\nfor(sheet in sheet_names){\n  excel_dfs[[sheet]] &lt;- as.data.frame(read_xls(excel_path, sheet = sheet))\n}\nlapply(excel_dfs, head, n = 5)\n\n$iris\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n\n$mtcars\n   mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n$chickwts\n  weight      feed\n1    179 horsebean\n2    160 horsebean\n3    136 horsebean\n4    227 horsebean\n5    217 horsebean\n\n$quakes\n     lat   long depth mag stations\n1 -20.42 181.62   562 4.8       41\n2 -20.62 181.03   650 4.2       15\n3 -26.00 184.10    42 5.4       43\n4 -17.97 181.66   626 4.1       19\n5 -20.42 181.96   649 4.0       11\n\n\nSearch the data documentation of the Gapminder project for data on GDP per capita in constant PPP dollars (PPP$2017 or PPP$2021). Gapminder offers all of their data sets in the form of publicly available Google Sheets documents. Open the corresponding Google Sheets document for the GDP per capita data and identify the sheet ID from the URL. Find the worksheet for regions (i.e. Africa, Asia, Europe and The Americas) and read the data from that worksheet into R as a data.frame with the help of the googlesheets4 package. Inspect the first couple of rows using head and verify that the columns have the correct data types.\n\n# See here for the correct page in the Gapminder documentation:\n# https://www.gapminder.org/data/documentation/gd001/\n\nlibrary(googlesheets4)\n\ngs4_deauth()\nsheet_id_2021 &lt;- \"1RuGcXnt6mrhXumhUuRv4-lC4DmPyohznCiJy8paPM7Q\"\nsheet_id_2017 &lt;- \"1KcsWpIxQXYER9Ydoo1WWzBzCVEGNtC_vV88gaPw-3w0\"\ngdp_per_cap &lt;- as.data.frame(read_sheet(sheet_id_2021,\n                                        sheet = \"data-for-regions-by-year\",\n                                        range = \"A1:F1205\"))\n\n✔ Reading from \"GM-GDP per capita - Dataset - v31\".\n\n\n✔ Range ''data-for-regions-by-year'!A1:F1205'.\n\nhead(gdp_per_cap)\n\n     geo   name time Income per person (ppp$2021)   GDP total\n1 africa Africa 1800                     437.6993 57102830744\n2 africa Africa 1801                     437.3229 57183022380\n3 africa Africa 1802                     436.9500 57268094833\n4 africa Africa 1803                     436.5015 57347632038\n5 africa Africa 1804                     436.0338 57428983174\n6 africa Africa 1805                     434.7656 57408910599\n  GDP per capita growth (%)\n1                        NA\n2                     -0.09\n3                     -0.09\n4                     -0.10\n5                     -0.11\n6                     -0.29\n\nunlist(lapply(gdp_per_cap, typeof))\n\n                         geo                         name \n                 \"character\"                  \"character\" \n                        time Income per person (ppp$2021) \n                    \"double\"                     \"double\" \n                   GDP total    GDP per capita growth (%) \n                    \"double\"                     \"double\" \n\n\n\n\n\nThe Data Science Workflow – Tidy and Transform\n\nThe flights data set:\n\nImport the nyc13_flights.csv data set and give it the name flights.\nUse the code from the lecture to give appropriate names to the columns of flights and inspect the first 10 rows of the data.frame using head.\nRemove all rows from flights that have an NA in any column (Hint: have a look at the function na.omit). How many rows in the data.frame are lost as a consequence of this NA removal step?\nThe departure times are given in a weird format distributed over several columns in the data.frame. To make this information usable, create a column called dep_time_dt, which contains the actual departure time as a POSIXct date-time. Hint: use paste to bring together the date information from the three relevant columns. Then use the %/% and %% operators to extract hour and minute from the dep_time column and paste it to the date. Finally, use as.POSIXct and the correct time zone to turn this string into a date-time.\nFilter the flights data set to only include:\n\nflights operated by United Air Lines or US Airways (carrier codes UA and US, respectively).\nflights from LaGuardia (LGA) to Fort Lauderdale (FLL).\nflights operated by Delta Air Lines (carrier code DL) with a departure delay of more than three hours.\nflights from Newark (EWR) to Chicago (ORD) operated by Envoy Air (carrier code MQ) on 1st June 2013.\nflights with an air time of at most half an hour operated by American Airlines (carrier code AA).\nflights from LaGuardia (LGA) or JFK to Nashville (BNA) with an air time of more than 2.5 hours.\nflights that departed after 16th May 2013 (careful: be aware of the time zone!).\n\nUsing appropriate functions in R, answer the following questions:\n\nWhat is the most frequent destination airport flown to from LaGuardia (LGA)?\nWhich carrier has the most departures out of JFK? Which proportion of the total number of departures do they cover there?\nWhich flight had the longest air time out of the ones contained in the data set?\nHow often is the flight with the shortest distance flown in the time period covered by the data set?\nWhat is the average departure delay at each of the three NYC origin airports?\nWhat is the average arrival delay of flights from JFK to Los Angeles (LAX) in each month?\nWhat is the tail number of the airplane that has covered the most distance over all flights in the data set?\nWhat is the distance of the longest flight (in terms of distance) offered out of LaGuardia (LGA) for each carrier?\n\nImport the nyc13_airports.csv data set and give it the name airports.\nThe airports data set contains the FAA code, the full name, latitude and longitude and time zone information of 1458 airports in the United States. Using an inner join, join the name, lat and lon of the destination airport into the flights data set (mind the differences in key column name between flights and airports!). Record the number of rows of the flights data set before and after the join. How many rows were lost due to the inner join? How could that have been avoided?\nRename the new columns in flights to dest_name, dest_lat and dest_lon, respectively.\nImport the nyc13_planes.csv data set and give it the name planes.\nThe planes data set contains the tail number, manufacturing year, type, manufacturer, model, number and type of engines and average cruising speed of 3322 airplanes in commercial service in the United States. Using a left join, join the manufacturer and model of the plane into the flights data set. For how many flights was no plane information found in planes?\nRename the new columns in flights to plane_manufacturer and plane_model, respectively.\n\n\n# a.\nflights &lt;- read.csv(\"../data/nyc13_flights.csv\")\n\n# b.\nnames(flights) &lt;- c(\"year\", \"month\", \"day\", \"dep_time\", \"sched_dep_time\", \"dep_delay\",\n                \"arr_time\", \"sched_arr_time\", \"arr_delay\", \"carrier\", \"flight\",\n                \"tail_num\", \"origin\", \"dest\", \"air_time\", \"distance\", \"time_hour\")\nhead(flights, 10)\n\n   year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1  2013     1   1      517            515         2      830            819\n2  2013     1   1      533            529         4      850            830\n3  2013     1   1      542            540         2      923            850\n4  2013     1   1      544            545        -1     1004           1022\n5  2013     1   1      554            600        -6      812            837\n6  2013     1   1      554            558        -4      740            728\n7  2013     1   1      555            600        -5      913            854\n8  2013     1   1      557            600        -3      709            723\n9  2013     1   1      557            600        -3      838            846\n10 2013     1   1      558            600        -2      753            745\n   arr_delay carrier flight tail_num origin dest air_time distance\n1         11      UA   1545   N14228    EWR  IAH      227     1400\n2         20      UA   1714   N24211    LGA  IAH      227     1416\n3         33      AA   1141   N619AA    JFK  MIA      160     1089\n4        -18      B6    725   N804JB    JFK  BQN      183     1576\n5        -25      DL    461   N668DN    LGA  ATL      116      762\n6         12      UA   1696   N39463    EWR  ORD      150      719\n7         19      B6    507   N516JB    EWR  FLL      158     1065\n8        -14      EV   5708   N829AS    LGA  IAD       53      229\n9         -8      B6     79   N593JB    JFK  MCO      140      944\n10         8      AA    301   N3ALAA    LGA  ORD      138      733\n             time_hour\n1  2013-01-01 05:00:00\n2  2013-01-01 05:00:00\n3  2013-01-01 05:00:00\n4  2013-01-01 05:00:00\n5  2013-01-01 06:00:00\n6  2013-01-01 05:00:00\n7  2013-01-01 06:00:00\n8  2013-01-01 06:00:00\n9  2013-01-01 06:00:00\n10 2013-01-01 06:00:00\n\n# c.\nn_before &lt;- nrow(flights)\nflights &lt;- na.omit(flights)\nn_after &lt;- nrow(flights)\nsprintf(\"%d rows were lost!\", n_before - n_after)\n\n[1] \"5480 rows were lost!\"\n\n# d.\nflights$dep_time_dt &lt;- as.POSIXct(paste(flights$year, flights$month, flights$day,\n                                        flights$dep_time %/% 100,\n                                        flights$dep_time %% 100),\n                                  format = \"%Y %m %d %H %M\", tz = \"America/New_York\")\nhead(flights[,c(\"year\", \"month\", \"day\", \"dep_time\", \"dep_time_dt\")])\n\n  year month day dep_time         dep_time_dt\n1 2013     1   1      517 2013-01-01 05:17:00\n2 2013     1   1      533 2013-01-01 05:33:00\n3 2013     1   1      542 2013-01-01 05:42:00\n4 2013     1   1      544 2013-01-01 05:44:00\n5 2013     1   1      554 2013-01-01 05:54:00\n6 2013     1   1      554 2013-01-01 05:54:00\n\n# e. (results always in included in head(), to avoid too long outputs)\nhead(subset(flights, carrier == \"UA\" | carrier == \"US\"))\n\n   year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1  2013     1   1      517            515         2      830            819\n2  2013     1   1      533            529         4      850            830\n6  2013     1   1      554            558        -4      740            728\n13 2013     1   1      558            600        -2      924            917\n14 2013     1   1      558            600        -2      923            937\n17 2013     1   1      559            600        -1      854            902\n   arr_delay carrier flight tail_num origin dest air_time distance\n1         11      UA   1545   N14228    EWR  IAH      227     1400\n2         20      UA   1714   N24211    LGA  IAH      227     1416\n6         12      UA   1696   N39463    EWR  ORD      150      719\n13         7      UA    194   N29129    JFK  LAX      345     2475\n14       -14      UA   1124   N53441    EWR  SFO      361     2565\n17        -8      UA   1187   N76515    EWR  LAS      337     2227\n             time_hour         dep_time_dt\n1  2013-01-01 05:00:00 2013-01-01 05:17:00\n2  2013-01-01 05:00:00 2013-01-01 05:33:00\n6  2013-01-01 05:00:00 2013-01-01 05:54:00\n13 2013-01-01 06:00:00 2013-01-01 05:58:00\n14 2013-01-01 06:00:00 2013-01-01 05:58:00\n17 2013-01-01 06:00:00 2013-01-01 05:59:00\n\nhead(subset(flights, origin == \"LGA\" & dest == \"FLL\"))\n\n    year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n18  2013     1   1      600            600         0      851            858\n62  2013     1   1      657            700        -3      959           1013\n199 2013     1   1      931            930         1     1237           1238\n220 2013     1   1      959           1002        -3     1313           1319\n253 2013     1   1     1053           1050         3     1402           1358\n297 2013     1   1     1155           1200        -5     1507           1519\n    arr_delay carrier flight tail_num origin dest air_time distance\n18         -7      B6    371   N595JB    LGA  FLL      152     1076\n62        -14      DL   1879   N318NB    LGA  FLL      164     1076\n199        -1      B6    375   N508JB    LGA  FLL      161     1076\n220        -6      DL   2379   N965DL    LGA  FLL      151     1076\n253         4      B6    373   N520JB    LGA  FLL      165     1076\n297       -12      DL   1443   N969DL    LGA  FLL      160     1076\n              time_hour         dep_time_dt\n18  2013-01-01 06:00:00 2013-01-01 06:00:00\n62  2013-01-01 07:00:00 2013-01-01 06:57:00\n199 2013-01-01 09:00:00 2013-01-01 09:31:00\n220 2013-01-01 10:00:00 2013-01-01 09:59:00\n253 2013-01-01 10:00:00 2013-01-01 10:53:00\n297 2013-01-01 12:00:00 2013-01-01 11:55:00\n\nhead(subset(flights, carrier == \"DL\" & dep_delay &gt; 180))\n\n      year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n2599  2013     1   3     2008           1540       268     2339           1909\n3970  2013     1   5     1344            817       327     1635           1127\n11064 2013     1  13     1809            810       599     2054           1042\n12196 2013     1  14     2229           1655       334      157           2029\n19670 2013     1  23     1551            753       478     1812           1006\n20555 2013     1  24     1554           1235       199     1940           1606\n      arr_delay carrier flight tail_num origin dest air_time distance\n2599        270      DL   2027   N338NW    JFK  FLL      158     1069\n3970        308      DL   1109   N309US    LGA  TPA      158     1010\n11064       612      DL    269   N322NB    JFK  ATL      116      760\n12196       328      DL    706   N370NW    JFK  AUS      243     1521\n19670       486      DL   2119   N326NB    LGA  MSP      166     1020\n20555       214      DL   2174   N3733Z    JFK  SLC      316     1990\n                time_hour         dep_time_dt\n2599  2013-01-03 15:00:00 2013-01-03 20:08:00\n3970  2013-01-05 08:00:00 2013-01-05 13:44:00\n11064 2013-01-13 08:00:00 2013-01-13 18:09:00\n12196 2013-01-14 16:00:00 2013-01-14 22:29:00\n19670 2013-01-23 07:00:00 2013-01-23 15:51:00\n20555 2013-01-24 12:00:00 2013-01-24 15:54:00\n\nhead(subset(flights,\n            origin == \"EWR\" & dest == \"ORD\" & carrier == \"MQ\" & month == 6 & day == 1))\n\n       year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n137957 2013     6   1      624            600        24      727            720\n138032 2013     6   1      755            755         0      936            920\n138218 2013     6   1     1151           1150         1     1305           1305\n138296 2013     6   1     1343           1315        28     1450           1435\n       arr_delay carrier flight tail_num origin dest air_time distance\n137957         7      MQ   3768   N9EAMQ    EWR  ORD      104      719\n138032        16      MQ   3737   N519MQ    EWR  ORD      106      719\n138218         0      MQ   3697   N546MQ    EWR  ORD      105      719\n138296        15      MQ   3765   N504MQ    EWR  ORD      104      719\n                 time_hour         dep_time_dt\n137957 2013-06-01 06:00:00 2013-06-01 06:24:00\n138032 2013-06-01 07:00:00 2013-06-01 07:55:00\n138218 2013-06-01 11:00:00 2013-06-01 11:51:00\n138296 2013-06-01 13:00:00 2013-06-01 13:43:00\n\nhead(subset(flights,\n            air_time &lt;= 30 & carrier == \"AA\"))\n\n       year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n26433  2013     1  31     1233           1245       -12     1320           1350\n103954 2013     4  25     1241           1245        -4     1330           1350\n142896 2013     6   6     1239           1245        -6     1329           1350\n146703 2013     6  10     1558           1600        -2     1713           1720\n154175 2013     6  18     1235           1245       -10     1327           1355\n       arr_delay carrier flight tail_num origin dest air_time distance\n26433        -30      AA   1850   N3JRAA    JFK  BOS       30      187\n103954       -20      AA   1850   N3CWAA    JFK  BOS       30      187\n142896       -21      AA   1850   N3CHAA    JFK  BOS       30      187\n146703        -7      AA    854   N3EUAA    JFK  BOS       29      187\n154175       -28      AA   1850   N3EGAA    JFK  BOS       30      187\n                 time_hour         dep_time_dt\n26433  2013-01-31 12:00:00 2013-01-31 12:33:00\n103954 2013-04-25 12:00:00 2013-04-25 12:41:00\n142896 2013-06-06 12:00:00 2013-06-06 12:39:00\n146703 2013-06-10 16:00:00 2013-06-10 15:58:00\n154175 2013-06-18 12:00:00 2013-06-18 12:35:00\n\nhead(subset(flights,\n            (origin == \"LGA\" | origin == \"JFK\") & flights$dest == \"BNA\" & air_time &gt; 150))\n\n      year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n640   2013     1   1     1803           1620       103     2008           1750\n643   2013     1   1     1806           1810        -4     2002           1945\n722   2013     1   1     1934           1725       129     2126           1855\n807   2013     1   1     2122           2125        -3     2312           2250\n2519  2013     1   3     1847           1725        82     2102           1855\n26792 2013     1  31     1945           1810        95     2158           1945\n      arr_delay carrier flight tail_num origin dest air_time distance\n640         138      MQ   4622   N504MQ    LGA  BNA      154      764\n643          17      MQ   4484   N711MQ    LGA  BNA      152      764\n722         151      MQ   4255   N909MQ    JFK  BNA      154      765\n807          22      MQ   4660   N1EAMQ    LGA  BNA      153      764\n2519        127      MQ   4255   N630MQ    JFK  BNA      164      765\n26792       133      MQ   4484   N722MQ    LGA  BNA      157      764\n                time_hour         dep_time_dt\n640   2013-01-01 16:00:00 2013-01-01 18:03:00\n643   2013-01-01 18:00:00 2013-01-01 18:06:00\n722   2013-01-01 17:00:00 2013-01-01 19:34:00\n807   2013-01-01 21:00:00 2013-01-01 21:22:00\n2519  2013-01-03 17:00:00 2013-01-03 18:47:00\n26792 2013-01-31 18:00:00 2013-01-31 19:45:00\n\nhead(subset(flights,\n            dep_time_dt &gt;= as.POSIXct(\"2013-05-17 00:00:00\", tz = \"America/New_York\")))\n\n       year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n124078 2013     5  17       11           2359        12      336            345\n124079 2013     5  17      458            500        -2      642            640\n124080 2013     5  17      506            515        -9      751            811\n124081 2013     5  17      533            540        -7      829            840\n124082 2013     5  17      548            550        -2      831            845\n124083 2013     5  17      551            600        -9      823            850\n       arr_delay carrier flight tail_num origin dest air_time distance\n124078        -9      B6    701   N524JB    JFK  SJU      189     1598\n124079         2      US   1431   N196UW    EWR  CLT       83      529\n124080       -20      UA   1288   N62631    EWR  IAH      188     1400\n124081       -11      AA    701   N613AA    JFK  MIA      142     1089\n124082       -14      UA   1077   N73270    EWR  MIA      145     1085\n124083       -27      AA    707   N3FYAA    LGA  DFW      186     1389\n                 time_hour         dep_time_dt\n124078 2013-05-17 23:00:00 2013-05-17 00:11:00\n124079 2013-05-17 05:00:00 2013-05-17 04:58:00\n124080 2013-05-17 05:00:00 2013-05-17 05:06:00\n124081 2013-05-17 05:00:00 2013-05-17 05:33:00\n124082 2013-05-17 05:00:00 2013-05-17 05:48:00\n124083 2013-05-17 06:00:00 2013-05-17 05:51:00\n\n# f.\nsort(table(subset(flights, origin == \"LGA\", dest)),\n     decreasing = TRUE)[1:3] # Answer: ATL (Atlanta)\n\ndest\n ATL  ORD  MIA \n5069 3950 2739 \n\njfk_carriers &lt;- sort(table(subset(flights, origin == \"JFK\", carrier)),\n                     decreasing = TRUE)\nprop.table(jfk_carriers) # Answer: B6 (JetBlue Airways), covering 37.8% of departures\n\ncarrier\n         B6          DL          9E          AA          MQ          UA \n0.378462963 0.181851852 0.136240741 0.128537037 0.060222222 0.040444444 \n         VX          US          EV          HA \n0.033037037 0.025537037 0.012314815 0.003351852 \n\nsubset(flights, air_time == max(air_time)) # Answer: EWR - HNL on 17th March 2013.\n\n      year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n67176 2013     3  17     1337           1335         2     1937           1836\n      arr_delay carrier flight tail_num origin dest air_time distance\n67176        61      UA     15   N77066    EWR  HNL      695     4963\n                time_hour         dep_time_dt\n67176 2013-03-17 13:00:00 2013-03-17 13:37:00\n\nnrow(subset(flights, distance == min(distance))) # Answer: 48 times (EWR - PHL)\n\n[1] 48\n\ntapply(flights$dep_delay, flights$origin, mean) # Answer on avg. dep_delays\n\n     EWR      JFK      LGA \n16.86164 12.68735 10.77449 \n\njfk_la &lt;- subset(flights, origin == \"JFK\" & dest == \"LAX\", c(month, arr_delay))\ntapply(jfk_la$arr_delay, jfk_la$month, mean) # Answer on avg. arr_delay (JFK-LAX)\n\n        1         2         3         4         5         6 \n-6.396146 -8.195489 -6.568277  2.062567 -6.106583 10.857451 \n\nplane_dists &lt;- tapply(flights$distance, flights$tail_num, sum)\nplane_dists[which.max(plane_dists)] # Answer: N327AA\n\nN327AA \n461291 \n\nlga_flights &lt;- subset(flights, origin == \"LGA\", c(dest, distance, carrier))\ntapply(lga_flights$distance, lga_flights$carrier, max) # Answer on max dist out of LGA\n\n  9E   AA   B6   DL   EV   F9   FL   MQ   OO   UA   US   WN   YV \n 659 1389 1080 1620 1389 1620  762 1147  733 1620  544 1620  544 \n\n# g.\nairports &lt;- read.csv(\"../data/nyc13_airports.csv\")\n\n# h.\nn_before &lt;- nrow(flights)\nflights &lt;- merge(flights, airports[, c(\"faa\", \"name\", \"lat\", \"lon\")],\n                 by.x = \"dest\", by.y = \"faa\", all = FALSE)\nn_after &lt;- nrow(flights)\nn_before - n_after\n\n[1] 3855\n\n# 3855 rows were lost due to the inner join, i.e. for 3855 flights, we did not find\n# the destination airport in the airports data set. To avoid losing these flights,\n# we could use a left join instead, which would put NA's for the values of name,\n# lat and lon of the airports we were not able to find, instead of removing the\n# corresponding rows altogether.\n\n# i.\nairport_cols &lt;- c(\"name\", \"lat\", \"lon\")\nnames(flights)[names(flights) %in% airport_cols] &lt;- paste0(\"dest_\", airport_cols)\nhead(flights[,c(\"month\", \"day\", \"dep_time\", \"origin\", \"dest\",\n                \"dest_name\", \"dest_lat\", \"dest_lon\")])\n\n  month day dep_time origin dest                         dest_name dest_lat\n1     6  25     2015    JFK  ABQ Albuquerque International Sunport 35.04022\n2     5   6     1956    JFK  ABQ Albuquerque International Sunport 35.04022\n3     5  21     1954    JFK  ABQ Albuquerque International Sunport 35.04022\n4     5  13     2053    JFK  ABQ Albuquerque International Sunport 35.04022\n5     5  20     1959    JFK  ABQ Albuquerque International Sunport 35.04022\n6     6  22     2006    JFK  ABQ Albuquerque International Sunport 35.04022\n   dest_lon\n1 -106.6092\n2 -106.6092\n3 -106.6092\n4 -106.6092\n5 -106.6092\n6 -106.6092\n\n# j.\nplanes &lt;- read.csv(\"../data/nyc13_planes.csv\")\n\n# k.\nflights &lt;- merge(flights, planes[, c(\"tail_num\", \"manufacturer\", \"model\")],\n                 by = \"tail_num\", all.x = TRUE)\nflights &lt;- flights[order(flights$dep_time_dt), ] # Re-order by departure time\nsprintf(\"For %d flights, no plane information was found!\",\n        sum(is.na(flights$manufacturer)))\n\n[1] \"For 23400 flights, no plane information was found!\"\n\n# l.\nplane_cols &lt;- c(\"manufacturer\", \"model\")\nnames(flights)[names(flights) %in% plane_cols] &lt;- paste0(\"plane_\", plane_cols)\nhead(flights[,c(\"month\", \"day\", \"dep_time\", \"origin\", \"dest\",\n                \"tail_num\", \"plane_manufacturer\", \"plane_model\")])\n\n       month day dep_time origin dest tail_num plane_manufacturer plane_model\n12392      1   1      517    EWR  IAH   N14228             BOEING     737-824\n29840      1   1      533    LGA  IAH   N24211             BOEING     737-824\n101559     1   1      542    JFK  MIA   N619AA             BOEING     757-223\n59037      1   1      554    EWR  ORD   N39463             BOEING   737-924ER\n109996     1   1      554    LGA  ATL   N668DN             BOEING     757-232\n80413      1   1      555    EWR  FLL   N516JB   AIRBUS INDUSTRIE    A320-232\n\n\nThe UCI Adult Income data set:\n\nDownload the Adult Income data set from the UCI Machine Learning Repository. It contains socio-economic data on individuals from the 1994 US census. When you download the data from the provided link, you will receive a zip archive that contains five files. Import the file adult.data into R using read.table and give the resulting data.frame the name income. Hint: the file can be opened with any text editor to inspect the delimiter and the presence of a header.\nUsing the information provided under the link above, give appropriate names to the columns of income.\nHave a look at the head of some individual columns. What do you notice?\nAdditionally to the leading spaces, this data set uses a question mark ? to indicate missing values. Both of these things would require manual work to handle. Fortunately, the read.table function has arguments strip.white to remove leading and trailing white space and na.strings to specify strings that are to be interpreted as NA. Use these two arguments to read adult.data into R again, redefining the income object you just created. Repeat exercise b. to rename the columns appropriately. Now, inspect the columns to verify that the leading spaces are gone and that the question marks have been turned into NAs.\nVerify that the variables in the data.frame have the correct data types for the data stored in them. Decide which of the included variables should be turned into a factor and go ahead with their transformation.\nA detail on factors left out from the lecture is that there are actually two types of them: unordered and ordered. Unordered factors should be used for nominal scales, while ordered factors should be used for ordinal scales. Turn the income variable in the data set into an ordered factor (you can use as.ordered) and verify that the two levels are in the correct order by inspecting head head of that variable.\nThe variable marital_status uses three different levels to mean “married”, namely Married-AF-spouse, Married-civ-spouse and Married-spouse-absent. This distinction is not relevant for us, we want to subsume all of these levels under the category Married. To do this, redefine the factor for marital_status and use the labels argument of factor function to map all three “married”-levels to the level Married. Verify that redefining the factor worked by computing the frequency table for the different marital statuses.\nUsing appropriate functions in R, answer the following questions:\n\nWhat is the average age of individuals broken down by education level?\nWhich proportion of men are married broken down by race? (Hint: you can create two-dimensional contingency tables by passing more than one vector into table and compute row or column percentages with prop.table.)\nHow many hours per week do people with a doctorate work on average? Is that the highest average work load of all education levels?\nWhich proportion of people whose native country is the US make more than 50k?\nHow many people in the data set are black and have a masters or a doctoral degree?\n\n\n\n# a.\nincome &lt;- read.table(\"../data/adult.data\", sep = \",\")\n\n# b.\nnames(income) &lt;- c(\"age\", \"workclass\", \"fnlwgt\", \"educ\", \"educ_num\", \"marital_status\",\n                   \"occup\", \"relationship\", \"race\", \"sex\", \"cap_gain\", \"cap_loss\",\n                   \"hrs_per_week\", \"native_country\", \"income\")\n\n# c.\nhead(income$workclass)\n\n[1] \" State-gov\"        \" Self-emp-not-inc\" \" Private\"         \n[4] \" Private\"          \" Private\"          \" Private\"         \n\nhead(income$educ)\n\n[1] \" Bachelors\" \" Bachelors\" \" HS-grad\"   \" 11th\"      \" Bachelors\"\n[6] \" Masters\"  \n\nhead(income$race)\n\n[1] \" White\" \" White\" \" White\" \" Black\" \" Black\" \" White\"\n\nhead(income$age)\n\n[1] 39 50 38 53 28 37\n\n# All data in columns containing categorical data seem to have a leading space \" \".\n\n# d.\nincome &lt;- read.table(\"../data/adult.data\", sep = \",\", strip.white = TRUE, na.strings = \"?\")\nnames(income) &lt;- c(\"age\", \"workclass\", \"fnlwgt\", \"educ\", \"educ_num\", \"marital_status\",\n                   \"occup\", \"relationship\", \"race\", \"sex\", \"cap_gain\", \"cap_loss\",\n                   \"hrs_per_week\", \"native_country\", \"income\")\nhead(income$workclass)\n\n[1] \"State-gov\"        \"Self-emp-not-inc\" \"Private\"          \"Private\"         \n[5] \"Private\"          \"Private\"         \n\nhead(income$educ)\n\n[1] \"Bachelors\" \"Bachelors\" \"HS-grad\"   \"11th\"      \"Bachelors\" \"Masters\"  \n\n# No more leading white space\nsapply(income, function(x) sum(x == \"?\", na.rm = TRUE)) # No more question marks\n\n           age      workclass         fnlwgt           educ       educ_num \n             0              0              0              0              0 \nmarital_status          occup   relationship           race            sex \n             0              0              0              0              0 \n      cap_gain       cap_loss   hrs_per_week native_country         income \n             0              0              0              0              0 \n\nsapply(income, function(x) sum(is.na(x))) # But now, several NA's in some columns\n\n           age      workclass         fnlwgt           educ       educ_num \n             0           1836              0              0              0 \nmarital_status          occup   relationship           race            sex \n             0           1843              0              0              0 \n      cap_gain       cap_loss   hrs_per_week native_country         income \n             0              0              0            583              0 \n\n# e.\nsapply(income, typeof)\n\n           age      workclass         fnlwgt           educ       educ_num \n     \"integer\"    \"character\"      \"integer\"    \"character\"      \"integer\" \nmarital_status          occup   relationship           race            sex \n   \"character\"    \"character\"    \"character\"    \"character\"    \"character\" \n      cap_gain       cap_loss   hrs_per_week native_country         income \n     \"integer\"      \"integer\"      \"integer\"    \"character\"    \"character\" \n\n# Factors should be: workclass, educ, marital_status, occup, relationship, race, sex,\n# native_country and income.\nincome$workclass &lt;- factor(income$workclass)\nincome$educ &lt;- factor(income$educ)\nincome$marital_status &lt;- factor(income$marital_status)\nincome$occup &lt;- factor(income$occup)\nincome$relationship &lt;- factor(income$relationship)\nincome$race &lt;- factor(income$race)\nincome$sex &lt;- factor(income$sex)\nincome$native_country &lt;- factor(income$native_country)\nincome$income &lt;- factor(income$income)\n\n# f.\nincome$income &lt;- as.ordered(income$income)\nhead(income$income) # Correct order! Note the \"&lt;\" sign between the factor levels.\n\n[1] &lt;=50K &lt;=50K &lt;=50K &lt;=50K &lt;=50K &lt;=50K\nLevels: &lt;=50K &lt; &gt;50K\n\n# g.\nincome$marital_status &lt;- factor(income$marital_status,\n                                labels = c(\"Divorced\", \"Married\", \"Married\", \"Married\",\n                                           \"Never-married\", \"Separated\", \"Widowed\"))\ntable(income$marital_status)\n\n\n     Divorced       Married Never-married     Separated       Widowed \n         4443         15417         10683          1025           993 \n\n# h.\ntapply(income$age, income$educ, mean)\n\n        10th         11th         12th      1st-4th      5th-6th      7th-8th \n    37.42980     32.35574     32.00000     46.14286     42.88589     48.44582 \n         9th   Assoc-acdm    Assoc-voc    Bachelors    Doctorate      HS-grad \n    41.06031     37.38144     38.55355     38.90495     47.70218     38.97448 \n     Masters    Preschool  Prof-school Some-college \n    44.04991     42.76471     44.74653     35.75627 \n\nprop.table(table(income$marital_status == \"Married\", income$race), margin = 2)\n\n\n        Amer-Indian-Eskimo Asian-Pac-Islander     Black     Other     White\n  FALSE          0.5980707          0.4716073 0.7119078 0.5571956 0.5066508\n  TRUE           0.4019293          0.5283927 0.2880922 0.4428044 0.4933492\n\nmean(income$hrs_per_week[income$educ == \"Doctorate\"])\n\n[1] 46.97337\n\nsort(tapply(income$hrs_per_week, income$educ, mean), decreasing = TRUE)\n\n Prof-school    Doctorate      Masters    Bachelors    Assoc-voc      HS-grad \n    47.42535     46.97337     43.83633     42.61401     41.61071     40.57537 \n  Assoc-acdm      7th-8th      5th-6th Some-college      1st-4th          9th \n    40.50422     39.36687     38.89790     38.85228     38.25595     38.04475 \n        10th    Preschool         12th         11th \n    37.05252     36.64706     35.78060     33.92596 \n\n# No, Professors work even more hours per week on average (47.42 vs. 46.97)\n\nprop.table(table(income$income[income$native_country == \"United-States\"]))\n\n\n    &lt;=50K      &gt;50K \n0.7541652 0.2458348 \n\nnrow(subset(income, race == \"Black\" & (educ == \"Masters\" | educ == \"Doctorate\")))\n\n[1] 97",
    "crumbs": [
      "Exercise Solutions",
      "Solutions II"
    ]
  },
  {
    "objectID": "exercises/exercise-solutions-i.html",
    "href": "exercises/exercise-solutions-i.html",
    "title": "Exercises I",
    "section": "",
    "text": "The essentials of R programming\n\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers 1 through \\(n\\) is \\(n(n+1)/2\\). Define \\(n = 100\\) and then use R to compute the sum of 1 through 100 using the formula. What is the sum?\n\nn &lt;- 100\nn*(n+1)/2\n\n[1] 5050\n\n\nNow use the same formula to compute the sum of the integers from 1 through 1000.\n\nn &lt;- 1000\nn*(n+1)/2\n\n[1] 500500\n\n\nChoose any positive number and assign it to a variable named x. Then perform the following steps all in one line of R code:\n\nAdd 2 to x.\nMultiply the result by 3.\nSubtract 6 from the answer.\nDivide what you get by 3.\nCompute the logarithm in base 2 of the number you get in Step 4.\n\n\nx &lt;- 2\nlog((((x + 2) * 3) - 6)/3, base = 2)\n\n[1] 1\n\n\nNow, write a function called f that performs the five steps outlined in exercise 3 for any given input x.\n\nf &lt;- function(x) return(log((((x + 2) * 3) - 6)/3, base = 2))\nf(2)\n\n[1] 1\n\n\nAssign the value 3 to a variable named x, the value 4 to a variable named y and the value 5 to a variable named z. x, y and z form what is a called a Pythagorean Triple, i.e. they are integers that satisfy the Pythagoras Theorem \\(x^2 + y^2 = z^2\\). Verify that this is true with R.\n\nx &lt;- 3\ny &lt;- 4\nz &lt;- 5\nz^2 == x^2 + y^2\n\n[1] TRUE\n\n\nSimulate 100 flips of a fair coin. Hint: use the function sample in conjunction with a character vector indicating the possible outcomes of a coin flip (i.e. “heads” and “tails”). Call the function multiple times to see that the outcome appears random.\n\nsample(c(\"heads\", \"tails\"), 100, replace = TRUE)\n\n  [1] \"heads\" \"tails\" \"heads\" \"heads\" \"tails\" \"tails\" \"tails\" \"heads\" \"heads\"\n [10] \"heads\" \"heads\" \"heads\" \"heads\" \"heads\" \"tails\" \"heads\" \"tails\" \"tails\"\n [19] \"heads\" \"heads\" \"tails\" \"heads\" \"tails\" \"tails\" \"heads\" \"heads\" \"heads\"\n [28] \"tails\" \"heads\" \"heads\" \"tails\" \"tails\" \"heads\" \"heads\" \"heads\" \"heads\"\n [37] \"heads\" \"heads\" \"heads\" \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"heads\"\n [46] \"tails\" \"heads\" \"heads\" \"heads\" \"tails\" \"heads\" \"tails\" \"heads\" \"tails\"\n [55] \"tails\" \"heads\" \"tails\" \"heads\" \"heads\" \"tails\" \"tails\" \"tails\" \"tails\"\n [64] \"heads\" \"heads\" \"heads\" \"tails\" \"heads\" \"heads\" \"tails\" \"heads\" \"heads\"\n [73] \"tails\" \"tails\" \"tails\" \"tails\" \"tails\" \"heads\" \"tails\" \"heads\" \"tails\"\n [82] \"heads\" \"tails\" \"heads\" \"heads\" \"heads\" \"tails\" \"tails\" \"heads\" \"tails\"\n [91] \"tails\" \"tails\" \"tails\" \"heads\" \"tails\" \"heads\" \"heads\" \"tails\" \"tails\"\n[100] \"tails\"\n\n\nWhat are the data types of the following objects? If in doubt, determine them using typeof.\n\nx1 &lt;- 10\nx2 &lt;- \"10\"\nx3 &lt;- (10 &gt; 10)\nx4 &lt;- \"ten\"\nx5 &lt;- 10L\n\n\ntypeof(x1)\n\n[1] \"double\"\n\ntypeof(x2)\n\n[1] \"character\"\n\ntypeof(x3)\n\n[1] \"logical\"\n\ntypeof(x4)\n\n[1] \"character\"\n\ntypeof(x5)\n\n[1] \"integer\"\n\n\nIn the following code block, try to predict the output of each individual line. Then run the code in R to see if you were right.\n\nx &lt;- 5 &gt; 3\nis.logical(x)\n\n[1] TRUE\n\nis.logical(!x)\n\n[1] TRUE\n\n!is.logical(x)\n\n[1] FALSE\n\n!is.logical(!x)\n\n[1] FALSE\n\n\nCreate a vector called some_thing containing your first name, your age and whether or not you like ice cream (TRUE vs. FALSE). Can you predict the data type of the resulting vector? Verify using R.\n\nsome_thing &lt;- c(\"James\", 24, TRUE)\ntypeof(some_thing)\n\n[1] \"character\"\n\n\nUsing the functions seq and rep as needed, create the vectors\n0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4\nand\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\nrep(seq(0, 4), each = 5)\n\n [1] 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4\n\nrep(seq(1, 5), 5)\n\n [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\n\nUsing the functions seq and rep as needed, create the vector\n1 2 3 4 5 2 3 4 5 6 3 4 5 6 7 4 5 6 7 8 5 6 7 8 9\n\nrep(seq(0, 4), each = 5) + rep(seq(1, 5), 5)\n\n [1] 1 2 3 4 5 2 3 4 5 6 3 4 5 6 7 4 5 6 7 8 5 6 7 8 9\n\n\nWrite a function which outputs whether a given number is positive or negative.\n\nf &lt;- function(x){\n  if(x &lt; 0){\n    return(\"negative\")\n  } else if(x &gt; 0){\n    return(\"positive\")\n  } else {\n    return(\"zero\")\n  }\n}\nf(-4.2)\n\n[1] \"negative\"\n\nf(2.34)\n\n[1] \"positive\"\n\nf(0)\n\n[1] \"zero\"\n\n\nWrite a function which, for a given natural number \\(n\\), returns a sequence where each \\(i \\leq n\\) is repeated \\(i\\) times, in ascending order. For example, for \\(n = 4\\), the function should return 1 2 2 3 3 3 4 4 4 4.\n\nf &lt;- function(n){\n  return(rep(1:n, 1:n))\n}\nf(4)\n\n [1] 1 2 2 3 3 3 4 4 4 4\n\nf(8)\n\n [1] 1 2 2 3 3 3 4 4 4 4 5 5 5 5 5 6 6 6 6 6 6 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8\n\n\nWrite a function which outputs which of two given character strings is shorter. Hint: use the function nchar.\n\nselect_shorter &lt;- function(c1, c2){\n  n1 &lt;- nchar(c1)\n  n2 &lt;- nchar(c2)\n  if(n1 &lt; n2){\n    return(c1)\n  } else if(n1 &gt; n2){\n    return(c2)\n  } else {\n    return(\"Equal length\")\n  }\n}\nselect_shorter(\"Hello\", \"Hi\")\n\n[1] \"Hi\"\n\n\nWrite a function which takes three numbers as arguments and returns the sum of the squares of the two larger numbers.\n\nf &lt;- function(x, y, z){\n  nums &lt;- c(x, y, z)\n  return(sum(nums[-which.min(nums)]^2))\n}\nf(3, 4, 5)\n\n[1] 41\n\n\nWrite a function that for a given character vector, extracts all those that contain only lower-case letters, i.e. for the input c(\"a\", \"ab, \"abC\"), it should return \"a\" \"ab\". Hint: use the function tolower.\n\nf &lt;- function(x){\n  return(x[tolower(x) == x])\n}\nf(c(\"a\", \"ab\", \"abC\"))\n\n[1] \"a\"  \"ab\"\n\n\nCreate two vectors, one called heights containing the values 176, 178 and 156, and another called names containing the values “Anna”, “Jakob” and “Lisa”. Use cbind to combine these vectors into a matrix called m. What will be the data type of that matrix and why? Why is a matrix not a good data structure to store heights and names together? What would be a better alternative?\n\nheights &lt;- c(176, 178, 156)\nnames &lt;- c(\"Anna\", \"Jakob\", \"Lisa\")\nm &lt;- cbind(heights, names)\nm\n\n     heights names  \n[1,] \"176\"   \"Anna\" \n[2,] \"178\"   \"Jakob\"\n[3,] \"156\"   \"Lisa\" \n\ntypeof(m)\n\n[1] \"character\"\n\n# Character matrix due to coercion. All elements have to be of the same type.\n# Not a good data structure because we cannot perform numeric operations on\n# character strings:\nsum(m[,1])\n\nError in sum(m[, 1]): ungültiger 'type' (character) des Argumentes\n\n# Use a data.frame instead:\ndf &lt;- data.frame(heights = heights, names = names)\nsum(df$heights)\n\n[1] 510\n\n\nCreate three vectors, one called state containing the values “Vorarlberg”, “Tyrol”, “Salzburg”, “Upper Austria”, “Lower Austria”, “Carinthia”, “Styria”, “Burgenland” and “Vienna”, one called population containing the values 402, 764, 561, 1505, 1699, 565, 1253, 298 and 1932, and one called area containing the values 2602, 12648, 7155, 11983, 19180, 9537, 16399, 3965 and 415. Create a list whose elements are these three vectors. Name the list elements in accordance with the variable names. Turn this list into a data.frame called state_df.\n\nstate &lt;- c(\"Vorarlberg\", \"Tyrol\", \"Salzburg\", \"Upper Austria\", \"Lower Austria\",\n           \"Carinthia\", \"Styria\", \"Burgenland\", \"Vienna\")\npopulation &lt;- c(402, 764, 561, 1505, 1699, 565, 1253, 298, 1932)\narea &lt;- c(2602, 12648, 7155, 11983, 19180, 9537, 16399, 3965, 415)\nstate_list &lt;- list(state, population, area)\nnames(state_list) &lt;- c(\"state\", \"population\", \"area\")\nstate_df &lt;- data.frame(state_list)\nstate_df\n\n          state population  area\n1    Vorarlberg        402  2602\n2         Tyrol        764 12648\n3      Salzburg        561  7155\n4 Upper Austria       1505 11983\n5 Lower Austria       1699 19180\n6     Carinthia        565  9537\n7        Styria       1253 16399\n8    Burgenland        298  3965\n9        Vienna       1932   415\n\n\nThe data.frame from exercise 18 contains the population (in thousands) and the area (in square kilometres) of all nine Austrian federal states. Use it to perform the following tasks:\n\nObtain the population of Upper Austria using numerical indexing of the data.frame.\nObtain the area of Carinthia using logical indexing for the row and character-based indexing for the column of the data.frame.\nAdd a new column called density to the data.frame, which holds the population densities of the nine states in number of people per square kilometre.\nUse the $ operator and numerical subsetting to get the population density of Vienna.\nObtain the row of the data.frame that holds all information of the state with the lowest population density.\nUse the order function to sort the data.frame by population in descending order.\nSet the rownames of the data.frame equal to the state variable and then remove this variable from the data.frame.\n\n\n# Population of Upper Austria\nstate_df[4,2]\n\n[1] 1505\n\n# Area of Carinthia\nstate_df[state_df$state == \"Carinthia\", \"area\"]\n\n[1] 9537\n\n# Add density column\nstate_df$density &lt;- state_df$population*1000/state_df$area\n# Population density of Vienna:\nstate_df$density[9]\n\n[1] 4655.422\n\n# State with lowest population density:\nstate_df[which.min(state_df$density), ]\n\n      state population area  density\n6 Carinthia        565 9537 59.24295\n\n# Sort by population in descending order:\nstate_df &lt;- state_df[order(state_df$population, decreasing = TRUE), ]\n# Set state as rownames and then remove it:\nrownames(state_df) &lt;- state_df$state\nstate_df &lt;- state_df[,-1]\nstate_df\n\n              population  area    density\nVienna              1932   415 4655.42169\nLower Austria       1699 19180   88.58186\nUpper Austria       1505 11983  125.59459\nStyria              1253 16399   76.40710\nTyrol                764 12648   60.40481\nCarinthia            565  9537   59.24295\nSalzburg             561  7155   78.40671\nVorarlberg           402  2602  154.49654\nBurgenland           298  3965   75.15763\n\n\nUsing lapply and the data.frame from the previous exercise, find the (unweighted) average population, area and population density of the nine Austrian federal states. Hint: use the function mean.\n\nlapply(state_df, mean)\n\n$population\n[1] 997.6667\n\n$area\n[1] 9320.444\n\n$density\n[1] 597.0793\n\n\nWrite a function called compute_s_n, which, for a given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\ldots + n^2\\). Report the value of the sum for \\(n = 10\\).\n\ncompute_s_n &lt;- function(n){\n  sum((1:n)^2)\n}\ncompute_s_n(10)\n\n[1] 385\n\n\nUsing the function numeric, instantiate an empty numeric vector called s_n of length 25. Now, using a for loop and the function compute_s_n from the previous exercise, store in s_n the results of \\(S_1, S_2, \\ldots, S_{25}\\).\n\ns_n &lt;- numeric(25)\nfor(i in seq_along(s_n)){\n  s_n[i] &lt;- compute_s_n(i)\n}\ns_n\n\n [1]    1    5   14   30   55   91  140  204  285  385  506  650  819 1015 1240\n[16] 1496 1785 2109 2470 2870 3311 3795 4324 4900 5525\n\n\nRepeat exercise 22 using a while loop instead.\n\ns_n &lt;- numeric(25)\ni &lt;- 1\nwhile(i &lt;= 25){\n  s_n[i] &lt;- compute_s_n(i)\n  i &lt;- i + 1\n}\ns_n\n\n [1]    1    5   14   30   55   91  140  204  285  385  506  650  819 1015 1240\n[16] 1496 1785 2109 2470 2870 3311 3795 4324 4900 5525\n\n\nRepeat exercise 22 using lapply instead. Hint: to turn the list output of lapply into an atomic vector, have a look at the function unlist.\n\nunlist(lapply(1:25, compute_s_n))\n\n [1]    1    5   14   30   55   91  140  204  285  385  506  650  819 1015 1240\n[16] 1496 1785 2109 2470 2870 3311 3795 4324 4900 5525\n\n\nWrite a function called find_weekday, which takes as input a character string representing a date in DD.MM.YYYY format, and serves as output the weekday that this particular date was. Use it to find out on which day of the week you were born.\n\nfind_weekday &lt;- function(datestring){\n  format(as.Date(datestring, \"%d.%m.%Y\"), \"%A\")\n}\nfind_weekday(\"16.04.1998\")\n\n[1] \"Thursday\"\n\n\nInstall and load the package ggplot2.\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)",
    "crumbs": [
      "Exercise Solutions",
      "Solutions I"
    ]
  },
  {
    "objectID": "exercises/exercises-i.html",
    "href": "exercises/exercises-i.html",
    "title": "Exercises I",
    "section": "",
    "text": "The essentials of R programming\n\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers 1 through \\(n\\) is \\(n(n+1)/2\\). Define \\(n = 100\\) and then use R to compute the sum of 1 through 100 using the formula. What is the sum?\nNow use the same formula to compute the sum of the integers from 1 through 1000.\nChoose any positive number and assign it to a variable named x. Then perform the following steps all in one line of R code:\n\nAdd 2 to x.\nMultiply the result by 3.\nSubtract 6 from the answer.\nDivide what you get by 3.\nCompute the logarithm in base 2 of the number you get in Step 4.\n\nNow, write a function called f that performs the five steps outlined in exercise 3 for any given input x.\nAssign the value 3 to a variable named x, the value 4 to a variable named y and the value 5 to a variable named z. x, y and z form what is a called a Pythagorean Triple, i.e. they are integers that satisfy the Pythagoras Theorem \\(x^2 + y^2 = z^2\\). Verify that this is true with R.\nSimulate 100 flips of a fair coin. Hint: use the function sample in conjunction with a character vector indicating the possible outcomes of a coin flip (i.e. “heads” and “tails”). Call the function multiple times to see that the outcome appears random.\nWhat are the data types of the following objects? If in doubt, determine them using typeof.\n\nx1 &lt;- 10\nx2 &lt;- \"10\"\nx3 &lt;- (10 &gt; 10)\nx4 &lt;- \"ten\"\nx5 &lt;- 10L\n\nIn the following code block, try to predict the output of each individual line. Then run the code in R to see if you were right.\n\nx &lt;- 5 &gt; 3\nis.logical(x)\nis.logical(!x)\n!is.logical(x)\n!is.logical(!x)\n\nCreate a vector called some_thing containing your first name, your age and whether or not you like ice cream (TRUE vs. FALSE). Can you predict the data type of the resulting vector? Verify using R.\nUsing the functions seq and rep as needed, create the vectors\n0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4\nand\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nUsing the functions seq and rep as needed, create the vector\n1 2 3 4 5 2 3 4 5 6 3 4 5 6 7 4 5 6 7 8 5 6 7 8 9\nWrite a function which outputs whether a given number is positive or negative.\nWrite a function which, for a given natural number \\(n\\), returns a sequence where each \\(i \\leq n\\) is repeated \\(i\\) times, in ascending order. For example, for \\(n = 4\\), the function should return 1 2 2 3 3 3 4 4 4 4.\nWrite a function which outputs which of two given character strings is shorter. Hint: use the function nchar.\nWrite a function which takes three numbers as arguments and returns the sum of the squares of the two larger numbers.\nWrite a function that for a given character vector, extracts all those that contain only lower-case letters, i.e. for the input c(\"a\", \"ab, \"abC\"), it should return \"a\" \"ab\". Hint: use the function tolower.\nCreate two vectors, one called heights containing the values 176, 178 and 156, and another called names containing the values “Anna”, “Jakob” and “Lisa”. Use cbind to combine these vectors into a matrix called m. What will be the data type of that matrix and why? Why is a matrix not a good data structure to store heights and names together? What would be a better alternative?\nCreate three vectors, one called state containing the values “Vorarlberg”, “Tyrol”, “Salzburg”, “Upper Austria”, “Lower Austria”, “Carinthia”, “Styria”, “Burgenland” and “Vienna”, one called population containing the values 402, 764, 561, 1505, 1699, 565, 1253, 298 and 1932, and one called area containing the values 2602, 12648, 7155, 11983, 19180, 9537, 16399, 3965 and 415. Create a list whose elements are these three vectors. Name the list elements in accordance with the variable names. Turn this list into a data.frame called state_df.\nThe data.frame from exercise 18 contains the population (in thousands) and the area (in square kilometres) of all nine Austrian federal states. Use it to perform the following tasks:\n\nObtain the population of Upper Austria using numerical indexing of the data.frame.\nObtain the area of Carinthia using logical indexing for the row and character-based indexing for the column of the data.frame.\nAdd a new column called density to the data.frame, which holds the population densities of the nine states in number of people per square kilometre.\nUse the $ operator and numerical subsetting to get the population density of Vienna.\nObtain the row of the data.frame that holds all information of the state with the lowest population density.\nUse the order function to sort the data.frame by population in descending order.\nSet the rownames of the data.frame equal to the state variable and then remove this variable from the data.frame.\n\nUsing lapply and the data.frame from the previous exercise, find the (unweighted) average population, area and population density of the nine Austrian federal states. Hint: use the function mean.\nWrite a function called compute_s_n, which, for a given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\ldots + n^2\\). Report the value of the sum for \\(n = 10\\).\nUsing the function numeric, instantiate an empty numeric vector called s_n of length 25. Now, using a for loop and the function compute_s_n from the previous exercise, store in s_n the results of \\(S_1, S_2, \\ldots, S_{25}\\).\nRepeat exercise 22 using a while loop instead.\nRepeat exercise 22 using lapply instead. Hint: to turn the list output of lapply into an atomic vector, have a look at the function unlist.\nWrite a function called find_weekday, which takes as input a character string representing a date in DD.MM.YYYY format, and serves as output the weekday that this particular date was. Use it to find out on which day of the week you were born.\nInstall and load the package ggplot2.",
    "crumbs": [
      "Exercises",
      "Exercises I"
    ]
  },
  {
    "objectID": "exercises/exercises-iv.html",
    "href": "exercises/exercises-iv.html",
    "title": "Exercises IV",
    "section": "",
    "text": "The Data Science Workflow – Model\n\nThe ISLR2 package contains the Boston data set, which records medv (median house value) for 506 suburbs of Boston. We will seek to predict medv using 12 predictors such as rm (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940) and lstat (percent of households with low socio-economic status).\n\nInstall and load the ISLR2 package. Investigate the description of all variables in the Boston data set via ?Boston.\n\n\nWe will first do a random train-test split. Using the function sample, randomly choose 405 (i.e. approximately 80%) of the observations and assign them to a new data.frame called train. Assign all others to a data.frame called test.\n\n\nUsing the training data set, estimate a simple linear regression model with medv as the response and lstat as the predictor. Inspect the created lm object using summary and interpret regression coefficients, their statistical significance and the \\(R^2\\) statistic. Furthermore determine a 95%-confidence interval for the slope coefficient \\(\\beta_1\\).\n\n\nVisualize the simple linear regression model you just fitted on the training data using a scatter plot. Hint: simply use geom_smooth with the method equal to lm and se set to FALSE to add the regression line into the scatter plot. Does the linear fit appear appropriate for the data at hand?\n\n\nNow, estimate a new model that uses the logarithm of lstat as an explanatory variable. Inspect its \\(R^2\\) and compare to the one of the simple model that is linear in lstat. Visualize the corresponding regression line in another scatter plot. Hint: for the visualization of the non-linear regression line, use geom_function. This requires you to specify a function to be drawn into the scatter plot, for which you can use predict.\n\n\nWe now want to compare the performance of different linear regression models using 10-fold cross validation. For this, first assign each training observation randomly to a fold between 1 and 10. Next, using these folds, determine the test MSE for each of the following linear regression models.\n\nM1: medv ~ lstat\nM2: medv ~ log(lstat)\nM3: medv ~ log(lstat) + crim\nM4: medv ~ log(lstat) + crim + dis\nM5: medv ~ log(lstat) + dis + rm\nM6: medv ~ log(lstat) + dis + ptratio\nM7: medv ~ lstat + dis + ptratio + crim + nox\nM8: medv ~ log(lstat) + dis + ptratio + crim + nox\n\n\n\nWhich of the models investigated in f. gives the lowest test MSE? Fit the corresponding model to the entire training data and investigate the model more deeply using summary.\n\n\nUsing your cross-validation setup from f., compare the test MSE performance of the best-performing linear regression model now to the performance of KNN regression models that uses the same variables, i.e. lstat, dis, ptratio, crim and nox. Let \\(K\\) vary between 2 and 10 and only compare the best-performing KNN approach to model M8. Is the non-parametric approach able to outperform the parametric one?\n\n\nUsing all variables available in the data set, can you find an even better model? Fit your model, the model M8 and the KNN regression model with \\(K = 5\\) from before on the entire train data set. Using these fits, compare the performance of the three models on the initial hold-out test data set from b. Which of the three approaches has the lowest MSE on that data set?\n\n\nNow, fit a regression tree that uses predictors lstat, dis, ptratio, crim, nox, rm, tax and rad to the entire training data. Make sure to set the complexity parameter cp to zero in the fitting process. Visualize the resulting tree using corresponding functionality from the rpart.plot package.\n\n\nUsing the created rpart object, determine the optimal complexity parameter cp and create an optimally pruned tree. Finally, use both the unpruned and the optimally pruned tree to predict the response in the initial hold-out test data set from b. How do their MSEs compare to the models investigated in i.? Hint: Prediction with decision trees from rpart works just like it does for linear regression, so that you can use the predict function.\n\nThe ISLR2 package also contains the Default data set, which contains information on ten thousand credit card customers. More specifically, the data.frame holds the following four variables: default (a factor with levels No and Yes indicating whether the customer defaulted on their debt), student (a factor indicating whether the customer is a student), balance (average outstanding balance at the end of the month), income (income of the customer in USD).\n\nStart by appropriately visualizing the relationship between default (as the response) and each of the other three variables (as the predictors). Also create a scatter plot of balance against income, where the default cases are highlighted via colour and shape.\n\n\nSince the data set is quite big (\\(n = 10000\\)), we can afford to again set some observations aside as a test set for final evaluation. Using the function sample, randomly assign 90% of the observations to a new data.frame called train and assign all others to a data.frame called test.\n\n\nWrite a function called binary_class_metrics that takes two factor arguments y_pred and y_true, both with two levels. The first represents the predicted classes and the second represents the true classes. The function is then supposed to return a named vector holding the accuracy, recall, precision and f1 score of the binary classification of y_true using y_pred. Test your function for the case where the predictions are 1,1,1,0,0,0 and the truth is 1,1,0,1,1,0. Hint: first compute the confusion matrix by using the function table and then compute the four metrics from that table.\n\n\nOn the entire training data, estimate a logistic regression model for the default variable, using the other three variables as predictors. Inspect the model and interpret the coefficients. Which of them are significantly different from zero at the 5% significance level?\n\n\nEmploying your function binary_class_metrics from earlier, investigate the training set performance of using the logistic regression model from d. as a classifier. Interpret the values of accuracy, recall and precision. Hint: with the fitted function, you can get the estimated default probabilities from your glm object. Be careful to set the factor levels to No and Yes to comply with the default factor in the training data.\n\n\nIn the Default data set, we have one categorical predictor, namely whether the customer is a student or not. While logistic regression was easily able to handle that, this is more difficult for KNN classification, as one must decide how to measure distance in a feature space that combines categorical and numerical features. As we have a lot of data, we will follow a simple approach: fitting a separate KNN classifier for students and non-students, both with the same value for \\(K\\). Use 10-fold cross validation on the training data to find the value for \\(K\\) that maximizes the F1 score. Hint: in every iteration over the 10 folds, fit and predict the students and non-students separately using the two numeric features as predictors. Remember to scale the numeric features in every iteration.\n\n\nNext, fit a classification tree for default using the other three variables as predictors on the entire training data. Make sure to set the complexity parameter cp to zero in the fitting process. Visualize the resulting tree using corresponding functionality from the rpart.plot package.\n\n\nUsing the fitted classification tree from g., determine the optimal complexity parameter cp and fit and visualize the optimally pruned classification tree.\n\n\nFinally, use the logistic regression model from d., the combined KNN classifier with optimal \\(K\\) from f., the full classification tree from g. and the pruned classification tree from h. to predict the test observations in the initial hold-out sample from b. Feed the predictions of each into your function binary_class_metrics to evaluate their test set performances. Beyond just the metrics, consider the business implications of the different types of errors each model makes:\n\nFalse Positive (Predicting default when the customer will not): What are the potential costs to the credit card company?\nFalse Negative (Predicting no default when the customer will default): What are the potential costs to the credit card company?\nBased on these costs and the performance metrics, which model do you think the credit card company might prefer and why? Present the performance metrics in a table and follow it with your discussion.\n\n\nIn the course materials, you should find the advertising data set discussed extensively in the lecture. Using 5-fold cross validation, we compared the performance of a linear regression and a KNN regression in class. A linear regression using all three advertising budgets as predictors had an estimated test MSE of 3.06, while a KNN regression with the same predictors and \\(K=2\\), even had an estimated test MSE of 1.94. The goal is now to additionally examine the performance of a regression tree on that data set.\n\nWe will start with quite an advanced exercise that manually determines the first split in the tree. This is meant to build deeper understanding of what goes on behind the scenes of rpart, but is not strictly necessary, so you can skip ahead to c. if you wish to. The goal here is to write a function that takes two arguments: a vector of training observations of a quantitative feature and a vector of training observations of a quantitative response. The function should return the optimal split point \\(s\\) and the resulting \\(RSS\\) for that variable. Hint: first determine all possible split points of the predictor. Then cycle through the split points, at each point determining the left \\(RSS\\), the right \\(RSS\\) and then the sum. Save the split points and total \\(RSS\\) in a matrix and return the row of the matrix for which the total \\(RSS\\) is minimized. You may want to write a simple helper function to compute \\(RSS\\).\n\n\nNow, import the advertising data set and select only the columns 3 to 6 (we do not need the rest). Use your function from a. to determine the optimal split for social_media, for radio and for newspaper, when using sales as a response variable. Which of those three predictors and split points is able to generate the lowest \\(RSS\\)? This will be our first split.\n\n\nIf you have not done it so far, import the advertising data set and select only the columns 3 to 6 (we do not need the rest). Employ the rpart package to fit a regression tree for sales, using the other three variables as predictors and setting the complexity parameter cp to zero for the fitting process. Visualize the resulting tree. If you did exercises a. and b., compare the first split (at the very top) to the one you determined in exercise b. Did you come to the same result as the package?\n\n\nUsing the fitted regression tree from c., determine the optimal complexity parameter cp and fit and visualize the optimally pruned regression tree for this data set.\n\n\nFinally, determine the test MSE of the optimally pruned tree in 5-fold cross validation. To do this, simply set the complexity parameter cp to the optimal value you found in d. already during fitting in each loop cycle of the cross validation. How does the performance of the optimally pruned regression tree compare to the linear regression and KNN model from the lecture?\n\nWe already know the penguins data set of the palmerpenguins package from the visualization part of this course. We will now try to build classification models for determining the species of penguin based on certain physical characteristics.\n\nLoad the data set and create a relative frequency table of the different penguin species in the entire data set. What is a crucial difference of this classification task compared to the previous one on defaults of credit card customers in exercise 2.?\n\n\nIn the usual way, perform a random 80-20 train-test split. Make sure to remove any NA’s in the data set before you do so.\n\n\nOut of the classification methods that we have learned, the ones that can easily be applied to multi-class classification problems are KNN classification and decision trees. We will start with the former: run a 5-fold cross-validation experiment to determine the accuracy-maximizing value of \\(K\\) for two different KNN classifiers: one that only uses predictors bill_length_mm and bill_depth_mm and one that additionally uses predictors flipper_length_mm and body_mass_g to predict species. Make sure you scale the predictors in every iteration over the 5 folds. Hint: it might be helpful to write a little helper function that – given a vector of class predictions and a vector of true classes – computes the accuracy, i.e. the proportion of cases, where predicted class is equal to true class.\n\n\nNow, we will look at a classification tree for this data set. Fit a full classification tree for species based on predictors bill_length_mm, bill_depth_mm, flipper_length_mm and body_mass_g to the training data in the same way you would normally do for a binary classification problem. Determine the optimal complexity parameter cp and use it to also fit an optimally pruned tree. Visualize the outcome. What do you notice about the visualized tree compared to the binary classification case?\n\n\nUse the best-performing KNN model from c. as well as both the full and the optimally pruned tree from d. to predict the species of the penguins in the hold-out test data you created in b. Which of the three models gives the highest test accuracy in predicting the penguin species?\n\nThe Seeds data set (obtainable from here) contains measurements of geometrical properties of kernels belonging to three different varieties of wheat: Kama, Rosa, and Canadian. Although the target class (variety) is known, we will approach this task as an unsupervised learning problem.\n\nDownload the data set from the given link to the UCI Machine Learning Repository. Note that the data set does not include any column names, so we have to set them manually. From left to right, the columns contain the following measurements: area \\(A\\), perimeter \\(P\\), compactness \\(C = 4\\pi \\cdot A/P^2\\), length of kernel, width of kernel, asymmetry coefficient, length of kernel groove and wheat variety (1 for Kama, 2 for Rosa or 3 for Canadian). Name the columns of the data.frame accordingly and transform the wheat variety column into a factor with the labels argument set appropriately.\n\n\nVisualize the following bivariate relationships in a scatter plot: perimeter vs. groove_length, kernel_width vs. groove_length and kernel_length vs. groove_length. Do you see any clearly identifiable groups emerging from these bivariate plots?\n\n\nCreate a new data.frame called seeds_scaled, which contains all variables except for variety and all of them in their scaled version, i.e. with their mean removed and divided by their standard deviation.\n\n\nOn the entire seeds_scaled data set, run K-means clustering with different values of \\(k\\) (e.g. from 1 to 10). In each run, extract the within-cluster sum of squares and store the results in a vector named withinss. Hint: the within-cluster sum of squares can be obtained under the name tot.withinss from the fitted kmeans object.\n\n\nPlot the number of clusters \\(k\\) (on the x-axis) against the values of withinss (on the y-axis). Where is the “elbow” in that plot and which value of \\(k\\) do you deem most appropriate as a consequence?\n\n\nRun K-means clustering again, this time with the optimal \\(k\\) you chose in e. Add the cluster assignment vector (as a factor) to your original data.frame and highlight the identified clusters by colour in the bivariate scatter plots from b. Do they match with your expectations regarding the groups you visually identified in b.?\n\n\nFinally, for each cluster, determine which wheat variety appears in it most frequently and label the cluster according to that variety. Then create a cross-tabulation of that label with the actual wheat varieties. How well was the K-means algorithm able to identify wheat varieties without having any information about these classes?",
    "crumbs": [
      "Exercises",
      "Exercises IV"
    ]
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-a-sexy-profession-in-2012",
    "href": "slides/01_Introduction_to_DS.html#data-science-a-sexy-profession-in-2012",
    "title": "Data Science and Data Analytics",
    "section": "Data Science – A sexy profession in 2012?",
    "text": "Data Science – A sexy profession in 2012?\n\n\nSource: Harvard Business Review in October 2012"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-a-sexy-profession-today",
    "href": "slides/01_Introduction_to_DS.html#data-science-a-sexy-profession-today",
    "title": "Data Science and Data Analytics",
    "section": "Data Science – A sexy profession today?",
    "text": "Data Science – A sexy profession today?\n\n\nSource: Harvard Business Review in July 2022"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#what-is-data-science-1",
    "href": "slides/01_Introduction_to_DS.html#what-is-data-science-1",
    "title": "Data Science and Data Analytics",
    "section": "What is Data Science?",
    "text": "What is Data Science?"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#what-is-data-science-2",
    "href": "slides/01_Introduction_to_DS.html#what-is-data-science-2",
    "title": "Data Science and Data Analytics",
    "section": "What is Data Science?",
    "text": "What is Data Science?\nData science is an interdisciplinary academic field that combines statistics, mathematics and computing with specific subject matter expertise to uncover actionable insights hidden in an organization’s data."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#a-brief-history-of-data-science",
    "href": "slides/01_Introduction_to_DS.html#a-brief-history-of-data-science",
    "title": "Data Science and Data Analytics",
    "section": "A brief history of Data Science",
    "text": "A brief history of Data Science\n\n\n1962: Statistician John Tukey describes a field he calls data analysis, similar to what we now understand to be data science.\n1974: Computer scientist Peter Naur proposes the term data science, but in a different sense to today, namely as an alternative name for computer science.\n1992: Conference participants at the University of Montpellier II acknowledge the emergence of a new, inherently interdisciplinary field called data science focused on gaining insight from very diverse types of data.\n2001: William S. Cleveland introduces data science as its own discipline in his article Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.\n2003: Columbia University starts publishing The Journal of Data Science.\n2012: With the HBR article on data scientist as the “sexiest job of the 21st century”, the term finds its way from the scientific realm into the mainstream."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#a-brief-history-of-data-science-1",
    "href": "slides/01_Introduction_to_DS.html#a-brief-history-of-data-science-1",
    "title": "Data Science and Data Analytics",
    "section": "A brief history of Data Science",
    "text": "A brief history of Data Science"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-and-artificial-intelligence-ai",
    "href": "slides/01_Introduction_to_DS.html#data-science-and-artificial-intelligence-ai",
    "title": "Data Science and Data Analytics",
    "section": "Data Science and Artificial Intelligence (AI)",
    "text": "Data Science and Artificial Intelligence (AI)\nSince the arrival of ChatGPT, everyone is talking about AI…\n\nIs Data Science = AI? How are the two related?"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-and-artificial-intelligence-ai-1",
    "href": "slides/01_Introduction_to_DS.html#data-science-and-artificial-intelligence-ai-1",
    "title": "Data Science and Data Analytics",
    "section": "Data Science and Artificial Intelligence (AI)",
    "text": "Data Science and Artificial Intelligence (AI)\nTo answer these questions, we first need some further definitions:\n\n\n\nArtificial Intelligence (AI) is the broad field of developing machines that can replicate human behaviour, including tasks related to perception, reasoning, learning and problem-solving.\nOne way of achieving this is through Machine Learning (ML) algorithms that detect patterns in large data sets and learn to make predictions from them.\nDeep Learning (DL) is an advanced branch of ML based on so-called neural networks. Innovations in this field are responsible for recent AI breakthroughs, such as ChatGPT."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-at-the-heart-of-ai",
    "href": "slides/01_Introduction_to_DS.html#data-science-at-the-heart-of-ai",
    "title": "Data Science and Data Analytics",
    "section": "Data Science at the heart of AI",
    "text": "Data Science at the heart of AI\n\n\n\nWith all of these fields, data science has significant overlap, but nonetheless, constitutes its own discipline.\nAnalogously, there are branches of AI (e.g. robotics or symbolic reasoning) that do not revolve around learning patterns from various data sources.\nMany of the advances in AI require precisely the interdisciplinary combination of computer science, maths, statistics and domain expertise that data science provides.\nAnd yet, a data scientist does much more than develop and apply algorithms for AI…"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#so-what-does-a-data-scientist-do",
    "href": "slides/01_Introduction_to_DS.html#so-what-does-a-data-scientist-do",
    "title": "Data Science and Data Analytics",
    "section": "So what does a Data Scientist do?",
    "text": "So what does a Data Scientist do?\n\n\n\nNow that we know roughly, what data science is, what does this discipline actually entail more concretely?\nIn other words, what is it that a data scientist does every day?\nThere are many ways to structure the data science, let’s first look at the seven stages of a data science project."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem",
    "href": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem",
    "title": "Data Science and Data Analytics",
    "section": "Step 1 – Identify the problem",
    "text": "Step 1 – Identify the problem"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-key-aspects",
    "href": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-key-aspects",
    "title": "Data Science and Data Analytics",
    "section": "Step 1 – Identify the problem – Key aspects",
    "text": "Step 1 – Identify the problem – Key aspects\n\n\nUnderstand the business context:\n\nFamiliarize yourself with the industry, market trends, and internal operations.\nExample: As a data scientist in a struggling e-commerce company, study industry benchmarks and analyze internal sales, user behavior, and engagement data.\n\nDefine the problem statement:\n\nTogether with the business, clearly articulate what issue needs solving.\nExample: “Our customer churn rate has increased by 20% over the last six months, impacting revenue.”\n\nIdentify stakeholders:\n\nDetermine who is impacted and who can provide insights.\nExample: marketing to explain and refine their existing customer engagement strategies or customer service to point out existing issues in post-purchase support."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-key-aspects-1",
    "href": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-key-aspects-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 1 – Identify the problem – Key aspects",
    "text": "Step 1 – Identify the problem – Key aspects\n\n\nOutline goals & objectives:\n\nIn collaboration with business, specify what success looks like and define measurable KPIs.\nExample: “Reduce the churn rate by 10% within the next six months.”\n\nMake the goals actionable:\n\nIdentify potential strategies to achieve the goals as well as the data required to implement them.\nExample: train a machine learning model to predict customer churn and use this model to tailor new customer engagement strategies. Requires data on past customer behaviour on the website and whether or not they churned."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-ds-contributions",
    "href": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-ds-contributions",
    "title": "Data Science and Data Analytics",
    "section": "Step 1 – Identify the problem – DS contributions",
    "text": "Step 1 – Identify the problem – DS contributions"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-2-identify-available-data-sources",
    "href": "slides/01_Introduction_to_DS.html#step-2-identify-available-data-sources",
    "title": "Data Science and Data Analytics",
    "section": "Step 2 – Identify available data sources",
    "text": "Step 2 – Identify available data sources"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-2-identify-available-data-sources-1",
    "href": "slides/01_Introduction_to_DS.html#step-2-identify-available-data-sources-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 2 – Identify available data sources",
    "text": "Step 2 – Identify available data sources\n\nDefinition: A data source refers to the physical or digital system where data is generated, stored and managed as a data table, data object or any other storage format. It is also where, stakeholders of this data (e.g. a data scientist) can access data for further use.\nWhy identify data sources early?\n\nFoundation for analysis: Data sources determine the quality, scope, and reliability of your insights.\nStrategic planning: Knowing where data comes from helps define project goals and design the analytics pipeline.\nIntegration & innovation: Combining diverse sources (internal and external) can reveal hidden trends and create competitive advantages."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#classifying-data-sources",
    "href": "slides/01_Introduction_to_DS.html#classifying-data-sources",
    "title": "Data Science and Data Analytics",
    "section": "Classifying data sources",
    "text": "Classifying data sources\n\n\nInternal sources\n\nDefinition: Data generated, stored, and maintained within the organization.\nExamples:\n\nEnterprise systems (CRM, ERP)\nIn-house databases\nSpreadsheets, documents, …\n\nKey point: Proprietary data that reflects your organization’s operations.\n\n\nExternal sources\n\nDefinition: Data sourced from outside the organization.\nExamples:\n\nPublic datasets (government statistics, market research)\nAPIs (social media, financial data feeds)\n\nKey point: Supplement internal data and provide broader market or industry insights."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-crm-system",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-crm-system",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – CRM system",
    "text": "Internal data sources – CRM system\nA Customer Relationship Management (CRM) system helps manage customer data. It helps businesses keep customer contact details up to date, track every customer interaction, and manage customer accounts."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-erp-system",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-erp-system",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – ERP system",
    "text": "Internal data sources – ERP system\nThe CRM typically forms part of the Enterprise Resource Planning (ERP) system. This type of software integrates data about all main business processes in a single system."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-databases",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-databases",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Databases",
    "text": "Internal data sources – Databases\n\n\n\nA database is an organized collection of data stored and accessed electronically.\nDatabases are managed using specialized software called a Database Management System (DBMS), which allows users to store, retrieve, and manipulate data efficiently.\nDatabases are the backbone of modern applications, supporting businesses, organizations, and systems across industries.\nDifferent types of databases can be classified in terms of their structure, usage, or storage methods. Two main types are:\n\nRelational / SQL\nNoSQL"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\nA relational database organizes data into tables with certain pre-defined relationships to each other. These relationships are logical connections, established on the basis of interaction among these tables."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-1",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-1",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\nThe relational model underlying such databases uses special lingo to refer to the entities that make up its structure:\n\n\n\n\n\nFamiliar term\nRelational DB term\n\n\n\n\nTable\nRelation\n\n\nRow\nTuple\n\n\nColumn\nAttribute\n\n\n\n\n\n\n\n\n\n\n\n\nUnlike tables, relations are unordered, i.e. you can shuffle the order of rows or columns and still get the same relation.\nAttributes (columns) specify a data type (e.g. integer or text), and each tuple (or row) contains the value of that specific data type."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-2",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-2",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\n\nAll tables in a relational database have an attribute known as the primary key, which is a unique identifier of a row.\nEach row can be used to create a relationship between different tables using a foreign key, i.e. a reference to a primary key of another existing table.\n\n\nLet’s see how this looks in practice: Suppose we have the following table:\n\n\n\n\n\n\n\n\n\n\n\n\n\nbook_id\nTitle\nAuthor\nFormat\nPublisher\nCountry\nPrice\n\n\n\n\n1\nHarry Potter\nJ.K. Rowling\nPaperback\nBanana Press\nUK\n$20\n\n\n2\nHarry Potter\nJ.K. Rowling\nE-book\nBanana Press\nUK\n$10\n\n\n3\nSherlock Holmes\nConan Doyle\nPaperback\nGuava Press\nUS\n$30\n\n\n4\nThe Hobbit\nJ.R.R. Tolkien\nPaperback\nBanana Press\nUK\n$30\n\n\n5\nSherlock Holmes\nConan Doyle\nPaperback\nGuava Press\nUS\n$15\n\n\n\n\n\n\nClearly, the primary key in this table is book_id."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-3",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-3",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\n\n\nWhat if “Banana Press” changed its name to “Pineapple Press”?\nWe would have to change every single instance of “Banana Press” in the Publisher attribute. Very cumbersome…\nWhat if we instead organized our database like this?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbook_id\nTitle\nAuthor\nFormat\nPublisher_ID\nPrice\n\n\n\n\n1\nHarry Potter\nJ.K. Rowling\nPaperback\npub_1\n$20\n\n\n2\nHarry Potter\nJ.K. Rowling\nE-book\npub_1\n$10\n\n\n3\nSherlock Holmes\nConan Doyle\nPaperback\npub_2\n$30\n\n\n4\nThe Hobbit\nJ.R.R. Tolkien\nPaperback\npub_1\n$30\n\n\n5\nSherlock Holmes\nConan Doyle\nPaperback\npub_2\n$15\n\n\n\n\n\n\nPublisher_ID\nPublisher\nCountry\n\n\n\n\npub_1\nBanana Press\nUK\n\n\npub_2\nGuava Press\nUS"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-4",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-4",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\n\nWe introduced the foreign key “Publisher_ID” into book relation.\nNow, we would just have to change the name of the publisher once, namely in the publisher relation.\nThis is an example of a process called database normalization, which helps reduce redundancy and improve data integrity.\nTo retrieve data from a relational database, one uses a specialized computer language called Structured Query Language (SQL), which gives relational databases an alternative name.\nTo reconstruct our original book relation, the SQL statement would be:\n\n\nSELECT b.book_id, b.Title, b.Author, b.Format,\n       p.Publisher, p.Country, b.Price\nFROM Books AS b\nLEFT JOIN Publishers AS p ON b.Publisher_ID = p.Publisher_ID"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-5",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-5",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\nSQL will not be part of this course, but data scientists working in industry tend to write a lot of SQL…"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-types-of-data",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-types-of-data",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Types of data",
    "text": "Internal data sources – Types of data\n\nRelational databases are the de-facto standard for managing structured data, i.e. data that is highly organized in a table-like format.\nData can also take other forms though:\n\nUnstructured data is not organized in a specific way and does therefore not lend itself to the relational model.\nExamples: text documents, images and video.\nSemi-structured data is a hybrid of the two preceding forms, containing some organizational elements (like tags or markers), but allowing for more flexibility than data stored in a relational model.\nExamples: Data stored in JSON, XML or HTML files."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-types-of-data-1",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-types-of-data-1",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Types of data",
    "text": "Internal data sources – Types of data\nFor unstructured and semi-structured data, the relational model is typically not suitable, or at least suboptimal. For this purpose, non-relational models are preferred."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – NoSQL databases",
    "text": "Internal data sources – NoSQL databases\n\nDespite some fuzzy terminology, non-relational databases are commonly referred to as NoSQL.\nTypes of NoSQL databases include:\n\nKey-value store: simplest form of a NoSQL database, typically of little use in data science.\nWide column store: uses tables, rows and columns similar to relational DBs, but names and format of columns can vary from row to row.\nDocument store: perfect for semi-structured data, can handle complex data structures.\nGraph database prioritize relationships between data objects. They use nodes (data entities) and edges (relationships) to model data.\n\nAs an illustration of the kind of benefits achievable through NoSQL databases, we consider an example for a document store."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-1",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-1",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – NoSQL databases",
    "text": "Internal data sources – NoSQL databases"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-2",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-2",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – NoSQL databases",
    "text": "Internal data sources – NoSQL databases\n\nAs the name suggests, a document store is centred around the concept of “document”, like a JSON or XML file.\nAll documents are assumed to be encoded in the same format.\nEach document has a unique key that can be used to retrieve it.\nA collection of documents could be considered analogous to a table in a relational database, where each row is a document. However, a collection of documents is much more flexible:\n\nIn a table, all rows must have the same sequence of columns.\nIn a collection of documents however, each document can have completely different fields."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-3",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-3",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – NoSQL databases",
    "text": "Internal data sources – NoSQL databases\n\n\n\n\n\n\n\n\n\nDocument 1 – Harry Potter\n\n\n\n\n\n\n\nDocument 2 – Sherlock Holmes\n\n\n\n\n\n\n\nDocument 3 – The Hobbit\n\n\n\n\n\nAdvantages of such a NoSQL database:\n\nNo need to normalize\nEasier extensibility: changes in one document do not affect others.\nBetter locality (all information on a document in one place instead of spread across several tables)."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-others",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-others",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Others",
    "text": "Internal data sources – Others\n\nUnfortunately, a lot of data in corporations is not in a nicely structured form in a database… Instead, big amounts of data are spread around in messy spread sheets, e-mails, presentations, csv-files, etc.\nOrganizing and cleaning such data to gain meaningful insight is a key part of being a data scientist."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-public-data-sets",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-public-data-sets",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – Public data sets",
    "text": "External data sources – Public data sets\n\nExternal data sources are often crucial to understand the general economic environment, market or industry trends, and much more.\nThe first approach towards external data sources is usually to verify whether there are any publicly available data sets from reliable sources, such as:"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs\n\nOften, rather than downloading a data set in form of a csv or xls file, data providers offer Application Programming Interfaces (APIs) to access their data directly.\nAPIs serve as a bridge between different software applications, enabling them to communicate and share data in (near) real-time.\nAs a data source, APIs can be used, for instance, to access data from…\n\n… social media to detect and leverage current viral trends.\n… AI-providers like OpenAI to incorporate information produced by ChatGPT.\n… tech companies like Google to identify highly-rated competitors in the area based on Google Maps.\n…"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-1",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-1",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs\n\nLet’s say you wanted to find out, how many views all of your company’s tweets on X generated last week. If your company tweets a lot, that’s a lot of manual labour… Instead, we could write a program (in R) that uses the X API to gather this information."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-2",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-2",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs\n\n\nCommunication with APIs works through a request and response cycle: the request is sent to the API, which retrieves the data and returns it to the user in form of the response.\nAn API request will typically include the following components:\n\nEndpoint: a dedicated URL declared by the API provider to access the desired resource, e.g. https://api.x.com/2/insights/historical (see here).\nMethod: the type of operation the client wants to perform. Data retrieval operations typically use the GET method (see here for more information on HTTP methods).\nParameters: the variables indicating the specific instructions for the API to process, e.g. the time period for which you want to analyze the views of all tweets.\nRequest headers: meta information about the request, e.g. authentication credentials, such as an API key.\nRequest body: contains further information to be passed to the API, e.g. parameters that are not passed as part of the endpoint."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-3",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-3",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-4",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-4",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs\n\nThe response returned by the API will then typically include the following components:\n\nStatus code: HTTP status codes are three-digit codes that indicate the outcome of an API request. Some common ones are:\n\n200: OK (server successfully returned the requested data)\n400: Bad request (e.g. malformatted request syntax)\n404: Not found (server cannot find requested resource)\n\nResponse headers: very similar to request headers, but provide additional information about response.\nResponse body: includes actual requested data, e.g. the number of views for each of last week’s tweets."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-5",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-5",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-3-identify-if-additional-sources-are-needed",
    "href": "slides/01_Introduction_to_DS.html#step-3-identify-if-additional-sources-are-needed",
    "title": "Data Science and Data Analytics",
    "section": "Step 3 – Identify if additional sources are needed",
    "text": "Step 3 – Identify if additional sources are needed"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-4-statistical-analysis",
    "href": "slides/01_Introduction_to_DS.html#step-4-statistical-analysis",
    "title": "Data Science and Data Analytics",
    "section": "Step 4 – Statistical analysis",
    "text": "Step 4 – Statistical analysis"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-4-statistical-analysis-1",
    "href": "slides/01_Introduction_to_DS.html#step-4-statistical-analysis-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 4 – Statistical analysis",
    "text": "Step 4 – Statistical analysis\n\nAfter data has been obtained, the data scientist needs to clean, process and explore it. This can involve:\n\nHandling missing values, outliers, and noise.\nExploratory data analysis (EDA): Visualizations, correlation analysis, and summary statistics.\n\nBased on clean data, statistical tests can be employed to validate existing business hypotheses. Here, all the tests you have (hopefully) learned in your statistics classes come in, e.g. T-tests, ANOVA, \\(\\chi^2\\) tests, etc.\nThrough this process, the data scientist can start to develop an understanding of the data."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development",
    "href": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development",
    "title": "Data Science and Data Analytics",
    "section": "Step 5 – Implementation and development",
    "text": "Step 5 – Implementation and development"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development-1",
    "href": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 5 – Implementation and development",
    "text": "Step 5 – Implementation and development\n\nAfter familiarizing themselves with the data, data scientists then have to translate insights into action. This involves offering some kind of product back to the business.\nThis could take the form of:\n\nTraining and deploying a machine learning model, e.g. offering the business a model that tells them the probability that a given customer will churn.\nBuilding dashboards to make the data-driven insights accessible to non-technical people in business.\nSupport strategic decision making by providing appropriate statistics, figures, graphs and other data."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development-2",
    "href": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development-2",
    "title": "Data Science and Data Analytics",
    "section": "Step 5 – Implementation and development",
    "text": "Step 5 – Implementation and development"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-6-communicate-results",
    "href": "slides/01_Introduction_to_DS.html#step-6-communicate-results",
    "title": "Data Science and Data Analytics",
    "section": "Step 6 – Communicate results",
    "text": "Step 6 – Communicate results"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-6-communicate-results-1",
    "href": "slides/01_Introduction_to_DS.html#step-6-communicate-results-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 6 – Communicate results",
    "text": "Step 6 – Communicate results\n\nCommunication is an often underappreciated aspect of data science in business: any data insight can only be as good as it can be made understood by the people that (should) act on it.\nKey communication goals:\n\nSimplify complex technical details without losing accuracy.\nTailor the message to your audience (executives, technical teams, clients).\n\nCommunication strategies:\n\nVisual storytelling: Use charts, graphs, and dashboards to highlight trends and key metrics.\nNarrative & context: Frame results within a business story: What problem was solved? How do the results impact the business?\nActionable insights: Include clear recommendations and next steps."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-7-maintenance",
    "href": "slides/01_Introduction_to_DS.html#step-7-maintenance",
    "title": "Data Science and Data Analytics",
    "section": "Step 7 – Maintenance",
    "text": "Step 7 – Maintenance"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-7-maintenance-1",
    "href": "slides/01_Introduction_to_DS.html#step-7-maintenance-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 7 – Maintenance",
    "text": "Step 7 – Maintenance\n\nOnce a data science product like a machine learning model or a dashboard is “in production”, i.e. is used by business, it will not age like fine wine…\nInstead, changes in data over time and general software rot will quickly cause the product to be useless if left unchanged.\nHence, data scientists need to subject their products to continuous maintenance:\n\nMonitor performance over time: track metrics to detect data drift or deterioration in model performance.\nUpdate models and systems: retrain models with new data when necessary.\nRegularly engage stakeholders for feedback\nIdentify new problems and areas for improvement and restart the cycle."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-essence-of-data-science",
    "href": "slides/01_Introduction_to_DS.html#the-essence-of-data-science",
    "title": "Data Science and Data Analytics",
    "section": "The essence of Data Science",
    "text": "The essence of Data Science\n\n\n\nData science is a demanding field that requires skills in maths, statistics and computer science, but also high levels of domain expertise and excellent communication.\nThus, it is impossible to cover all areas of DS in a single course…\nThis course focuses on what could be called the essence of data science:\n\nStatistical analysis\nImplementation and development\nCommunication of results\n\nWe will further break these steps down in the data science workflow."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-1",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-1",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow",
    "text": "The Data Science workflow\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-i",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-i",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part I",
    "text": "The Data Science workflow – Part I\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-i-1",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-i-1",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part I",
    "text": "The Data Science workflow – Part I\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-ii",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-ii",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part II",
    "text": "The Data Science workflow – Part II\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-iii",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-iii",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part III",
    "text": "The Data Science workflow – Part III\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-understanding-data",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-understanding-data",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Understanding data",
    "text": "The Data Science workflow – Understanding data\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-iv",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-iv",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part IV",
    "text": "The Data Science workflow – Part IV\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-how-program",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-how-program",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – How? Program!",
    "text": "The Data Science workflow – How? Program!\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#how-about-data-analytics",
    "href": "slides/01_Introduction_to_DS.html#how-about-data-analytics",
    "title": "Data Science and Data Analytics",
    "section": "How about Data Analytics?",
    "text": "How about Data Analytics?\n\nThe name of the course is Data Science and Data Analytics, but only data science has been mentioned so far. How about data analytics?\nEssentially, data analytics is data science without the strong computer science and machine learning focus. Therefore, the steps involved in data analytics are typically a subset of the steps in a data science project:"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#how-about-data-analytics-1",
    "href": "slides/01_Introduction_to_DS.html#how-about-data-analytics-1",
    "title": "Data Science and Data Analytics",
    "section": "How about Data Analytics?",
    "text": "How about Data Analytics?\nThe two fields thus have a large intersection, which is also reflected in the tasks that data scientists and data analysts share:\n\nAs we are focused on tasks in the intersection, we will mostly be referring to data science as the overarching topic in the remainder of this course."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#programming-languages-for-data-science",
    "href": "slides/02_The_Essentials_of_R_Programming.html#programming-languages-for-data-science",
    "title": "Data Science and Data Analytics",
    "section": "Programming languages for Data Science",
    "text": "Programming languages for Data Science"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#what-is-r",
    "href": "slides/02_The_Essentials_of_R_Programming.html#what-is-r",
    "title": "Data Science and Data Analytics",
    "section": "What is R?",
    "text": "What is R?\n\n\nR is an open-source statistical programming language.\nR is not a programming language like C or Java. It was not created by software engineers for software development. Instead, it was developed by statisticians as an environment for statistical computing and graphics.\nSome of its attractive features are:\n\nR is free and open source.\nR runs on all major platforms: Windows, macOS, UNIX/Linux.\nThere is a large, growing, and active community of R users and, as a result, there are numerous resources for learning and asking questions.\nIt is easily extensible through packages. In this way, it is easy to share software implementations of new data science methodologies."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#a-very-brief-history-of-r",
    "href": "slides/02_The_Essentials_of_R_Programming.html#a-very-brief-history-of-r",
    "title": "Data Science and Data Analytics",
    "section": "A (very) brief history of R",
    "text": "A (very) brief history of R\n\n\nR was first developed by Robert Gentleman and Ross Ihaka (University of Auckland, New Zealand) in the 1990s.\nVery early in the development of R, two statisticians at TU Vienna (Kurt Hornik and Fritz Leisch) got word about the initiative in New Zealand and started contributing to R.\nSoon, the growing interest in this project from researchers all over the world led to the establishment of the R Core team, which maintains R to this day.\nThe strong involvement of the two Viennese researchers also led to the establishment of the R Foundation for Statistical Computing in Vienna (now at WU). Its main mission is the support of the continued development of R.\nToday, R is the lingua franca of statistics and an essential tool for data science."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#importance-of-r-in-data-science",
    "href": "slides/02_The_Essentials_of_R_Programming.html#importance-of-r-in-data-science",
    "title": "Data Science and Data Analytics",
    "section": "Importance of R in Data Science",
    "text": "Importance of R in Data Science"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#what-is-rstudio",
    "href": "slides/02_The_Essentials_of_R_Programming.html#what-is-rstudio",
    "title": "Data Science and Data Analytics",
    "section": "What is RStudio?",
    "text": "What is RStudio?\n\n\nRStudio is a convenient interface for R called an integrated development environment (IDE). An IDE is the program on a computer that a developer uses to write code.\nRStudio is an open-source IDE by Posit specifically designed to support developers in writing R code. It is not a requirement for programming with R, but it is very commonly used by R programmers and data scientists."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#what-is-rstudio-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#what-is-rstudio-1",
    "title": "Data Science and Data Analytics",
    "section": "What is RStudio?",
    "text": "What is RStudio?"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#a-tour-of-rstudio",
    "href": "slides/02_The_Essentials_of_R_Programming.html#a-tour-of-rstudio",
    "title": "Data Science and Data Analytics",
    "section": "A Tour of RStudio",
    "text": "A Tour of RStudio"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#a-tour-of-rstudio-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#a-tour-of-rstudio-1",
    "title": "Data Science and Data Analytics",
    "section": "A Tour of RStudio",
    "text": "A Tour of RStudio\n\nWhile we can write and execute code directly in the R console in the bottom left pane in RStudio, it is better to write all code in an R script.\nAn R script is simply a text file that contains R code.\nIn RStudio, R scripts can be opened, viewed and edited in the top left pane. To open a new R script, go to File &gt; New File &gt; R script.\nA single line of code in an R script can be sent to the console for execution via Ctrl + Enter (or Cmd + Enter on Mac).\nTo write a comment in an R script, simply put a hashtag (#) in front of the comment:\n\n\n# This is our first R script\n2 + 2 # We calculate 2 + 2\n\n[1] 4"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-mathematical-operators",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-mathematical-operators",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Mathematical operators",
    "text": "First steps in R – Mathematical operators\nFirst, let’s use R as a calculator. We can write a calculation into our R script and R will give us the result, when the code is executed:\n\n2 + 3 # Addition\n\n[1] 5\n\n2 - 4 # Subtraction\n\n[1] -2\n\n3 * 4 # Multiplication\n\n[1] 12\n\n4 / 2 # Division\n\n[1] 2\n\n3^3 # Exponentiation\n\n[1] 27\n\n(2 + 3)*(3 + 4) # Bracketing\n\n[1] 35\n\n2^(3 + 4/2)*4 # All together\n\n[1] 128"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-relational-and-logical-operators",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-relational-and-logical-operators",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Relational and logical operators",
    "text": "First steps in R – Relational and logical operators\nWe can also perform comparisons using R:\n\n3 &lt; 5\n\n[1] TRUE\n\n3 &gt; 5\n\n[1] FALSE\n\n3 &lt;= 3\n\n[1] TRUE\n\n3 &gt;= 5\n\n[1] FALSE\n\n3 == 5 # Attention for the double equal sign!\n\n[1] FALSE\n\n3 != 5\n\n[1] TRUE\n\n\n\nWith the help of & and |, we can also carry out AND and OR operations:\n\n(3 == 5) | (5 == 5) # OR\n\n[1] TRUE\n\n(3 == 5) & (5 == 5) # AND\n\n[1] FALSE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions\nR comes with many functions that you can use to perform tasks from simple to sophisticated. Functions have inputs (aka arguments) that you pass into them and outputs (aka return values) that they give back. Functions are fundamental to how R works.\n\nLet’s see an example. Say, we want to round the number 5.293586 to two decimal digits. Fortunately, there is a function in R called round. But how do we use it?\nTo find out, we can bring up the R help, which provides documentation for every function in R, by typing ? and the name of the function into the console:\n\n?round"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-1",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-2",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions\n\nround(5.293586, 2)\n\n[1] 5.29\n\n\n\nGenerally, we can use the args function to see the arguments that a function takes. For example, let’s say, we wanted to simulate someone throwing a regular die, i.e. randomly sample numbers from 1 to 6. Luckily, there is an R function called sample. Let’s inspect it.\n\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n\n\n\nTo find out more information on what these arguments actually mean, let’s consult the documentation again:\n\n?sample"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-3",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-4",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-4",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions\nSo, to simulate 20 throws of a regular die, we have to use the function like this:\n\nsample(6, 20, replace = TRUE)\n\n [1] 1 5 1 1 2 4 2 2 1 4 1 5 6 4 2 2 3 1 1 3\n\n\n\nNote that we can give the arguments to the function\n\n\nin the order that they are given in the documentation (then they do not need to be named).\nby explicitly naming them when calling the function (then the order does not matter).\nby mixing unnamed and named arguments (like we have done here).\n\n\n\n\nLet’s see another example of this."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-5",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-5",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions\nLet’s say, we wanted to compute the base 2 logarithm of 10. For this, we need the log function in R:\n\nargs(log)\n\nfunction (x, base = exp(1)) \nNULL\n\n\n\nSo, based on the three options given on the previous slide, all of the following three ways of calling log lead to the correct result:\n\nlog(10, 2)\n\n[1] 3.321928\n\nlog(base = 2, x = 10)\n\n[1] 3.321928\n\nlog(10, base = 2)\n\n[1] 3.321928\n\n\nHowever, options 1 and 3 are preferable, as switching the order of arguments tends to make code less easily readable for others (which could be yourself in the future…)"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Variables and assignment",
    "text": "First steps in R – Variables and assignment\nLet’s say, you were given the task to solve the quadratic equation\n\\[3x^2 - 5x - 1 = 0\\]\nYou will remember from back in your high school days that we can use the “midnight formula” for this:\n\\[x_{1/2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\]\nWith R, we can easily compute this of course:\n\n(-(-5) + sqrt((-5)^2 - 4*3*(-1)))/(2*3)\n\n[1] 1.847127\n\n(-(-5) - sqrt((-5)^2 - 4*3*(-1)))/(2*3)\n\n[1] -0.1804604"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-1",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Variables and assignment",
    "text": "First steps in R – Variables and assignment\nNext, we are given another quadratic equation\n\\[4x^2 - 8x + 2 = 0\\]\nTo compute the solution with R, we would have to replace every occurrence of \\(a\\), \\(b\\) and \\(c\\), so in total, we would have to make 10 replacements to get both solutions. That’s too cumbersome and error-prone…\n\nInstead, we can define variables \\(a\\), \\(b\\) and \\(c\\) and simply assign different values to them every time we have to solve a quadratic equation. Such assignments happen in R with the help of the assignment operator &lt;- (read it as “gets”):\n\na &lt;- 3\nb &lt;- -5\nc &lt;- -1\n# Note that nothing is printed to the console when assigning variables.\n\nNow, we can write expressions using these variables like we would in maths."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-2",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Variables and assignment",
    "text": "First steps in R – Variables and assignment\n\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n\n[1] 1.847127\n\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n\n[1] -0.1804604\n\n\nNow, to compute the solution to the second quadratic equation, we simply re-define the variables a, b and c and then evaluate the same expression again:\n\na &lt;- 4\nb &lt;- -8\nc &lt;- 2\n\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n\n[1] 1.707107\n\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n\n[1] 0.2928932\n\n\n\nNote that R is case-sensitive, so it now knows variables with names a, b and c, but it does not know A, B or C:\n\nA\n\nError: Objekt 'A' nicht gefunden"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-3",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Variables and assignment",
    "text": "First steps in R – Variables and assignment\nWith assignments, we are creating objects in R that are saved and can be referenced by the names that we give them (e.g. a, b and c). Creating objects like this will make them appear in the work space in pane 4 of the RStudio window:\n\n\n\n\n\nWe can also see all variables currently defined in the work space by typing ls():\n\nls()\n\n[1] \"a\" \"b\" \"c\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-user-defined-functions",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-user-defined-functions",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – User-defined functions",
    "text": "First steps in R – User-defined functions\nWith the assignment operator, we can also define our own functions of course! To define a function, we need a function name, arguments, a function body and a return value:"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-user-defined-functions-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-user-defined-functions-1",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – User-defined functions",
    "text": "First steps in R – User-defined functions\nConsider the following example: say we want to write a function called quadratic_solver (function name) that gives us the solution to any quadratic equation. It needs input arguments a, b and c and should return the two solutions. In the function body, the two solutions should be computed for the three inputs. So, we could create the function as follows:\n\nquadratic_solver &lt;- function(a, b, c){\n  sol_1 &lt;- (-b + sqrt(b^2 - 4*a*c))/(2*a)\n  sol_2 &lt;- (-b - sqrt(b^2 - 4*a*c))/(2*a)\n  return(c(sol_1, sol_2))\n}\n\n\nNow, we can use this function like any other:\n\nquadratic_solver(3, -5, -1)\n\n[1]  1.8471271 -0.1804604\n\nquadratic_solver(4, -8, 2)\n\n[1] 1.7071068 0.2928932"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#data-types-in-r",
    "href": "slides/02_The_Essentials_of_R_Programming.html#data-types-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Data types in R",
    "text": "Data types in R\nIn R, the following six data types are available:\n\ndouble (i.e., double precision floating-point number – R lingo for real number),\ninteger,\ncharacter (sometimes referred to as string),\nlogical/boolean (can only take values TRUE or FALSE),\ncomplex (as in complex numbers, not relevant for us),\nraw (not relevant for us),\n\nWe can find the type of an object using the function typeof. We can verify, whether an object is of a certain &lt;type&gt; by using the function is.&lt;type&gt;. Let’s see some examples."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#double",
    "href": "slides/02_The_Essentials_of_R_Programming.html#double",
    "title": "Data Science and Data Analytics",
    "section": "Double",
    "text": "Double\nBy default, R will save any number that you type in as a double.\n\na &lt;- 2\nb &lt;- 3.141593\n\ntypeof(a)\n\n[1] \"double\"\n\ntypeof(b)\n\n[1] \"double\"\n\nis.double(a)\n\n[1] TRUE\n\nis.double(b)\n\n[1] TRUE\n\n\nTogether with integer, the data type double is one of the two numeric types, i.e. representing numbers:\n\nis.numeric(a)\n\n[1] TRUE\n\nis.numeric(b)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#integer",
    "href": "slides/02_The_Essentials_of_R_Programming.html#integer",
    "title": "Data Science and Data Analytics",
    "section": "Integer",
    "text": "Integer\nIntegers (whole numbers) are (positive or negative) numbers that can be written without a decimal component. This data type is more important for developers as it saves memory (compared to doubles). To specify an integer over a double, the number has to be followed by an uppercase L:\n\ni &lt;- 10L\n\ntypeof(i)\n\n[1] \"integer\"\n\nis.integer(i)\n\n[1] TRUE\n\nis.double(i)\n\n[1] FALSE\n\n\n(Pure) integers are also numeric of course:\n\nis.numeric(i)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#character",
    "href": "slides/02_The_Essentials_of_R_Programming.html#character",
    "title": "Data Science and Data Analytics",
    "section": "Character",
    "text": "Character\nText data is represented in R with the help of the character data type. To demarcate a string of characters, you can use double or single quotes (\"\" or '').\n\nletter &lt;- \"a\"\nw &lt;- 'Hello, world.'\n\ntypeof(letter)\n\n[1] \"character\"\n\ntypeof(w)\n\n[1] \"character\"\n\nis.character(letter)\n\n[1] TRUE\n\nis.character(w)\n\n[1] TRUE\n\n\nOf course, character objects are not numeric.\n\nis.numeric(letter)\n\n[1] FALSE\n\nis.numeric(w)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#logical",
    "href": "slides/02_The_Essentials_of_R_Programming.html#logical",
    "title": "Data Science and Data Analytics",
    "section": "Logical",
    "text": "Logical\nWhen working with relational operators, we already saw a couple of instances of the logical data type. It can only take the values TRUE and FALSE. Negation of a logical object (i.e. saying NOT) can be achieved with the help of the ! operator:\n\nbigger &lt;- 5 &gt; 3\n\ntypeof(bigger)\n\n[1] \"logical\"\n\nis.logical(bigger)\n\n[1] TRUE\n\n!bigger\n\n[1] FALSE\n\n\nLogical objects are also not numeric.\n\nis.numeric(bigger)\n\n[1] FALSE\n\nis.numeric(!bigger)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#type-coercion",
    "href": "slides/02_The_Essentials_of_R_Programming.html#type-coercion",
    "title": "Data Science and Data Analytics",
    "section": "Type coercion",
    "text": "Type coercion\nWe can coerce an object to be of a certain &lt;type&gt; by using the function as.&lt;type&gt;. This process is called type coercion.\nIn some cases, this is very intuitive…\n\nas.double(\"2.71828\")\n\n[1] 2.71828\n\nas.integer(\"10\")\n\n[1] 10\n\nas.numeric(\"10\") # creates a double\n\n[1] 10\n\nas.character(42)\n\n[1] \"42\"\n\n\n\n… in others, maybe not so much…\n\nas.integer(2.9) # returns the closest smaller integer\n\n[1] 2\n\nas.logical(2.9) # returns TRUE for any number other than 0\n\n[1] TRUE\n\nas.logical(0)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#special-values",
    "href": "slides/02_The_Essentials_of_R_Programming.html#special-values",
    "title": "Data Science and Data Analytics",
    "section": "Special values",
    "text": "Special values\nNon-sensical attempts at coercion are translated as NA (not available).\n\nas.numeric(\"Hello, world.\")\n\nWarning: NAs durch Umwandlung erzeugt\n\n\n[1] NA\n\nas.logical(\"5\")\n\n[1] NA\n\n\n\nNA is one of four special values. While NA can occur for any data type, the other three can only occur for numeric data types. These are:\n\nInf: positive infinity\n-Inf: negative infinity\nNaN: not a number\n\nWhile they are not technically numbers, these special values still follow logical rules when applying mathematical operations on them."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#special-values-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#special-values-1",
    "title": "Data Science and Data Analytics",
    "section": "Special values",
    "text": "Special values\n\n\n\n2 / 0\n\n[1] Inf\n\n2 / 0 + 2 / 0\n\n[1] Inf\n\n-2 / 0\n\n[1] -Inf\n\n-2 / 0 - 2 / 0\n\n[1] -Inf\n\n\n\n\n0 / 0\n\n[1] NaN\n\n2 / 0 - 2 / 0\n\n[1] NaN\n\n- 1 / 0 + 2 / 0\n\n[1] NaN\n\n\n\n\nUnlike NaN, NAs are genuinely unknown values. Nevertheless, they also have their logical rules. Consider the following example. Let’s think about why each of the following four results makes sense:\n\n\n\nTRUE | NA\n\n[1] TRUE\n\nTRUE & NA\n\n[1] NA\n\n\n\n\nFALSE | NA\n\n[1] NA\n\nFALSE & NA\n\n[1] FALSE\n\n\n\n\n\nNote: all special values have their own is.&lt;special&gt; function to check for them, i.e. is.na, is.nan and is.infinite."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#data-structures",
    "href": "slides/02_The_Essentials_of_R_Programming.html#data-structures",
    "title": "Data Science and Data Analytics",
    "section": "Data structures",
    "text": "Data structures\n\nOn top of these six basic data types, R builds several kinds of more complex data structures in different ways to suit different applications that are regularly encountered in the statistics / data science context.\nWe will use the following regularly and hence discuss them in more detail:\n\n(Atomic) vector\nlist\nmatrix\ndata.frame\nfactor\nDate"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#atomic-vector",
    "href": "slides/02_The_Essentials_of_R_Programming.html#atomic-vector",
    "title": "Data Science and Data Analytics",
    "section": "Atomic vector",
    "text": "Atomic vector\nAn atomic vector is a simple vector of values of one data type. Values can be concatenated together into a vector using the c function. For example:\n\nx &lt;- c(3, 4, 5)\nx\n\n[1] 3 4 5\n\n\n\nSince we have filled the vector x with values of type double, it will itself also be of type double. It has several attributes / characteristics such as length:\n\ntypeof(x)\n\n[1] \"double\"\n\nis.double(x)\n\n[1] TRUE\n\nlength(x)\n\n[1] 3\n\n\n\n\nWe can also assign names to the vector elements:\n\nnames(x) &lt;- c(\"a\", \"b\", \"c\")\nx\n\na b c \n3 4 5"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#atomic-vector-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#atomic-vector-1",
    "title": "Data Science and Data Analytics",
    "section": "Atomic vector",
    "text": "Atomic vector\nWhat happens if we try to create a vector with data of different types?\n\ny &lt;- c(1, \"a\", TRUE)\ny\n\n[1] \"1\"    \"a\"    \"TRUE\"\n\n\n\nWhen we attempt to combine different types, R will coerce the data in a fixed order, namely character \\(\\to\\) double \\(\\to\\) integer \\(\\to\\) logical, i.e. if any data of a higher-order type appears in the vector creation, the vector will be of that type:\n\ntypeof(y)\n\n[1] \"character\"\n\n\n\n\nSo, what will be the type of the following vector?\n\nz &lt;- c(0L, 1L, 2, TRUE)\n\n\n\nAs the 2 is of type double, this is the highest-order type in the vector, so:\n\ntypeof(z)\n\n[1] \"double\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-vectorization",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-vectorization",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Vectorization",
    "text": "Working with vectors – Vectorization\nMost operations in R are vectorized, which means they are (automatically) performed element by element:\n\nx\n\na b c \n3 4 5 \n\nx^2\n\n a  b  c \n 9 16 25 \n\nlog(x)\n\n       a        b        c \n1.098612 1.386294 1.609438 \n\n\nThis also applies when we want to add / subtract / multiply / divide two vectors element-wise:\n\ny &lt;- c(5, 12, 13)\nx + y\n\n a  b  c \n 8 16 18 \n\nx * y\n\n a  b  c \n15 48 65"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-vectorization-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-vectorization-1",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Vectorization",
    "text": "Working with vectors – Vectorization\nIf two vectors are of different lengths, then R recycles the smaller one to allow operations like the following:\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(10, 20, 30, 40, 50, 60)\nx + 2\n\n[1] 3 4 5\n\nx + y\n\n[1] 11 22 33 41 52 63\n\n\nHowever, beware of recycling:\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(10, 20, 30, 40, 50, 60, 70)\nx + y\n\nWarning in x + y: Länge des längeren Objektes\n     ist kein Vielfaches der Länge des kürzeren Objektes\n\n\n[1] 11 22 33 41 52 63 71"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-sequences",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-sequences",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Sequences",
    "text": "Working with vectors – Sequences\nTo create vectors that are sequences, there is a very useful R function called seq:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(2, 20, by = 2)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\nseq(1.2, 0.2, by = -0.1)\n\n [1] 1.2 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2\n\n\n\nSequences of consecutive integers – like the first one we saw – are particularly frequently needed in (R) programming. For this reason, there is a short-hand notation (“syntactic sugar”) to create sequences of that kind, which is start_value:end_value:\n\ns &lt;- 1:10\ntypeof(s)\n\n[1] \"integer\"\n\ns\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-subsetting",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-subsetting",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Subsetting",
    "text": "Working with vectors – Subsetting\nIf we want to extract or replace elements of a vector, you can use square brackets [] with logical or numeric input:\n\nalphabet &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n              \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\n# Numeric subsetting\nalphabet[1:3] # first three\n\n[1] \"a\" \"b\" \"c\"\n\nalphabet[3:1] # first three in descending order\n\n[1] \"c\" \"b\" \"a\"\n\nalphabet[c(2, 4)] # second and fourth\n\n[1] \"b\" \"d\"\n\nalphabet[-(1:10)] # all except the first 10\n\n [1] \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\nalphabet[5] &lt;- \"E\" # replace the fifth element by a capital E\n\n# Logical subsetting\nalphabet == \"E\"\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE\n\nalphabet[alphabet == \"E\"]\n\n[1] \"E\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-subsetting-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-subsetting-1",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Subsetting",
    "text": "Working with vectors – Subsetting\nWe can also use the which function to turn a logical vector into an integer vector that gives the indices of all elements in the vector that are TRUE:\n\nwhich(alphabet == \"E\")\n\n[1] 5\n\nalphabet[which(alphabet == \"E\")]\n\n[1] \"E\"\n\n\n\nWith the %in% operator, we can check whether elements of the vector are contained in another vector:\n\nalphabet %in% c(\"x\", \"y\", \"z\")\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[25]  TRUE  TRUE\n\nwhich(alphabet %in% c(\"x\", \"y\", \"z\"))\n\n[1] 24 25 26\n\nalphabet[alphabet %in% c(\"x\", \"y\", \"z\")]\n\n[1] \"x\" \"y\" \"z\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-sorting",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-sorting",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Sorting",
    "text": "Working with vectors – Sorting\nFor sorting vectors, we can use the sort function. This works with character vectors:\n\n# Sorting a character vector\nunsorted_letters &lt;- sample(alphabet)\nunsorted_letters\n\n [1] \"d\" \"E\" \"m\" \"y\" \"t\" \"b\" \"h\" \"c\" \"a\" \"j\" \"v\" \"n\" \"k\" \"f\" \"q\" \"p\" \"z\" \"l\" \"x\"\n[20] \"u\" \"w\" \"i\" \"r\" \"g\" \"s\" \"o\"\n\nsorted_letters &lt;- sort(unsorted_letters)\nsorted_letters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"E\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\n… as well as with numeric vectors:\n\n# Sorting a numeric vector in decreasing order:\nunsorted_numbers &lt;- sample(1:20)\nunsorted_numbers\n\n [1]  3 17 18  6 19  2  4 20 16  5 15  7 12 10  9 11  1 14 13  8\n\nsorted_numbers &lt;- sort(unsorted_numbers, decreasing = TRUE)\nsorted_numbers\n\n [1] 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3  2  1"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#list",
    "href": "slides/02_The_Essentials_of_R_Programming.html#list",
    "title": "Data Science and Data Analytics",
    "section": "List",
    "text": "List\nLists are a step up in complexity from atomic vectors: each element can be any type, not just vectors. We construct lists with the function list:\n\nl1 &lt;- list(1:3, \"R\", c(TRUE, FALSE, TRUE), c(2.3, 5.9))\n\nl1\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] \"R\"\n\n[[3]]\n[1]  TRUE FALSE  TRUE\n\n[[4]]\n[1] 2.3 5.9\n\ntypeof(l1)\n\n[1] \"list\"\n\n\nLists are sometimes called recursive vectors because a list can contain other lists. This makes them fundamentally different from atomic vectors."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#list-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#list-1",
    "title": "Data Science and Data Analytics",
    "section": "List",
    "text": "List\nThe elements of a list can also have names. These can be accessed with the help of the $ operator. Alternatively, to access a single element of a list, we can also use double square brackets [[]]:\n\nnames(l1) &lt;- c(\"A\", \"B\", \"C\", \"D\")\nl1\n\n$A\n[1] 1 2 3\n\n$B\n[1] \"R\"\n\n$C\n[1]  TRUE FALSE  TRUE\n\n$D\n[1] 2.3 5.9\n\nl1$A\n\n[1] 1 2 3\n\nl1[[3]]\n\n[1]  TRUE FALSE  TRUE\n\nl1[[\"D\"]]\n\n[1] 2.3 5.9"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-lists-subsetting",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-lists-subsetting",
    "title": "Data Science and Data Analytics",
    "section": "Working with lists – Subsetting",
    "text": "Working with lists – Subsetting\nFor subsetting lists, essentially the same rules apply as for atomic vectors, i.e. we can subset using numeric or logical arguments and single square brackets []:\n\n# Numeric subsetting\nl1[1:2] # First two elements\n\n$A\n[1] 1 2 3\n\n$B\n[1] \"R\"\n\nl1[-(1:2)] # All elements except the first\n\n$C\n[1]  TRUE FALSE  TRUE\n\n$D\n[1] 2.3 5.9\n\n# Logical subsetting\nl1[c(FALSE, FALSE, FALSE, TRUE)]\n\n$D\n[1] 2.3 5.9"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#matrix",
    "href": "slides/02_The_Essentials_of_R_Programming.html#matrix",
    "title": "Data Science and Data Analytics",
    "section": "Matrix",
    "text": "Matrix\nOne way to construct more complex data structures on top of elementary building blocks like vectors or lists is to assign a class to them. A class is metadata about the object that can determine how common functions operate on that object.\n\nProbably the easiest example of this is a matrix. A matrix is just a two-dimensional array of numbers:\n\nm &lt;- matrix(1:12, nrow = 3, ncol = 4)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe function matrix will fill up the matrix column by column by default. There is an option to fill them by row instead, see ?matrix."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#matrix-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#matrix-1",
    "title": "Data Science and Data Analytics",
    "section": "Matrix",
    "text": "Matrix\nTechnically speaking, in R, a matrix is actually just a vector with an attribute that specifies the dimensions (i.e. number of rows and columns) of the matrix. We can access the dimensions of a matrix with the help of the functions dim, nrow and ncol:\n\ntypeof(m) # just an integer vector behind the scenes\n\n[1] \"integer\"\n\ndim(m) # but dimensions are 3 x 4 (3 rows, 4 columns)\n\n[1] 3 4\n\nnrow(m) # number of rows\n\n[1] 3\n\nncol(m) # number of columns\n\n[1] 4\n\nclass(m) # It is of class matrix and array\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#matrix-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#matrix-2",
    "title": "Data Science and Data Analytics",
    "section": "Matrix",
    "text": "Matrix\nWe can add additional rows or columns with the help of rbind or cbind:\n\nm &lt;- cbind(m, c(13, 14, 15))\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    4    7   10   13\n[2,]    2    5    8   11   14\n[3,]    3    6    9   12   15\n\nm &lt;- rbind(rep(1, 5), m)\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    1    1    1\n[2,]    1    4    7   10   13\n[3,]    2    5    8   11   14\n[4,]    3    6    9   12   15\n\n\n\nFor a matrix, we can set both row names and column names:\n\nrownames(m) &lt;- c(\"row_1\", \"row_2\", \"row_3\", \"row_4\")\ncolnames(m) &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\")\nm\n\n      A B C  D  E\nrow_1 1 1 1  1  1\nrow_2 1 4 7 10 13\nrow_3 2 5 8 11 14\nrow_4 3 6 9 12 15"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-matrices-subsetting",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-matrices-subsetting",
    "title": "Data Science and Data Analytics",
    "section": "Working with matrices – Subsetting",
    "text": "Working with matrices – Subsetting\nAs a matrix is a two-dimensional object, we need two indices separated by a comma for subsetting. If we do not specify one dimension, all elements across that dimension are selected:\n\nm[2, ] # 2nd row\n\n A  B  C  D  E \n 1  4  7 10 13 \n\nm[, 3] # 3rd column\n\nrow_1 row_2 row_3 row_4 \n    1     7     8     9 \n\nm[3, 4] # 3rd element of the 4th column\n\n[1] 11\n\nm[c(FALSE, FALSE, FALSE, TRUE), ] # 4th row\n\n A  B  C  D  E \n 3  6  9 12 15 \n\n\n\nWe can also subset by name if column and / or row names are provided:\n\nm[\"row_4\", \"E\"]\n\n[1] 15"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#data-frame",
    "href": "slides/02_The_Essentials_of_R_Programming.html#data-frame",
    "title": "Data Science and Data Analytics",
    "section": "Data Frame",
    "text": "Data Frame\nAs a matrix is basically just a vector underneath, all values still need to be of the same data type. However, real tabular data often includes data of different types (e.g. name, height, education). To represent this, we need a data structure where the columns can be of different types.\n\nIn R, such a data structure is provided by the data.frame. Underlying it is a named list, whose elements represent columns. Therefore, all elements need to have the same length. Consider the following example:\n\nheights &lt;- c(176, 178, 156)\nnames &lt;- c(\"Anna\", \"Jakob\", \"Lisa\")\neduc &lt;- c(\"BSc\", \"MA\", \"PhD\")\ndf &lt;- data.frame(heights = heights, names = names, educ = educ)\ndf\n\n  heights names educ\n1     176  Anna  BSc\n2     178 Jakob   MA\n3     156  Lisa  PhD"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#data-frame-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#data-frame-1",
    "title": "Data Science and Data Analytics",
    "section": "Data Frame",
    "text": "Data Frame\n\nLet’s inspect the “list nature” of the data.frame object df we just created:\n\ntypeof(df)\n\n[1] \"list\"\n\nclass(df)\n\n[1] \"data.frame\"\n\n\n\n\n\nEven though df is fundamentally a list, it behaves differently than a standard list:\n\ndf\n\n  heights names educ\n1     176  Anna  BSc\n2     178 Jakob   MA\n3     156  Lisa  PhD\n\nas.list(df)\n\n$heights\n[1] 176 178 156\n\n$names\n[1] \"Anna\"  \"Jakob\" \"Lisa\" \n\n$educ\n[1] \"BSc\" \"MA\"  \"PhD\"\n\n\nThis is because, on top of being a list, the object df is of class data.frame, which changes how common functions operate on it."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-data-frames",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-data-frames",
    "title": "Data Science and Data Analytics",
    "section": "Working with Data Frames",
    "text": "Working with Data Frames\nSince a data.frame also represents two-dimensional data, many of the functions we used for matrices work on it as well:\n\ndim(df)\n\n[1] 3 3\n\nnrow(df)\n\n[1] 3\n\ncolnames(df)\n\n[1] \"heights\" \"names\"   \"educ\"   \n\n\n\nIn particular, all the subsetting methods we showed are also applicable:\n\ndf[1, ]\n\n  heights names educ\n1     176  Anna  BSc\n\ndf[, 2]\n\n[1] \"Anna\"  \"Jakob\" \"Lisa\" \n\ndf[c(TRUE, TRUE, FALSE), 3]\n\n[1] \"BSc\" \"MA\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-data-frames-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-data-frames-1",
    "title": "Data Science and Data Analytics",
    "section": "Working with Data Frames",
    "text": "Working with Data Frames\nHowever, as data.frames are lists underneath, we can also use the $ operator to access the elements of the list as before.\n\ndf$heights\n\n[1] 176 178 156\n\ndf$names\n\n[1] \"Anna\"  \"Jakob\" \"Lisa\" \n\ndf$educ\n\n[1] \"BSc\" \"MA\"  \"PhD\"\n\n\n\nThis is a particularly common action when working with real data, as it allows us to access the variables in the columns of the data.frame. For example, let’s say we made a mistake and found out that Jakob was really 187 cm tall. We could change the corresponding data point as follows:\n\ndf$heights[2] &lt;- 187\ndf\n\n  heights names educ\n1     176  Anna  BSc\n2     187 Jakob   MA\n3     156  Lisa  PhD"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#factor",
    "href": "slides/02_The_Essentials_of_R_Programming.html#factor",
    "title": "Data Science and Data Analytics",
    "section": "Factor",
    "text": "Factor\nA factor is R’s way to represent categorical data. Say, for example, we want to add the sex of the three people in our data set to the data.frame. For this, we create a factor and add it as an additional column to df:\n\nsex &lt;- factor(c(\"f\", \"m\", \"f\"))\ndf$sex &lt;- sex\ndf\n\n  heights names educ sex\n1     176  Anna  BSc   f\n2     187 Jakob   MA   m\n3     156  Lisa  PhD   f\n\n\n\nA factor has levels that represent the categories that this variable can take (here: “m” for male and “f” for female). In the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters.\n\ndf$sex\n\n[1] f m f\nLevels: f m"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#factor-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#factor-1",
    "title": "Data Science and Data Analytics",
    "section": "Factor",
    "text": "Factor\nA factor is another example of a data structure that is built on top of an atomic vector using a class attribute. The data is stored as an integer vector (data type), but because the object is of class factor, R uses different methods to act on the object (compared to a standard integer vector). This type of behaviour is one of the things that makes R very powerful for data science.\n\ntypeof(df$sex)\n\n[1] \"integer\"\n\nas.integer(df$sex)\n\n[1] 1 2 1\n\nclass(df$sex)\n\n[1] \"factor\"\n\ndf$sex\n\n[1] f m f\nLevels: f m\n\n\nAs we will see, properly encoding categorical data as factors is an essential step in data preparation."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#date",
    "href": "slides/02_The_Essentials_of_R_Programming.html#date",
    "title": "Data Science and Data Analytics",
    "section": "Date",
    "text": "Date\nFinally, a common type of data that needs to be represented in R are dates. Dates are typically represented in some sort of format like “DD-MM-YYYY” in the European context, for instance. Let’s say, we are given Anna’s, Jakob’s and Lisa’s birthday in different formats:\n\nbday_anna &lt;- \"1999-12-01\" # YYYY-MM-DD\nbday_jakob &lt;- \"16.4.98\" # MM.DD.YYYY\nbday_lisa &lt;- \"7/June/1995\" # DD/MM/YYYY\n\n\nR has a specific way of recognizing such date formats using so called format strings. For each way of representing date information, there is a corresponding format string. Some important ones are:\n\n\n\n\n\n\n\n\n\n\nFormat string\nDescription\nFormat string\nDescription\n\n\n\n\n%Y\nYear with century\n%y\nYear without century\n\n\n%m\nMonth of year (01-12)\n%j\nDay of the year (0-366)\n\n\n%d\nDay of month (01-31)\n%W\nCalendar week (0-53)\n\n\n%B\nFull month (e.g. June)\n%b\nAbbreviated month (e.g. Jun)"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates",
    "title": "Data Science and Data Analytics",
    "section": "Working with dates",
    "text": "Working with dates\nSo let’s try to use this to convert our dates (which are now just character objects) to actual dates that R understands using as.Date:\n\nbday_anna &lt;- as.Date(\"1999-12-01\", \"%Y-%m-%d\")\nbday_jakob &lt;- as.Date(\"16.4.98\", \"%d.%m.%y\")\nbday_lisa &lt;- as.Date(\"7/June/1995\", \"%d/%B/%Y\")\nbday_anna\n\n[1] \"1999-12-01\"\n\nbday_jakob\n\n[1] \"1998-04-16\"\n\nbday_lisa\n\n[1] \"1995-06-07\"\n\n\n\nNow, we can add these dates as a column to our data.frame\n\ndf$bday &lt;- c(bday_anna, bday_jakob, bday_lisa)\ndf\n\n  heights names educ sex       bday\n1     176  Anna  BSc   f 1999-12-01\n2     187 Jakob   MA   m 1998-04-16\n3     156  Lisa  PhD   f 1995-06-07"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-1",
    "title": "Data Science and Data Analytics",
    "section": "Working with dates",
    "text": "Working with dates\nDates are stored in R as the number of days since Unix time, which is the 1st January, 1970. Hence, date vectors are simply double vectors of class Date.\n\ntypeof(df$bday)\n\n[1] \"double\"\n\nas.double(df$bday)\n\n[1] 10926 10332  9288\n\nclass(df$bday)\n\n[1] \"Date\"\n\n\n\nWith this underlying representation, we can do maths on dates:\n\ndf$bday\n\n[1] \"1999-12-01\" \"1998-04-16\" \"1995-06-07\"\n\ndf$bday + 30\n\n[1] \"1999-12-31\" \"1998-05-16\" \"1995-07-07\"\n\ndf$bday[2] - df$bday[1]\n\nTime difference of -594 days"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-2",
    "title": "Data Science and Data Analytics",
    "section": "Working with dates",
    "text": "Working with dates\nNote that these operations cannot be performed on the original character strings representing the birthdays:\n\n\"1999-12-01\" + 10\n\nError in \"1999-12-01\" + 10: nicht-numerisches Argument für binären Operator\n\n\n\nTo extract individual parts of a date (like year, month, day, weekday, etc.), we can use the format function in conjunction with the format strings we saw before for date creation:\n\nformat(df$bday, \"%Y\") # extract birth year\n\n[1] \"1999\" \"1998\" \"1995\"\n\nformat(df$bday, \"%B\") # extract birth month\n\n[1] \"December\" \"April\"    \"June\"    \n\nformat(df$bday, \"%A\") # extract week day of birth date\n\n[1] \"Wednesday\" \"Thursday\"  \"Wednesday\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-3",
    "title": "Data Science and Data Analytics",
    "section": "Working with dates",
    "text": "Working with dates"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#r-programming-for-data-science",
    "href": "slides/02_The_Essentials_of_R_Programming.html#r-programming-for-data-science",
    "title": "Data Science and Data Analytics",
    "section": "R programming for Data Science",
    "text": "R programming for Data Science\n\nBy coding in R, we can efficiently perform exploratory data analysis, build machine learning pipelines, and prepare beautiful plots to communicate results effectively using data visualization tools.\nHowever, R is not just a data analysis environment but a programming language.\nThe goal of this is course is not to teach advanced R programming, but in order to facilitate our lives as data scientists, we do need access to certain core programming principles like conditionals, loops and functionals.\nSo let’s dive right in!"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#conditionals",
    "href": "slides/02_The_Essentials_of_R_Programming.html#conditionals",
    "title": "Data Science and Data Analytics",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals are one of the basic features of programming. They are used for what is called control flow. The most common conditional expression is the if-else statement. It’s best illustrated with an example:\n\na &lt;- 0\nif (a != 0) {\n  print(1/a)\n} else{\n  print(\"Can't divide by zero!\")\n}\n\n[1] \"Can't divide by zero!\"\n\n\n\nThe condition a != 0 was evaluated to be FALSE (since a was assigned 0). So, the conditional went to the else statement and printed “Can’t divide by zero!” to the console as we told it to. By contrast:\n\na &lt;- 4\nif (a != 0) {\n  print(1/a)\n} else{\n  print(\"Can't divide by zero!\")\n}\n\n[1] 0.25"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#conditionals-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#conditionals-1",
    "title": "Data Science and Data Analytics",
    "section": "Conditionals",
    "text": "Conditionals\nLet’s incorporate conditionals into our quadratic_solver function from before. Remember that a quadratic equation has no real solutions if the discriminant \\(D\\) is negative (because of the root in the “midnight” formula):\n\\[D = b^2 - 4ac\\]\n\nSo far, when this is case, our function produces NaNs and a warning:\n\nquadratic_solver(2, 2, 2)\n\nWarning in sqrt(b^2 - 4 * a * c): NaNs wurden erzeugt\nWarning in sqrt(b^2 - 4 * a * c): NaNs wurden erzeugt\n\n\n[1] NaN NaN\n\n\n\n\nLet’s fix this by:\n\nReturning no solutions / NA if \\(D &lt; 0\\).\nReturning one solution if \\(D = 0\\).\nReturning two solutions (as before) if \\(D &gt; 0\\)."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#conditionals-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#conditionals-2",
    "title": "Data Science and Data Analytics",
    "section": "Conditionals",
    "text": "Conditionals\nOne solution could be the following:\n\nquadratic_solver_new &lt;- function(a, b, c){\n  D &lt;- b^2 - 4*a*c\n  if(D &lt; 0){\n    print(\"Discriminant is negative, no real solutions!\")\n    return(NA)\n  } else if(D &gt; 0) {\n    print(\"Discriminant is positive, two real solutions!\")\n    sol_1 &lt;- (-b + sqrt(D))/(2*a)\n    sol_2 &lt;- (-b - sqrt(D))/(2*a)\n    return(c(sol_1, sol_2))\n  } else {\n    print(\"Discriminant is zero, one real solution!\")\n    sol &lt;- -b/(2*a)\n    return(sol)\n  }\n}\n\n\n\n\n\nquadratic_solver_new(2, 2, 2)\n\n[1] \"Discriminant is negative, no real solutions!\"\n\n\n[1] NA\n\nquadratic_solver_new(3, -5, 1)\n\n[1] \"Discriminant is positive, two real solutions!\"\n\n\n[1] 1.4342585 0.2324081\n\n\n\n\nquadratic_solver_new(1, 2, 1)\n\n[1] \"Discriminant is zero, one real solution!\"\n\n\n[1] -1"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#conditionals-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#conditionals-3",
    "title": "Data Science and Data Analytics",
    "section": "Conditionals",
    "text": "Conditionals\nif-else statements like the ones we saw only work on a single logical. For a vectorized version, there is the very useful ifelse function:\n\na &lt;- c(0, 2, 4, 6, 0, 8)\nifelse(a != 0, 1/a, NA)\n\n[1]        NA 0.5000000 0.2500000 0.1666667        NA 0.1250000\n\n\n\nThis function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. When operating on vectors, ifelse takes the corresponding elements of the second or third argument.\nHere’ another example:\n\nx &lt;- 1:10\nifelse(x %% 2 == 0, \"even\", \"odd\")\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nNote: x %% 2 gives the remainder when dividing x by 2."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#for-loops",
    "href": "slides/02_The_Essentials_of_R_Programming.html#for-loops",
    "title": "Data Science and Data Analytics",
    "section": "For-loops",
    "text": "For-loops\nIn general, loops control flow structures that enable the repeated execution of a code block as long as a specified condition is met. This saves a lot of manual work and code duplication. We will discuss two types of loops: for loops and while loops.\n\nfor loops are used to iterate over items in a vector. The logic is “for every item in this vector, do the following”. This logic is implemented in the following basic form:\n\nfor(item in vector) {\n  perform_action\n}\n\n\n\nFollowing this notation, we can refer to the element of the vector in the current loop cycle with the name of item. In the first cycle, it will be the first element of the vector, in the second, it will be the second, and so on…"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#for-loops-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#for-loops-1",
    "title": "Data Science and Data Analytics",
    "section": "For-loops",
    "text": "For-loops\nLet’s see an example of a for loop that prints both solutions to the quadratic equation from before in a nice format:\n\nsolutions &lt;- quadratic_solver_new(3, -5, 1)\n\n[1] \"Discriminant is positive, two real solutions!\"\n\nn &lt;- length(solutions)\n\nfor(i in 1:n){\n  print(paste0(\"Solution \", i, \": \", round(solutions[i], 3)))\n}\n\n[1] \"Solution 1: 1.434\"\n[1] \"Solution 2: 0.232\"\n\n\n\nNote that we do not have to loop through an integer vector, we can loop through any atomic vector or list. For example, we could loop through all the elements of the list l1 from earlier and print it to the console only if it is a numeric vector:\n\nfor(el in l1){\n  if(is.numeric(el)){\n    print(el)\n  }\n}\n\n[1] 1 2 3\n[1] 2.3 5.9"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#while-loops",
    "href": "slides/02_The_Essentials_of_R_Programming.html#while-loops",
    "title": "Data Science and Data Analytics",
    "section": "While-loops",
    "text": "While-loops\nAnother type of loop is a while loop. It repeats a specified action as long as a certain condition is met. It has the following basic form:\n\nwhile(condition){\n  perform_action\n}\n\n\nNote that for and while loops are interchangeable in every circumstance, i.e. every for loop can be implemented as a while loop and vice versa. Usually, the choice between the two is a question of code readability and efficiency considerations. Compare the following two examples:\n\n\n\ni &lt;- 1L\nwhile(i &lt;= 3L){\n  print(i)\n  i &lt;- i + 1L\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\nNote that the last step in the loop is crucial. Otherwise we loop infinitely!\n\n\nfor(i in 1:3){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#functionals",
    "href": "slides/02_The_Essentials_of_R_Programming.html#functionals",
    "title": "Data Science and Data Analytics",
    "section": "Functionals",
    "text": "Functionals\nIn R, a very commonly used alternative to loops is the use of functionals. Functionals are functions that take another function as an input and returns a vector as output. Common functionals we will have a look at are lapply, and apply. Let’s start with lapply.\n\nlapply requires as arguments an atomic vector or a list and a function that it should apply to each element of that atomic vector or list. Let’s say we wanted to sort each vector in a list of vectors. We could do:\n\nl2 &lt;- list(sample(1:10), sample(1:20))\nl2\n\n[[1]]\n [1] 10  9  5  4  7  1  3  6  8  2\n\n[[2]]\n [1]  5 14 20  2 17  8  1 16 18 15  7 11 12  4 19 10  3 13  9  6\n\nlapply(l2, sort)\n\n[[1]]\n [1]  1  2  3  4  5  6  7  8  9 10\n\n[[2]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#functionals-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#functionals-1",
    "title": "Data Science and Data Analytics",
    "section": "Functionals",
    "text": "Functionals\nIn fact, we can pass named arguments that we would usually pass to the function to be applied directly to lapply instead:\n\nlapply(l2, sort, decreasing = TRUE)\n\n[[1]]\n [1] 10  9  8  7  6  5  4  3  2  1\n\n[[2]]\n [1] 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3  2  1\n\n\n\nInstead of using lapply with a pre-existing (or user-defined) function, we can also create an inline function that exists only for the purpose of that lapply call. For example, we might want to square each vector in the list after sorting:\n\nlapply(l2, function(x) sort(x, decreasing = TRUE)^2)\n\n[[1]]\n [1] 100  81  64  49  36  25  16   9   4   1\n\n[[2]]\n [1] 400 361 324 289 256 225 196 169 144 121 100  81  64  49  36  25  16   9   4\n[20]   1\n\n\nSuch functions are called anonymous functions as they do not have a name."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#functionals-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#functionals-2",
    "title": "Data Science and Data Analytics",
    "section": "Functionals",
    "text": "Functionals\nThe matrix equivalent of lapply is called apply. It can apply a given function to every row and / or every column of a matrix. Besides requiring the matrix and the function to be applied, it requires an indication of whether application should happen over rows (1) or columns (2). Consider the following example to compute the row and column sums of a matrix:\n\nm &lt;- matrix(1:9, 3, 3)\nm\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\napply(m, 1, sum)\n\n[1] 12 15 18\n\napply(m, 2, sum)\n\n[1]  6 15 24\n\n\n\nThe use of functionals like lapply and apply is usually preferable over loops because it pre-specifies what should happen with the result (e.g. lapply will always hold the results in a list)."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#packages-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#packages-1",
    "title": "Data Science and Data Analytics",
    "section": "Packages",
    "text": "Packages\n\nWith the base installation, R comes with a lot of functionality already. However, statisticians and data scientists all over the world constantly come up with new methods and clever code to solve problems.\nTo share this code, they create R packages: these include fundamental units of reproducible R code, including reusable R functions, the documentation that describes how to use them, and sample data.\nTo make these packages available, the creators typically upload them to the Comprehensive R Archive Network (CRAN).\nFrom there, any R user can download and install these packages and use them for their own workflows."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#packages-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#packages-2",
    "title": "Data Science and Data Analytics",
    "section": "Packages",
    "text": "Packages\n\nAs of 22nd February 2025, there are 22,100 R packages available for download on CRAN. Each of them has a unique name that we can use to obtain it.\nWe will be working only with a select few of them.\nOne of them will be ggplot2, which is one of the most popular R packages used for data visualization.\nAs an R programmer, one typically finds packages that solve a given problem by googling the problem one is encountering. Chances are that someone else has had this problem before…\n\n\nTo install a package, we call the function install.packages on the name of the package:\n\ninstall.packages(\"ggplot2\")\n\nThis downloads and installs the package on your computer."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#packages-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#packages-3",
    "title": "Data Science and Data Analytics",
    "section": "Packages",
    "text": "Packages\nAs with any software, an R package only needs to be installed once.\n\nAfter installation, we need to tell R to make the functions provided by this package available to us in our current R session. For this we call the function library on the name of the package:\n\nlibrary(ggplot2)\n\n\nNow, all data and functions offered by this package can be used.\nA package comes with a reference manual and often additional documentation in the form of so called package vignettes.\nThis helps the user understand how to use the functions in the package.\nAdditionally, documentation on individual functions is always available through the help of the ? operator."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#packages-4",
    "href": "slides/02_The_Essentials_of_R_Programming.html#packages-4",
    "title": "Data Science and Data Analytics",
    "section": "Packages",
    "text": "Packages\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point() + \n  xlab(\"Engine Displacement\") + ylab(\"MPG\") + \n  theme(legend.text = element_text(size=14), axis.text = element_text(size=14), axis.title = element_text(size=14))"
  },
  {
    "objectID": "slides/06_penguins_presentation.html#meet-the-penguins",
    "href": "slides/06_penguins_presentation.html#meet-the-penguins",
    "title": "Palmer Penguins",
    "section": "Meet the penguins",
    "text": "Meet the penguins\n\nFor this analysis, we will use the penguins data set from the palmerpenguins package. The website belonging to this package can be found here.\nThe data set comprises measurements of three types of penguins, namely:\n\nAdelie 🐧\nChinstrap 🐧\nGentoo 🐧"
  },
  {
    "objectID": "slides/06_penguins_presentation.html#meet-the-penguins-1",
    "href": "slides/06_penguins_presentation.html#meet-the-penguins-1",
    "title": "Palmer Penguins",
    "section": "Meet the penguins",
    "text": "Meet the penguins\nFigure 1 shows a scatter plot of the flipper length against the body mass of all penguins in the data set:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n\nFigure 1: A scatter plot of flipper length against body mass"
  },
  {
    "objectID": "slides/00_Getting_started.html#welcome-to-data-science-and-data-analytics",
    "href": "slides/00_Getting_started.html#welcome-to-data-science-and-data-analytics",
    "title": "Data Science and Data Analytics",
    "section": "Welcome to Data Science and Data Analytics!",
    "text": "Welcome to Data Science and Data Analytics!\n\nThis is B-GV-12 Data Science and Data Analytics.\nThe goal of this course is to teach you…\n\nabout the nature and role of data science in a data-driven world.\nabout the data science workflow.\nto use open-source software to analyse, visualize and model data from various sources.\nabout different machine learning algorithms.\nto implement your own data science projects.\nto communicate the results of your analyses effectively.\nand much more…"
  },
  {
    "objectID": "slides/00_Getting_started.html#lectures",
    "href": "slides/00_Getting_started.html#lectures",
    "title": "Data Science and Data Analytics",
    "section": "Lectures",
    "text": "Lectures\n\nWith a few exceptions, lectures will be held on Fridays from 13:30 to 16:30.\nPlease regularly check your course schedule to not miss any lectures.\nLectures will consist of theory and practice discussed with the help of slides as well as live coding sessions.\nYou are highly encouraged to actively participate!\nExercises to practice what you have learned will be provided (but not graded)."
  },
  {
    "objectID": "slides/00_Getting_started.html#grading",
    "href": "slides/00_Getting_started.html#grading",
    "title": "Data Science and Data Analytics",
    "section": "Grading",
    "text": "Grading\n\nThere will be no final exam in this course!\nInstead, grading will be based on group projects:\n\nTeams of 4 - 5 students will initiate and design a small data science project autonomously.\nEach group will:\n\nidentify a business case mimicking a real-world research problem and an accompanying available data set.\nformulate research questions on the basis of the chosen data set.\nperform analyses using the concepts and methods learned throughout this course."
  },
  {
    "objectID": "slides/00_Getting_started.html#grading-1",
    "href": "slides/00_Getting_started.html#grading-1",
    "title": "Data Science and Data Analytics",
    "section": "Grading",
    "text": "Grading\n\nGrading will therefore be based on the following components:\n\nGroup project report (5-10 pages per group member): 65 %\nGroup presentation (\\(\\leq\\) 20 mins/group and \\(\\geq\\) 2 mins/group member): 25 %\nPeer review: 10 %\n\nIn line with the usual grading scheme, grades will be given as follows:\n\n\n\n\n\n\n\n\nPercentage\nGrade\n\n\n\n\n95 - 100 %\n1,0\n\n\n90 - 94 %\n1,3\n\n\n85 - 89 %\n1,7\n\n\n80 - 84 %\n2,0\n\n\n75 - 79 %\n2,3\n\n\n70 - 74 %\n2,7\n\n\n\n\n\n\n\nPercentage\nGrade\n\n\n\n\n65 - 69 %\n3,0\n\n\n60 - 64 %\n3,3\n\n\n55 - 59 %\n3,7\n\n\n50 - 54 %\n4,0\n\n\nbelow 50 %\n5,0"
  },
  {
    "objectID": "slides/00_Getting_started.html#grading-group-project",
    "href": "slides/00_Getting_started.html#grading-group-project",
    "title": "Data Science and Data Analytics",
    "section": "Grading: Group project",
    "text": "Grading: Group project\n\nThe structure of the group project (reflected in report and presentation) should be something like this:\n\nIntroduction / Motivation and research question\nData (sources, description, statistics, visualizations, …)\nModels and model evaluation\nResults\nDiscussion and comments\n\nAspects that will influence your grade will be: the originality of the question, understanding of the business case, data and methods, correctness of application, thoroughness of evaluation, creativity and quality of report and presentation (both verbal and visual)\nDeductions will be made for purely AI-generated contributions."
  },
  {
    "objectID": "slides/00_Getting_started.html#grading-group-project-1",
    "href": "slides/00_Getting_started.html#grading-group-project-1",
    "title": "Data Science and Data Analytics",
    "section": "Grading: Group project",
    "text": "Grading: Group project\n\n\nChoice of topic: while you are completely free in your choice of topic in the group, here are some areas of suggestion:\n\nFinance / Economics / Marketing\nText analysis\nEntertainment (in particular: movies and music)\nSocial network analysis\nSocial sciences\n\n\nSources for data sets: while you are again free also in your choice of data set, good places to get you started are:\n\nStatistik Austria\nKaggle\nUCI Machine learning repository\nWorld bank\nEU\n…\n\n\n\n\n\n\n\n\n\nCaution\n\n\nWhen selecting a data set, make sure, you are allowed to use this data for the purposes of your project! When selecting from the sources given, this should generally be ensured."
  },
  {
    "objectID": "slides/00_Getting_started.html#grading-peer-review",
    "href": "slides/00_Getting_started.html#grading-peer-review",
    "title": "Data Science and Data Analytics",
    "section": "Grading: Peer review",
    "text": "Grading: Peer review\n\n\nAfter final project presentations, each individual student will be asked to write a peer review of one of the other groups’ projects.\nEach student will be randomly assigned two projects, out of which one should be reviewed (based on their presentation only).\nEvaluation is based on the quality of review that you write, not on the feedback that your project receives.\nThe review should be max 250 words answering the following questions:\n\nBriefly describe the topic, research questions and the employed methods.\nState and briefly explain two positive comments about the work.\nState and briefly explain two improvement suggestions.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nIn your peer review, focus on content, not on the formatting or quality of the slides, for instance."
  },
  {
    "objectID": "slides/00_Getting_started.html#schedule",
    "href": "slides/00_Getting_started.html#schedule",
    "title": "Data Science and Data Analytics",
    "section": "Schedule",
    "text": "Schedule\n\n\nMarch\n14th: Getting started, Introduction to Data Science (DS)\n21st: The essentials of R programming\n31st: The DS workflow – Part I: Import, Tidy and Transform\n\nApril\n4th/11th: The DS workflow – Part II: Visualize\n30th: The DS workflow – Part III: Model\n\nMay\n6th/9th/16th/23rd: The DS workflow – Part III: Model\n28th: The DS workflow – Part IV: Communicate\n\nJune\n6th: Buffer session\n13th: Final presentations of the group projects\n\n\n\n\n\n\n\nDeadlines\n\n\n\n21st March: organize in teams, send team members via e-mail\n12th June: Send presentations via e-mail\n27th June: Hand in group project reports and peer reviews"
  },
  {
    "objectID": "slides/00_Getting_started.html#questions-and-contact",
    "href": "slides/00_Getting_started.html#questions-and-contact",
    "title": "Data Science and Data Analytics",
    "section": "Questions and contact",
    "text": "Questions and contact\n\nAny questions?\n\n\n\nContact:\n\nAnytime via e-mail: julianamonphd@gmail.com"
  },
  {
    "objectID": "slides/00_Getting_started.html#software-excel",
    "href": "slides/00_Getting_started.html#software-excel",
    "title": "Data Science and Data Analytics",
    "section": "Software – Excel? ❌",
    "text": "Software – Excel? ❌"
  },
  {
    "objectID": "slides/00_Getting_started.html#software-r",
    "href": "slides/00_Getting_started.html#software-r",
    "title": "Data Science and Data Analytics",
    "section": "Software – R ✅",
    "text": "Software – R ✅"
  },
  {
    "objectID": "slides/00_Getting_started.html#software-rstudio",
    "href": "slides/00_Getting_started.html#software-rstudio",
    "title": "Data Science and Data Analytics",
    "section": "Software – RStudio ✅",
    "text": "Software – RStudio ✅"
  },
  {
    "objectID": "slides/00_Getting_started.html#software-quarto",
    "href": "slides/00_Getting_started.html#software-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Software – Quarto ✅",
    "text": "Software – Quarto ✅"
  },
  {
    "objectID": "slides/00_Getting_started.html#software",
    "href": "slides/00_Getting_started.html#software",
    "title": "Data Science and Data Analytics",
    "section": "Software",
    "text": "Software\n\nModern data science is unthinkable without computer programming: typically, either Python or R is used.\nFor the purposes of this course, we will use:\n\nThe open-source statistical programming language R.\nA bespoke integrated development environment (IDE) for R called RStudio.\nAn authoring framework for creating beautiful reports, presentations, web sites, etc., combining text, code, results and visualizations, called Quarto.\n\nUntil next time, therefore please\n\neither install R, RStudio and Quarto on your laptop (recommended) or\nregister for a free account at Posit Cloud."
  },
  {
    "objectID": "slides/00_Getting_started.html#resources",
    "href": "slides/00_Getting_started.html#resources",
    "title": "Data Science and Data Analytics",
    "section": "Resources",
    "text": "Resources\n\nPrimarily: slides and exercises provided\nHowever, for a deeper dive and additional materials, I recommend:\n\n\n\nFor R programming\n  Excellent courses from Harvard Professor Rafael Irizarry, available for free here and here.\n\nFor machine learning\n Excellent book, available for free here."
  },
  {
    "objectID": "slides/00_Getting_started.html#resources-how-about-ai",
    "href": "slides/00_Getting_started.html#resources-how-about-ai",
    "title": "Data Science and Data Analytics",
    "section": "Resources – How about AI?",
    "text": "Resources – How about AI?\n\n\nWith the large-scale adoption of AI tools like ChatGPT, the way data scientists work is rapidly changing.\nThis course therefore actively encourages the use of AI tools for R programming. Here are some guidelines:\n\nUse ChatGPT for programming, not for writing the project report.\nDo not just copy-paste code generated by ChatGPT. Run it line-by-line, try to understand and edit as needed.\nEngineer your prompts until the response starts to look like code you are learning in this course.\nIf the response is not correct, ask for a correction.\n\nWith the arrival of AI, programming is becoming ever more accessible, but the need for people like you who actually understand the code they are running, is also increasing."
  },
  {
    "objectID": "slides/00_Getting_started.html#resources-how-about-ai-1",
    "href": "slides/00_Getting_started.html#resources-how-about-ai-1",
    "title": "Data Science and Data Analytics",
    "section": "Resources – How about AI?",
    "text": "Resources – How about AI?"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#the-data-science-workflow-import",
    "href": "slides/03_The_Data_Science_Workflow_I.html#the-data-science-workflow-import",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Import",
    "text": "The Data Science workflow – Import"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-into-r",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-into-r",
    "title": "Data Science and Data Analytics",
    "section": "Importing data into R",
    "text": "Importing data into R\n\nIn order to get our data science workflow started, we need to be able to import data from different sources into R.\nWhile there is a huge number of different data formats, we will focus on how to import data stored in two of the most common types of formats, namely:\n\nText files (like csv or tsv)\nSpreadsheets (Excel or Google Sheets)\n\nBefore we dive into how to get the data stored in such files into R, we need to be able to find these files on our computer in the first place…\nFor this, we first have to look into how R orients itself in the file system on our computer."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#the-file-system",
    "href": "slides/03_The_Data_Science_Workflow_I.html#the-file-system",
    "title": "Data Science and Data Analytics",
    "section": "The file system",
    "text": "The file system\n\nA computer’s file system consists of nested folders (directories). It can be visualized as tree structures, with directories branching out from the root.\nThe root directory contains all other directories.\nThe working directory is the current location in the filesystem."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#relative-and-full-paths",
    "href": "slides/03_The_Data_Science_Workflow_I.html#relative-and-full-paths",
    "title": "Data Science and Data Analytics",
    "section": "Relative and full paths",
    "text": "Relative and full paths\n\nA path lists directory names leading to a file. Think of it like instructions on what folders to click on, and in what order, to find the file. We distinguish:\n\nFull paths: Starts from the root directory, i.e. the very top of the file system hierarchy. An example would be:\n\n\nexample_path &lt;- system.file(package = \"dslabs\")\nexample_path\n\n[1] \"/home/julian/R/x86_64-pc-linux-gnu-library/4.5/dslabs\"\n\n\n\nRelative paths: Starts from the current working directory. Imagine the current working directory would be /home/julian, then the relative path to the folder above would simply be:\n\n\n\n[1] \"R/x86_64-pc-linux-gnu-library/4.5/dslabs\"\n\n\nIn R, we can use the list.files function to explore directories:\n\nlist.files(example_path)\n\n [1] \"data\"        \"DESCRIPTION\" \"extdata\"     \"help\"        \"html\"       \n [6] \"INDEX\"       \"Meta\"        \"NAMESPACE\"   \"R\"           \"script\""
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#relative-and-full-paths-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#relative-and-full-paths-1",
    "title": "Data Science and Data Analytics",
    "section": "Relative and full paths",
    "text": "Relative and full paths"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#the-working-directory",
    "href": "slides/03_The_Data_Science_Workflow_I.html#the-working-directory",
    "title": "Data Science and Data Analytics",
    "section": "The working directory",
    "text": "The working directory\n\nWhen referring to files in your R script, it is highly recommended that you use relative paths.\nReason: paths are unique to your computer, so if someone else runs your code on their computer, it will not find files whose location is described by absolute paths.\nTo determine the working directory of your current R session, you can type:\n\ngetwd()\n\n[1] \"/home/julian/Dokumente/Projekte/Lehre/CFPU Data Science und Data Analytics/cfpu-ds-da/slides\"\n\n\nTo change the working directory, use the function setwd:\n\nsetwd(\"/path/to/your/directory\")\n\nIn RStudio, you can alternatively also select the working directory via Session &gt; Set Working Directory."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#generating-path-names",
    "href": "slides/03_The_Data_Science_Workflow_I.html#generating-path-names",
    "title": "Data Science and Data Analytics",
    "section": "Generating path names",
    "text": "Generating path names\n\nDifferent operating systems have different conventions when specifying paths.\nFor example, Linux and Mac use forward slashes /, while Windows uses backslashes \\ to separate directories.\nThe R function file.path combines characters to form a complete path, automatically ensuring compatibility with the respective operating system.\nThis function is useful because we often want to define paths using a variable.\nConsider the following example:\n\ndir &lt;- system.file(package = \"dslabs\")\nfile.path(dir, \"extdata\", \"murders.csv\")\n\n[1] \"/home/julian/R/x86_64-pc-linux-gnu-library/4.5/dslabs/extdata/murders.csv\"\n\n\nHere the variable dir contains the full path for the dslabs package (needs to be installed!) and extdata/murders.csv is the relative path of a specific csv file in that folder."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files",
    "text": "Importing data from text files\n\nAll of us know text files. They are easy to open, can be easily read by humans and are easily transferable.\nWhen text files are used to store tabular data, line breaks are used to separate rows and a predefined character (the so-called delimiter) is used to separate columns within a row. Which one is used can depend on the file format:\n\n.csv (comma-separated values) typically uses comma (,) or semicolon (;).\n.tsv (tab-separated values) typically uses tab (which can be a preset number of spaces or \\t).\n.txt (“text”) can use any of the above or a simple space ( )\n\nHow we read text files into R depends on the delimiter used. Therefore, we need to have a look at the file to determine the delimiter."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-1",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files",
    "text": "Importing data from text files\nThis is the first couple of lines of the murders.csv text file from the dslabs package we saw referenced before. It contains the number of gun murders in each US state in the year 2010 as well as each state’s population. Clearly, it uses commas (,) as delimiter. Also note the use of a header in the first row."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.csv",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.csv",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .csv",
    "text": "Importing data from text files – .csv\n\nFor comma-delimited csv files, R offers the function read.csv to run on the (full or relative) path of the file. By default, it assumes that decimal points are used and that a header giving column names is present:\n\nargs(read.csv)\n\nfunction (file, header = TRUE, sep = \",\", quote = \"\\\"\", dec = \".\", \n    fill = TRUE, comment.char = \"\", ...) \nNULL\n\n\nFor semicolon-delimited csv files, R offers the function read.csv2 to run on the (full or relative) path of the file. By default, it assumes that decimal commas are used and that a header giving column names is present.\n\nargs(read.csv2)\n\nfunction (file, header = TRUE, sep = \";\", quote = \"\\\"\", dec = \",\", \n    fill = TRUE, comment.char = \"\", ...) \nNULL\n\n\nBoth of these functions return a data.frame containing the data from the file."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.csv-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.csv-1",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .csv",
    "text": "Importing data from text files – .csv\nAs murders.csv is comma-delimited, we use read.csv to read it into R:\n\ndir &lt;- system.file(package = \"dslabs\")\nmurders_df &lt;- read.csv(file.path(dir, \"extdata\", \"murders.csv\"))\nhead(murders_df, 6) # shows the first 6 rows of the data frame\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\n\nNote that the categorical variables (state, abb and region) are imported as character vectors:\n\nunlist(lapply(murders_df, typeof))\n\n      state         abb      region  population       total \n\"character\" \"character\" \"character\"   \"integer\"   \"integer\" \n\n\nFor data analysis purposes, we should probably turn these into factors. But for now, we are only interested in the successful import."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.tsv",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.tsv",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .tsv",
    "text": "Importing data from text files – .tsv\n\nFor tab-delimited tsv files, R offers the functions read.delim and read.delim2, again assuming the use of decimal points and decimal commas, respectively:\n\nargs(read.delim)\n\nfunction (file, header = TRUE, sep = \"\\t\", quote = \"\\\"\", dec = \".\", \n    fill = TRUE, comment.char = \"\", ...) \nNULL\n\nargs(read.delim2)\n\nfunction (file, header = TRUE, sep = \"\\t\", quote = \"\\\"\", dec = \",\", \n    fill = TRUE, comment.char = \"\", ...) \nNULL\n\n\nOtherwise, the use is identical to read.csv: the function requires the path to the file you want to import and returns a data.frame containing the (hopefully) correctly parsed data from the file."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.tsv-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.tsv-1",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .tsv",
    "text": "Importing data from text files – .tsv"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.txt",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.txt",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .txt",
    "text": "Importing data from text files – .txt\n\nIn fact, all of the functions for importing data discussed so far are just interfaces to the R function read.table, which provides the most flexibility when importing data from text files:\n\nargs(read.table)\n\nfunction (file, header = FALSE, sep = \"\", quote = \"\\\"'\", dec = \".\", \n    numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"), row.names, \n    col.names, as.is = !stringsAsFactors, tryLogical = TRUE, \n    na.strings = \"NA\", colClasses = NA, nrows = -1, skip = 0, \n    check.names = TRUE, fill = !blank.lines.skip, strip.white = FALSE, \n    blank.lines.skip = TRUE, comment.char = \"#\", allowEscapes = FALSE, \n    flush = FALSE, stringsAsFactors = FALSE, fileEncoding = \"\", \n    encoding = \"unknown\", text, skipNul = FALSE) \nNULL\n\n\n(As always, to see the meaning of all of these arguments, see ?read.table)\nThis function is mostly used directly, when importing data from generic .txt files, where the format is often less strictly adhered to than in csv or tsv files."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#encoding",
    "href": "slides/03_The_Data_Science_Workflow_I.html#encoding",
    "title": "Data Science and Data Analytics",
    "section": "Encoding",
    "text": "Encoding\nNow, we know how to import data into R. So we download some data set, read it into R using the correct function, but then this happens…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#encoding-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#encoding-1",
    "title": "Data Science and Data Analytics",
    "section": "Encoding",
    "text": "Encoding\n\nSuch issues occur because of an incorrectly identified file encoding.\nEncoding refers to how the computer stores character strings as binary 0s and 1s. Examples of encoding systems are:\n\nASCII: uses 7 bits to represent symbols, enough for all English keyboard characters, but not much more…\nUnicode (especially UTF-8): the de-facto standard encoding of the internet, able to represent everything from the English alphabet to German Umlaute to Chinese characters and emojis.\n\nWhen reading a text file into R, its encoding needs to be known, otherwise the import either fails (see previous slide) or produces gibberish (e.g. German “Höhe” \\(\\to\\) “HÃ¶he”)."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#encoding-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#encoding-2",
    "title": "Data Science and Data Analytics",
    "section": "Encoding",
    "text": "Encoding\n\nRStudio typically uses UTF-8 as its default, which works in most cases. If it does not, you can use the guess_encoding function of the readr package to get insight into the encoding.\n\nlibrary(readr)\nweird_filepath &lt;- file.path(\"../data\", \"calificaciones.csv\")\nas.data.frame(guess_encoding(weird_filepath))\n\n    encoding confidence\n1 ISO-8859-1       0.92\n2 ISO-8859-2       0.72\n3 ISO-8859-9       0.53\n\n\nThe function deems the ISO-8859-1 encoding to be the most likely encoding of the previous file. So we pass this value as the fileEncoding argument to read.csv2:\n\nhead(read.csv2(weird_filepath, sep = \",\", fileEncoding = \"ISO-8859-1\"), 3)\n\n    nombre                     f.n.             estampa puntuación\n1  Beyoncé 04 de septiembre de 1981 2023-09-22 02:11:02       87.5\n2 Blümchen      20 de abril de 1980 2023-09-22 03:23:05       99.0\n3     João      10 de junio de 1931 2023-09-21 22:43:28       98.9\n\n\nThis time, it worked! 😊"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\n\nAnother common way of sharing tabular data is through the use of spreadsheets, like Excel or Google Sheets. We will see how to import data in both of these types of documents.\nWith Excel, spreadsheets typically either have a .xls or .xlsx file suffix. Note that those are binary file formats, i.e. unlike text files, they are not human-readable when opened with a text editor.\nBase R does not have functionality to import data from Excel spreadsheets. However, the package readxl does. Its two main functions are:\n\nread_xls to read Excel spreadsheets with .xls ending and\nread_xlsx to read Excel spreadsheets with .xlsx ending.\n\nThese functions allow to select only certain areas of certain sheets, transform data types and much more…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-1",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\nConsider the following simple example. Suppose we have the following .xlsx-spreadsheet of famous people that died in 2016:"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-2",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\n\nNote how we only want to import the range A5:F15 in the sheet called other. We can pass these values as the corresponding arguments to the read_xlsx function:\n\nlibrary(readxl)\nargs(read_xlsx)\n\nfunction (path, sheet = NULL, range = NULL, col_names = TRUE, \n    col_types = NULL, na = \"\", trim_ws = TRUE, skip = 0, n_max = Inf, \n    guess_max = min(1000, n_max), progress = readxl_progress(), \n    .name_repair = \"unique\") \nNULL\n\n\nHence, to import this data, we should call:\n\ndir &lt;- system.file(package = \"readxl\")\nxlsx_filepath &lt;- file.path(dir, \"extdata\", \"deaths.xlsx\")\nhead(as.data.frame(read_xlsx(xlsx_filepath, sheet = \"other\", range = \"A5:F15\")), 5)\n\n            Name Profession Age Has kids Date of birth Date of death\n1     Vera Rubin  scientist  88     TRUE    1928-07-23    2016-12-25\n2    Mohamed Ali    athlete  74     TRUE    1942-01-17    2016-06-03\n3   Morley Safer journalist  84     TRUE    1931-11-08    2016-05-19\n4   Fidel Castro politician  90     TRUE    1926-08-13    2016-11-25\n5 Antonin Scalia     lawyer  79     TRUE    1936-03-11    2016-02-13"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-3",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-3",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\n\nGoogle Sheets is another widely used spreadsheet program, which is free and web-based. Just like with Excel, in Google Sheets data are organized in worksheets (also called sheets) inside of spreadsheet files.\nAgain, Base R does not have functionality to import data from Google Sheets spreadsheets. However, the package googlesheets4 does. Its main function is read_sheet, which reads a Google Sheet from a URL or a file id.\nGiven such a URL, its use is very similar to read_xlsx:\n\nlibrary(googlesheets4)\n\ngs4_deauth() # used to read publicly available Google Sheets\n# Obtaining data on global life expectancy since 1800 from the Gapminder project\n# For more information, see http://gapm.io/dlex\nsheet_id &lt;- \"1RehxZjXd7_rG8v2pJYV6aY0J3LAsgUPDQnbY4dRdiSs\"\n\ndf_gs &lt;- as.data.frame(read_sheet(sheet_id,\n                                  sheet = \"data-for-world-by-year\",\n                                  range = \"A1:D302\"))\n\n✔ Reading from \"_GM-Life Expectancy- Dataset - v14\".\n\n\n✔ Range ''data-for-world-by-year'!A1:D302'."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-4",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-4",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\nNow, the data has successfully been imported:\n\nlibrary(knitr)\nkable(head(df_gs, 8)) # the function kable creates nice tables for presentations\n\n\n\n\ngeo\nname\ntime\nLife expectancy\n\n\n\n\nworld\nWorld\n1800\n30.64173\n\n\nworld\nWorld\n1801\n30.71239\n\n\nworld\nWorld\n1802\n30.60052\n\n\nworld\nWorld\n1803\n30.27759\n\n\nworld\nWorld\n1804\n30.19749\n\n\nworld\nWorld\n1805\n30.78082\n\n\nworld\nWorld\n1806\n30.79082\n\n\nworld\nWorld\n1807\n30.73985"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-in-other-formats",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-in-other-formats",
    "title": "Data Science and Data Analytics",
    "section": "Importing data in other formats",
    "text": "Importing data in other formats\n\nThere is a lot of other data formats, which can be read into R with the help of other packages. The following provides a brief and inevitably incomplete overview:\n\nPackage haven for data from SPSS, Stata or SAS.\nPackage DBI along with a DBMS-specific backend allows you to run SQL queries on a data base and obtain the result as a data.frame directly in R.\nPackage jsonline for importing JSON files.\nPackage xml2 for importing XML files.\n…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#the-data-science-workflow-tidy-and-transform",
    "href": "slides/03_The_Data_Science_Workflow_I.html#the-data-science-workflow-tidy-and-transform",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Tidy and Transform",
    "text": "The Data Science workflow – Tidy and Transform"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-and-transform-data-wrangling",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-and-transform-data-wrangling",
    "title": "Data Science and Data Analytics",
    "section": "Tidy and Transform – Data wrangling",
    "text": "Tidy and Transform – Data wrangling\n\nAfter we imported data into R, we want to make it easily processable for visualizations or model building. This process could involve:\n\nrenaming columns to avoid confusion and unnecessary typing.\nsubsetting the data to use only parts of it, i.e. filtering.\nhandling incorrect and/or missing values.\naggregating the data to compute summary statistics.\nreshaping the data to suit the needs of functions operating on them.\nadding or replacing columns.\njoining other data sets to enrich the information presented.\nand much more…\n\nJointly, we refer to these tidying and transformation tasks as data wrangling."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-and-transform-data-wrangling-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-and-transform-data-wrangling-1",
    "title": "Data Science and Data Analytics",
    "section": "Tidy and Transform – Data wrangling",
    "text": "Tidy and Transform – Data wrangling"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#example-data-set",
    "href": "slides/03_The_Data_Science_Workflow_I.html#example-data-set",
    "title": "Data Science and Data Analytics",
    "section": "Example data set",
    "text": "Example data set\nLet’s import some data that we will be using as an example:\n\nflights &lt;- read.csv(file.path(\"../data\", \"nyc13_flights.csv\"))\nhead(flights)\n\n  year month day actual.time.of.departure scheduled.time.of.departure\n1 2013     1   1                      517                         515\n2 2013     1   1                      533                         529\n3 2013     1   1                      542                         540\n4 2013     1   1                      544                         545\n5 2013     1   1                      554                         600\n6 2013     1   1                      554                         558\n  depature.delay actual.time.of.arrival scheduled.time.of.arrival arrival.delay\n1              2                    830                       819            11\n2              4                    850                       830            20\n3              2                    923                       850            33\n4             -1                   1004                      1022           -18\n5             -6                    812                       837           -25\n6             -4                    740                       728            12\n  carrier flight plane.tail.number origin destination air.time distance\n1      UA   1545            N14228    EWR         IAH      227     1400\n2      UA   1714            N24211    LGA         IAH      227     1416\n3      AA   1141            N619AA    JFK         MIA      160     1089\n4      B6    725            N804JB    JFK         BQN      183     1576\n5      DL    461            N668DN    LGA         ATL      116      762\n6      UA   1696            N39463    EWR         ORD      150      719\n            time.hour\n1 2013-01-01 05:00:00\n2 2013-01-01 05:00:00\n3 2013-01-01 05:00:00\n4 2013-01-01 05:00:00\n5 2013-01-01 06:00:00\n6 2013-01-01 05:00:00\n\n\nThis data was adapted from the nycflights13 package. It contains information on all 166,158 domestic flights that departed from New York City (airports EWR, JFK and LGA) in the first six months of 2013. The data itself originates from the US Bureau of Transportation Statistics."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#column-naming",
    "href": "slides/03_The_Data_Science_Workflow_I.html#column-naming",
    "title": "Data Science and Data Analytics",
    "section": "Column naming",
    "text": "Column naming\nThe columns of this data set have quite long and descriptive names. Column names generally should:\n\nbe as short as possible.\nbe as long as necessary to be descriptive enough.\nnot use spaces or special characters.\nonly contain lowercase letters.\nuse snake_case for multiple words.\n\nThese rules are good to keep in mind also when naming any other R object. Let’s see what kind of information is contained in our example data set and how we could name the corresponding columns appropriately."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#column-naming-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#column-naming-1",
    "title": "Data Science and Data Analytics",
    "section": "Column naming",
    "text": "Column naming\n\nOur example data set contains the following columns:\n\n\n\nyear, month, day: date of departure\nactual.time.of.departure, scheduled.time.of.departure: actual and scheduled time of departure (in format HHMM or HMM). Too long, how about dep_time and sched_dep_time?\nactual.time.of.arrival, scheduled.time.of.arrival: actual and scheduled time of arrival (in format HHMM or HMM). Too long, how about arr_time and sched_arr_time?\ndeparture.delay, arrival.delay: departure and arrival delays, in minutes. To be consistent, how about dep_delay and arr_delay?\ncarrier, flight: airline and flight number.\nplane.tail.number: plane tail number. Too long, how about tailnum?\norigin, destination: origin and destination airport. origin fine, but maybe dest?\nair.time: amount of time spent in the air, in minutes. Change to air_time?\ndistance: distance between airports, in miles.\ntime.hour: scheduled date and hour of the flight. Change to time_hour?"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#column-naming-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#column-naming-2",
    "title": "Data Science and Data Analytics",
    "section": "Column naming",
    "text": "Column naming\nTo reset the column names of a data.frame, we can either use colnames or names:\n\nhead(names(flights))\n\n[1] \"year\"                        \"month\"                      \n[3] \"day\"                         \"actual.time.of.departure\"   \n[5] \"scheduled.time.of.departure\" \"depature.delay\"             \n\nnames(flights) &lt;- c(\"year\", \"month\", \"day\", \"dep_time\", \"sched_dep_time\", \"dep_delay\",\n                    \"arr_time\", \"sched_arr_time\", \"arr_delay\", \"carrier\", \"flight\",\n                    \"tail_num\", \"origin\", \"dest\", \"air_time\", \"distance\", \"time_hour\")\nhead(flights)\n\n  year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1 2013     1   1      517            515         2      830            819\n2 2013     1   1      533            529         4      850            830\n3 2013     1   1      542            540         2      923            850\n4 2013     1   1      544            545        -1     1004           1022\n5 2013     1   1      554            600        -6      812            837\n6 2013     1   1      554            558        -4      740            728\n  arr_delay carrier flight tail_num origin dest air_time distance\n1        11      UA   1545   N14228    EWR  IAH      227     1400\n2        20      UA   1714   N24211    LGA  IAH      227     1416\n3        33      AA   1141   N619AA    JFK  MIA      160     1089\n4       -18      B6    725   N804JB    JFK  BQN      183     1576\n5       -25      DL    461   N668DN    LGA  ATL      116      762\n6        12      UA   1696   N39463    EWR  ORD      150      719\n            time_hour\n1 2013-01-01 05:00:00\n2 2013-01-01 05:00:00\n3 2013-01-01 05:00:00\n4 2013-01-01 05:00:00\n5 2013-01-01 06:00:00\n6 2013-01-01 05:00:00"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#checking-data-types",
    "href": "slides/03_The_Data_Science_Workflow_I.html#checking-data-types",
    "title": "Data Science and Data Analytics",
    "section": "Checking data types",
    "text": "Checking data types\nNext, we should make sure that the data type and/or class used for each variable suits the data presented in this variable. In particular:\n\nEach numerical variable should of type integer or double.\nEach categorical variable should be a factor.\nEach non-categorical text-based variable should be of type character.\nEach date / date-time variable should be a Date or a POSIXct, respectively.\n\nLet’s have a look in our data set:\n\nsapply(flights, typeof)\n\n          year          month            day       dep_time sched_dep_time \n     \"integer\"      \"integer\"      \"integer\"      \"integer\"      \"integer\" \n     dep_delay       arr_time sched_arr_time      arr_delay        carrier \n     \"integer\"      \"integer\"      \"integer\"      \"integer\"    \"character\" \n        flight       tail_num         origin           dest       air_time \n     \"integer\"    \"character\"    \"character\"    \"character\"      \"integer\" \n      distance      time_hour \n     \"integer\"    \"character\""
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#adapting-data-types",
    "href": "slides/03_The_Data_Science_Workflow_I.html#adapting-data-types",
    "title": "Data Science and Data Analytics",
    "section": "Adapting data types",
    "text": "Adapting data types\nMost of the data types in our data set seem appropriate, however we should:\n\nredefine carrier, tail_num, origin and dest to be a factor.\nredefine time_hour to be a POSIXct date-time.\n\nUsing the function factor, the first part should be no problem:\n\nfactor_vars &lt;- c(\"carrier\", \"tail_num\", \"origin\", \"dest\")\nfor(var in factor_vars){\n  flights[[var]] &lt;- factor(flights[[var]])\n  print(head(flights[[var]]))\n}\n\n[1] UA UA AA B6 DL UA\nLevels: 9E AA AS B6 DL EV F9 FL HA MQ OO UA US VX WN YV\n[1] N14228 N24211 N619AA N804JB N668DN N39463\n3825 Levels: D942DN N0EGMQ N10156 N102UW N103US N104UW N10575 N105UW ... N9EAMQ\n[1] EWR LGA JFK JFK LGA EWR\nLevels: EWR JFK LGA\n[1] IAH IAH MIA BQN ATL ORD\n100 Levels: ABQ ACK ALB ATL AUS AVL BDL BGR BHM BNA BOS BQN BTV BUF BUR ... XNA"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#date-times",
    "href": "slides/03_The_Data_Science_Workflow_I.html#date-times",
    "title": "Data Science and Data Analytics",
    "section": "Date-times",
    "text": "Date-times\n\nIn the previous lecture, we have talked about dates in R, but we have not talked about how to represent time.\nAs simple as time seem to be, representing time-related information with a computer can be incredibly complex. Think about:\n\ntime zones (changing in geographical composition over time),\ndaylight saving time (DST) and its relevance in different countries,\ndifferent formats for writing dates and times (e.g. 16:00 vs. 4:00 pm),\n…\n\nIn this course, we will fortunately stick to relatively simple cases. However, data in the real world does not always behave that nicely…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#date-times-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#date-times-1",
    "title": "Data Science and Data Analytics",
    "section": "Date-times",
    "text": "Date-times\n\nThis map partitions the world into regions where local clocks all show the same time and have done so since 1970. Talk about complexity…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#date-time-in-r-posixct",
    "href": "slides/03_The_Data_Science_Workflow_I.html#date-time-in-r-posixct",
    "title": "Data Science and Data Analytics",
    "section": "Date-time in R – POSIXct",
    "text": "Date-time in R – POSIXct\nOne way of representing date-time information in R is with the POSIXct class. Just as with dates, we can use format strings also to identify hour, minute, second, time zone, etc.\nIn our flights data set, the variable time_hour has a relatively simple format:\n\nhead(flights$time_hour)\n\n[1] \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\"\n[4] \"2013-01-01 05:00:00\" \"2013-01-01 06:00:00\" \"2013-01-01 05:00:00\"\n\n\n\nAdditionally, we know that these are all times from the NYC time zone. In R, we can make use of a data base of time zones with the help of the function OlsonNames, named after the original creator Arthur David Olson. In there, there is a time zone called America/New_York:\n\nOlsonNames()[170]\n\n[1] \"America/Nassau\""
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#date-time-in-r-posixct-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#date-time-in-r-posixct-1",
    "title": "Data Science and Data Analytics",
    "section": "Date-time in R – POSIXct",
    "text": "Date-time in R – POSIXct\nNow, we can use the appropriate format string and time zone name to turn the time_hour variable into a POSIXct date-time. For this purpose, we require the function as.POSIXct:\n\nflights$time_hour &lt;- as.POSIXct(flights$time_hour,\n                                tz = \"America/New_York\",\n                                format = \"%Y-%m-%d %H:%M:%S\")\ntypeof(flights$time_hour)\n\n[1] \"double\"\n\nclass(flights$time_hour)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nhead(flights$time_hour, 2)\n\n[1] \"2013-01-01 05:00:00 EST\" \"2013-01-01 05:00:00 EST\"\n\ntail(flights$time_hour, 2)\n\n[1] \"2013-06-30 20:00:00 EDT\" \"2013-06-30 19:00:00 EDT\"\n\n\nNote how these POSIXct date-times now have EST (Eastern Standard Time) or EDT (Eastern Daylight Time) on them to indicate the time zone. as.POSIXct automatically applied the daylight saving time for the correct period."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#filtering",
    "href": "slides/03_The_Data_Science_Workflow_I.html#filtering",
    "title": "Data Science and Data Analytics",
    "section": "Filtering",
    "text": "Filtering\nNow that we have the correct data types, we might want to filter our data.frame. Filtering refers to the process of keeping rows based on certain conditions imposed on the values of the columns. For example, we might want to find all flights on 14th February that were more than one hour delayed upon arrival at their destination.\n\nWe can filter a data.frame by logical subsetting of the rows. For this, we have to combine the conditions we want to impose by using the logical operators, & (and), | (or) and ! (not). If we want to find the carriers and flight numbers of the aforementioned flights, we could do:\n\nhead(flights[flights$month == 2 & flights$day == 14 & flights$arr_delay &gt; 60,\n             c(\"year\", \"month\", \"day\", \"carrier\", \"flight\", \"arr_delay\")])\n\n      year month day carrier flight arr_delay\n38528 2013     2  14      9E   4023        92\n38551 2013     2  14      DL    807       143\n38604 2013     2  14      B6     56       118\n38613 2013     2  14      B6    600        65\n38623 2013     2  14      DL   1959       200\n38624 2013     2  14      UA    517        95"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#filtering-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#filtering-1",
    "title": "Data Science and Data Analytics",
    "section": "Filtering",
    "text": "Filtering\nLet’s see another example: suppose we want to find all flights that left from either Newark (EWR) or JFK to Los Angeles (LAX) on 14th February after 6:00 pm. We could do:\n\nflights[flights$month == 2 & \n          flights$day == 14 & \n          (flights$origin == \"EWR\" | flights$origin == \"JFK\") & \n          flights$dest == \"LAX\" & \n          flights$dep_time &gt; 1800,\n        c(\"year\", \"month\", \"day\", \"carrier\", \"flight\", \"origin\", \"dest\", \"dep_time\")]\n\n      year month day carrier flight origin dest dep_time\n39020 2013     2  14      AA    119    EWR  LAX     1812\n39076 2013     2  14      DL     87    JFK  LAX     1906\n39111 2013     2  14      AA     21    JFK  LAX     1943\n39146 2013     2  14      VX    415    JFK  LAX     2017\n39150 2013     2  14      UA    771    JFK  LAX     2027\n39184 2013     2  14      DL   2363    JFK  LAX     2118\n39185 2013     2  14      B6    677    JFK  LAX     2118\n39201 2013     2  14      AA    185    JFK  LAX     2147\n\n\n\nNote that – since there are only three possible origin airports in this data set – we could replace the third condition with\n\nflights$origin != \"LGA\""
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#filtering-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#filtering-2",
    "title": "Data Science and Data Analytics",
    "section": "Filtering",
    "text": "Filtering\nThat starts to be quite a lot of typing… To reduce the number of times that we have to type the name of the data.frame, we can use the subset function:\n\nsubset(flights,\n       month == 2 & day == 14 & origin != \"LGA\" & dest == \"LAX\" & dep_time &gt; 1800,\n       c(year, month, day, carrier, flight, origin, dest, dep_time))\n\n      year month day carrier flight origin dest dep_time\n39020 2013     2  14      AA    119    EWR  LAX     1812\n39076 2013     2  14      DL     87    JFK  LAX     1906\n39111 2013     2  14      AA     21    JFK  LAX     1943\n39146 2013     2  14      VX    415    JFK  LAX     2017\n39150 2013     2  14      UA    771    JFK  LAX     2027\n39184 2013     2  14      DL   2363    JFK  LAX     2118\n39185 2013     2  14      B6    677    JFK  LAX     2118\n39201 2013     2  14      AA    185    JFK  LAX     2147\n\n\nThis is much more clear and compact. Note that when using subset, the names of the columns we want to select do not have to be specified with quotation marks \"\"."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values",
    "href": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values",
    "title": "Data Science and Data Analytics",
    "section": "Handling missing values",
    "text": "Handling missing values\nLet’s see where in our data set we have missing values (indicated by NA):\n\nn_missing &lt;- sapply(flights, function(x) sum(is.na(x)))\nn_missing[n_missing &gt; 0]\n\n dep_time dep_delay  arr_time arr_delay  tail_num  air_time \n     4883      4883      5101      5480      1521      5480 \n\n\n\nOnly six variables seem to have missing values. Note how in this data set, the missing values can be interpreted:\n\ndep_time and dep_delay \\(\\to\\) flight was cancelled. In these cases, arr_time and arr_delay are also NA.\nAdditional missing values in arr_time \\(\\to\\) flight was diverted to another destination airport.\nAdditional missing values in arr_delay and air_time. Unknown reason for missingness, hard to construct without additional information.\nIn some cases, the tail_num of the plane seems to be simply unknown."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values-1",
    "title": "Data Science and Data Analytics",
    "section": "Handling missing values",
    "text": "Handling missing values\nLet’s start handling the cancelled flights first. Depending on the circumstances, we might want to add a variable to indicate a cancelled flight, like so…\n\nflights$cancelled &lt;- is.na(flights$dep_time)\nhead(flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"arr_time\",\n                 \"carrier\", \"flight\", \"cancelled\")], 5)\n\n  year month day dep_time arr_time carrier flight cancelled\n1 2013     1   1      517      830      UA   1545     FALSE\n2 2013     1   1      533      850      UA   1714     FALSE\n3 2013     1   1      542      923      AA   1141     FALSE\n4 2013     1   1      544     1004      B6    725     FALSE\n5 2013     1   1      554      812      DL    461     FALSE\n\n\n\n… or remove cancelled flights from the data set all together and put them into their own data.frame. Let’s go for that option:\n\ncancelled_flights &lt;- flights[is.na(flights$dep_time), ]\nflights &lt;- flights[!is.na(flights$dep_time), ]\nhead(cancelled_flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"arr_time\",\n                           \"carrier\", \"flight\", \"cancelled\")], 5)\n\n     year month day dep_time arr_time carrier flight cancelled\n839  2013     1   1       NA       NA      EV   4308      TRUE\n840  2013     1   1       NA       NA      AA    791      TRUE\n841  2013     1   1       NA       NA      AA   1925      TRUE\n842  2013     1   1       NA       NA      B6    125      TRUE\n1778 2013     1   2       NA       NA      EV   4352      TRUE"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values-2",
    "title": "Data Science and Data Analytics",
    "section": "Handling missing values",
    "text": "Handling missing values\nNext, let’s do the same also with diverted flights, which will contain all remaining flights that have NAs in the arr_time variable:\n\ndiverted_flights &lt;- flights[is.na(flights$arr_time), ]\nflights &lt;- flights[!is.na(flights$arr_time), ]\nhead(diverted_flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"arr_time\",\n                          \"carrier\", \"flight\", \"origin\", \"dest\")], 5)\n\n     year month day dep_time arr_time carrier flight origin dest\n755  2013     1   1     2016       NA      EV   4204    EWR  OKC\n1715 2013     1   2     2041       NA      B6    147    JFK  RSW\n1757 2013     1   2     2145       NA      UA   1299    EWR  RSW\n7040 2013     1   9      615       NA      9E   3856    JFK  ATL\n7852 2013     1   9     2042       NA      B6    677    JFK  LAX\n\n\n\nLet’s look at how many missing values are remaining after separating out cancelled and diverted flights from our data set:\n\nn_missing &lt;- sapply(flights, function(x) sum(is.na(x)))\nn_missing[n_missing &gt; 0]\n\narr_delay  air_time \n      379       379 \n\n\nOnly a small number of NAs are remaining in the variables arr_delay and air_time. We will deal with them when we need to…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-numeric-variables",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-numeric-variables",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Numeric variables",
    "text": "Descriptive statistics – Numeric variables\nNow, we are finally ready to compute some descriptive statistics. Say, we want to know the average departure and arrival delay of a (not cancelled or diverted) flight. We use the function mean:\n\nmean(flights$dep_delay)\n\n[1] 13.65542\n\nmean(flights$arr_delay)\n\n[1] NA\n\n\nThe average departure delay is around 13.6 minutes, but the average arrival delay is NA? Why is that?\n\nOf course, that’s exactly because of the remaining missing values in arr_delay. R cannot know the average of a vector of values where some values are unknown. However, most functions for descriptive statistics have an argument called na.rm:\n\nmean(flights$arr_delay, na.rm = TRUE)\n\n[1] 8.15129"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-numeric-variables-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-numeric-variables-1",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Numeric variables",
    "text": "Descriptive statistics – Numeric variables\nLet’s say we wanted to know what the maximum departure and arrival delays were. In that case, we would use the function max and also set its na.rm option to TRUE (strictly necessary only for arr_delay):\n\nmax(flights$dep_delay, na.rm = TRUE) / 60 # divide by 60 to get hours from minutes\n\n[1] 21.68333\n\nmax(flights$arr_delay, na.rm = TRUE) / 60 # divide by 60 to get hours from minutes\n\n[1] 21.2\n\n\nSo both maximum departure and arrival delays were over 21 hours!\n\nOther functions for important univariate descriptive statistics of numeric variables are:\n\nrange for both min and max in one go.\nmedian and quantile for quantiles.\nvar and sd for variance and standard deviation.\nfivenum and summary for five-point summaries."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-categorical-variables",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-categorical-variables",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Categorical variables",
    "text": "Descriptive statistics – Categorical variables\nFrom categorical variables, we very often want to compute a frequency table. This can be achieved with the R function table. Let’s say we want to know how many flights started from each of the three NYC airports:\n\n# Absolute frequencies:\ntable(flights$origin)\n\n\n  EWR   JFK   LGA \n58649 54097 48311 \n\n# Relative frequencies:\nprop.table(table(flights$origin))\n\n\n      EWR       JFK       LGA \n0.3641506 0.3358873 0.2999621 \n\n\n\nOr we want to know the distribution of carriers operating out of LaGuardia in %:\n\nround(prop.table(table(flights$carrier[flights$origin == \"LGA\"]))*100, 2)\n\n\n   9E    AA    AS    B6    DL    EV    F9    FL    HA    MQ    OO    UA    US \n 1.10 15.24  0.00  6.14 24.03  5.64  0.69  3.68  0.00 16.70  0.00  7.78 12.84 \n   VX    WN    YV \n 0.00  5.70  0.46"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping",
    "text": "Descriptive statistics – Grouping\nA very common action in data wrangling is computing statistics of a variable for each level of a categorical variable. In our flights data, we might be interested for example in:\n\nthe average departure delay for each carrier,\nthe median arrival delay in each month broken down by NYC airport,\nthe fastest air time for each route,\n…\n\nSuch actions require us to group the data by the factor levels of the categorical variable and then compute statistics on the variable of interest for each of the resulting groups."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-1",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping",
    "text": "Descriptive statistics – Grouping\nIn R, this kind of action can be achieved with the functions tapply and aggregate. Let’s first look at how tapply works for the three examples given on the previous slide:\n\n# Average departure delay by carrier:\ntapply(flights$dep_delay, flights$carrier, mean)\n\n       9E        AA        AS        B6        DL        EV        F9        FL \n18.238843  9.987883  8.058333 13.796756  9.530427 23.308381 24.197605 14.879078 \n       HA        MQ        OO        UA        US        VX        WN        YV \n11.845304 11.580444 63.000000 12.404769  3.938624 14.777251 17.169844 22.549550 \n\n# Median arrival delay for each month and airport\ntapply(flights$arr_delay, list(flights$origin, flights$month), median, na.rm = TRUE)\n\n     1  2  3  4  5  6\nEWR  0 -2 -4 -1 -6 -1\nJFK -7 -5 -7 -4 -9 -1\nLGA -4 -4 -7 -2 -9 -4\n\n# Fastest air time on each route\nhead(tapply(flights$air_time, paste0(flights$origin, \"_\", flights$dest), min, na.rm = TRUE))\n\nEWR_ALB EWR_ATL EWR_AUS EWR_AVL EWR_BDL EWR_BNA \n     24      88     181      76      20      70"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-2",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping",
    "text": "Descriptive statistics – Grouping\nSo tapply generally works like this:\n\ntapply(variable_of_interest, list_of_grouping_variables, aggregation_function, ...)\n\n\nBy contrast, the function aggregate works like this:\n\naggregate(df_of_interest, list_of_grouping_variables, aggregation_function, ...)\n\naggregate then applies the aggregation_function to each variable in the df_of_interest by group.\n\n\nSo, we can compute both average departure and arrival delays by carrier in one go, for example:\n\nhead(aggregate(flights[,c(\"dep_delay\", \"arr_delay\")],\n               list(flights$carrier),\n               mean, na.rm = TRUE))\n\n  Group.1 dep_delay arr_delay\n1      9E 18.238843  9.519527\n2      AA  9.987883  1.475949\n3      AS  8.058333 -3.036415\n4      B6 13.796756 10.410469\n5      DL  9.530427  1.745742\n6      EV 23.308381 20.328515"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-with-cut",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-with-cut",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping with cut",
    "text": "Descriptive statistics – Grouping with cut\n\nA frequently encountered goal is to group not based on an existing categorical variable, but on different intervals of a numeric variable.\nFor example, we might be interested in analyzing delay patterns based on air time: short-haul (\\(\\leq\\) 3 hours), medium-haul (3-6 hours) and long-haul (6-16 hours) (according to the IATA).\nTo group by these flight length categories, we have to cut our air_time variable at these cut points. For this, we can use the function cut.\nBesides specifying the cut points, cut offers us also to label the levels of the resulting factor."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-with-cut-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-with-cut-1",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping with cut",
    "text": "Descriptive statistics – Grouping with cut\nSo to add this variable to our data.frame, we might do:\n\nflights$length_cat &lt;- cut(flights$air_time, c(0, 180, 360, Inf),\n                          labels = c(\"short-haul\", \"medium-haul\", \"long-haul\"))\n\nhead(flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"arr_time\",\n                 \"origin\", \"dest\", \"air_time\", \"length_cat\")])\n\n  year month day dep_time arr_time origin dest air_time  length_cat\n1 2013     1   1      517      830    EWR  IAH      227 medium-haul\n2 2013     1   1      533      850    LGA  IAH      227 medium-haul\n3 2013     1   1      542      923    JFK  MIA      160  short-haul\n4 2013     1   1      544     1004    JFK  BQN      183 medium-haul\n5 2013     1   1      554      812    LGA  ATL      116  short-haul\n6 2013     1   1      554      740    EWR  ORD      150  short-haul\n\n\nNow, we can analyse average departure and arrival delays by these categories using aggregate, for instance:\n\naggregate(flights[,c(\"dep_delay\", \"arr_delay\")],\n          list(flights$length_cat), mean, na.rm = TRUE)\n\n      Group.1 dep_delay arr_delay\n1  short-haul  14.55357  10.06333\n2 medium-haul  11.22840   2.39164\n3   long-haul  10.17673  16.59834"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables",
    "text": "Joining tables\nOur flights data set only contains codes for carrier, air plane and airports. To properly understand this data, we need to be able to enrich this data set by more information, such as:\n\nFull name of the carrier\nFull name and location of the origin and destination airports\nType, model and size of the aircraft used\nWeather information at the origin airport at the time of departure\n\nIndeed, any moderately complex data science project will involve multiple tables that must be joined together in order to answer the questions that you are interested in."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-1",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables",
    "text": "Joining tables\nThe nycflights13 package contains four additional tables that can be joined into the flights table. The below graph illustrates their relation:"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-2",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables",
    "text": "Joining tables\nTherefore, in the flights data set,\n\nthe variables origin and dest are foreign keys that correspond to the primary key faa in airports.\nthe variable tail_num is a foreign key that corresponds to the primary key tail_num in planes.\nthe variable carrier is a foreign key that corresponds to the primary key carrier in airlines.\nthe variables origin and time_hour constitute a compound foreign key that corresponds to the compound primary key constituted by origin and time_hour in weather.\n\nTo illustrate how we can join information from these tables into flights using keys, we start with the easiest example of airlines."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-3",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-3",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables",
    "text": "Joining tables\nFor this, we have to read in the airlines table and inspect it:\n\nairlines &lt;- read.csv(file.path(\"../data\", \"nyc13_airlines.csv\"))\nairlines\n\n   carrier                        name\n1       9E           Endeavor Air Inc.\n2       AA      American Airlines Inc.\n3       AS        Alaska Airlines Inc.\n4       B6             JetBlue Airways\n5       DL        Delta Air Lines Inc.\n6       EV    ExpressJet Airlines Inc.\n7       F9      Frontier Airlines Inc.\n8       FL AirTran Airways Corporation\n9       HA      Hawaiian Airlines Inc.\n10      MQ                   Envoy Air\n11      OO       SkyWest Airlines Inc.\n12      UA       United Air Lines Inc.\n13      US             US Airways Inc.\n14      VX              Virgin America\n15      WN      Southwest Airlines Co.\n16      YV          Mesa Airlines Inc.\n\n\nThis is a very simple and small data set with the carrier code and name of 16 American airline companies. Now, how do we join this information into the flights data set?"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-types-of-joins",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-types-of-joins",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Types of joins",
    "text": "Joining tables – Types of joins\n\nThere are many different types of joins that determine how two (or more) tables are brought together. We will illustrate only the most important ones with a toy example.\nSay, we have two tables, x and y, each with a key column and a column containing some values:\n\n\n\n\n\nThe colored key columns map background color to key value. The grey columns represent “value” columns."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-inner-join",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-inner-join",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Inner join",
    "text": "Joining tables – Inner join\n\nIn an inner join, we want to keep only the rows that have information in both tables.\nIn the example, this is only the case for data points with key 1 and 2. Therefore, the rows with key 3 and 4 do not make it to the joined table, if we join x and y on an inner join:"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-left-outer-join",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-left-outer-join",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Left outer join",
    "text": "Joining tables – Left outer join\n\nIn a left outer join, we want to keep all rows in the left table (in this case x). Rows without a matching key in y receive NA for the value column of y:\n\n\n\n\n\nFor simplicity, such joins are usually simply called left joins."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-right-outer-join",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-right-outer-join",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Right outer join",
    "text": "Joining tables – Right outer join\n\nIn a right outer join, we want to keep all rows in the right table (in this case y). Rows without a matching key in x receive NA for the value column of x:\n\n\n\n\n\nFor simplicity, such joins are usually simply called right joins."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-full-outer-join",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-full-outer-join",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Full outer join",
    "text": "Joining tables – Full outer join\n\nIn a full outer join, we want to keep all the rows across both tables and fill the missing values with NAs:\n\n\n\n\n\nFor simplicity, such joins are usually simply called full joins."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#performing-joins-with-merge",
    "href": "slides/03_The_Data_Science_Workflow_I.html#performing-joins-with-merge",
    "title": "Data Science and Data Analytics",
    "section": "Performing joins with merge",
    "text": "Performing joins with merge\nIn R, joining happens with the help of the function merge, whose arguments can be quite confusing at times… Here is the breakdown of the most important ones:\n\nx and y are the left and right data.frame to join.\nby is the name of the key(s) used for joining, if the column name of this key is the same in both x and y. If not, we specify the column names in x via the by.x argument and in y via the by.y argument.\nThe arguments all, all.x and all.y specify the type of join:\n\nSet all to FALSE for an inner join.\nSet all.x to TRUE for a left join.\nSet all.y to TRUE for a right join.\nSet all to TRUE for a full outer join."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#performing-joins-with-merge-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#performing-joins-with-merge-1",
    "title": "Data Science and Data Analytics",
    "section": "Performing joins with merge",
    "text": "Performing joins with merge\nSo let’s finally see joins in action! We left join the airlines data set into flights on the key of carrier. So, to do this in R, we run:\n\nflights &lt;- merge(flights, airlines, by = \"carrier\", all.x = TRUE)\nhead(flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"origin\", \"dest\", \"carrier\", \"name\")])\n\n  year month day dep_time origin dest carrier              name\n1 2013     5  19     2034    LGA  TYS      9E Endeavor Air Inc.\n2 2013     4  28     1621    JFK  MKE      9E Endeavor Air Inc.\n3 2013     4  23      749    EWR  CVG      9E Endeavor Air Inc.\n4 2013     6   5     2049    JFK  DCA      9E Endeavor Air Inc.\n5 2013     1  19     1944    JFK  IAD      9E Endeavor Air Inc.\n6 2013     6  27     1939    LGA  DAY      9E Endeavor Air Inc.\n\n\nNote two things about this successful join:\n\nFirst, in the airlines data set, we found a match for every carrier in flights, so the left join did not produce any NAs. Proof:\n\nsum(is.na(flights$name))\n\n[1] 0\n\n\nSecond, merge changed the order of the rows of the data.frame."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-data",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-data",
    "title": "Data Science and Data Analytics",
    "section": "Tidy data",
    "text": "Tidy data\n\nTo finish this chapter, let’s talk about tidy data.\nRather than just meaning “clean”, tidy data actually refers to a specific way of organizing your data that is beneficial for the types of actions we want to perform on them. There are three interrelated rules that make data tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nThese rules might seem pretty obvious, but actually, most real-world data sets do not meet these requirements as data is often organized to facilitate some goal other than analysis."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-1",
    "title": "Data Science and Data Analytics",
    "section": "Tidy data",
    "text": "Tidy data\n\nTo illustrate this point, we will have a look a new data set. It contains the fertility rates in Germany and South Korea in the years 1960 to 2015:\n\nfertility &lt;- read.csv(file.path(\"../data\", \"fertility.csv\"))\nfertility[, 1:12]\n\n      country X1960 X1961 X1962 X1963 X1964 X1965 X1966 X1967 X1968 X1969 X1970\n1     Germany  2.41  2.44  2.47  2.49  2.49  2.48  2.44  2.37  2.28  2.17  2.04\n2 South Korea  6.16  5.99  5.79  5.57  5.36  5.16  4.99  4.85  4.73  4.62  4.53\n\ndim(fertility)\n\n[1]  2 57\n\n\nThis data is untidy. Why?\n\nIt contains a variable (namely the year) in column names, but according to the principles of tidy data, each variable should be its own column.\nThe observations are fertility rates in two countries, so these values should be organized in rows."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-2",
    "title": "Data Science and Data Analytics",
    "section": "Tidy data",
    "text": "Tidy data\nDue to its shape, such data is said to be in a wide format (few rows, lots of columns). We want to reshape it to long format (lots of rows, few columns). For this purpose, we use the R function reshape:\n\nfertility_long &lt;- reshape(fertility, direction = \"long\",\n                          varying = list(names(fertility)[-1]), v.names = \"fertility_rate\",\n                          idvar = \"country\",\n                          timevar = \"year\", times = 1960:2015)\nrownames(fertility_long) &lt;- NULL\nhead(fertility_long, 8)\n\n      country year fertility_rate\n1     Germany 1960           2.41\n2 South Korea 1960           6.16\n3     Germany 1961           2.44\n4 South Korea 1961           5.99\n5     Germany 1962           2.47\n6 South Korea 1962           5.79\n7     Germany 1963           2.49\n8 South Korea 1963           5.57\n\n\nNow, the data is tidy! Note that it is exactly the same underlying data, just represented in a slightly different way."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-3",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-3",
    "title": "Data Science and Data Analytics",
    "section": "Tidy data",
    "text": "Tidy data\nThe function reshape takes some practice to get used to. But once we know how to get our data tidy, there are multiple advantages to using this consistent way of organizing your data:\n\nAs we will see, many functions for data analysis in R cannot be used, unless the data is tidy. This applies in particular to the visualizations we will create with the ggplot2 package.\nConsistent data structures are easier to work with. If every data set “looks and feels” the same, you can build routines that will make your analyses more efficient and effective.\nHaving variables consistently in columns is particularly sensible in R due to its vectorized nature. R performs at its best when it is able to run functions on vectors of values. With a tidy data format, we can fully leverage this potential."
  },
  {
    "objectID": "the-essentials-of-r-programming.html",
    "href": "the-essentials-of-r-programming.html",
    "title": "The Essentials of R Programming",
    "section": "",
    "text": "Download the PDF version here.",
    "crumbs": [
      "Slides",
      "The Essentials of R Programming"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Download the PDF version here.",
    "crumbs": [
      "Slides",
      "Getting Started"
    ]
  },
  {
    "objectID": "introduction-to-ds.html",
    "href": "introduction-to-ds.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Download the PDF version here.",
    "crumbs": [
      "Slides",
      "Introduction to Data Science"
    ]
  },
  {
    "objectID": "image_sources.html",
    "href": "image_sources.html",
    "title": "Image sources",
    "section": "",
    "text": "The following table displays the sources of all images used in the slides. To see the file names of these images, please refer to the corresponding folder in the GitHub repository.\n\n\n\n\n\n\n\n\n\n\nfile\nSource\nNote\n\n\n\n\nai_ml_dl_ds_venn.png\nhttps://www.guvi.com/blog/must-know-neural-networks-for-data-science/\n\n\n\nai_venn.png\nhttps://link.springer.com/chapter/10.1007/978-981-19-5272-2_1\n\n\n\naimeme.png\nReddit\n\n\n\nanyideas.png\nOwn creation\n\n\n\napi.png\nhttps://www.postman.com/what-is-an-api/\n\n\n\narrow_csv.png\nOwn creation\n\n\n\narrow_delim.png\nOwn creation\n\n\n\nbias_var_tradeoff.png\nAn Introduction to Statistical Learning\n\n\n\nchatgpt_headlines.png\nhttps://www.searchengineacademy.com/blog/the-search-engine-sky-is-falling-does-chatgpt-mean-the-twilight-of-google/\n\n\n\ncitation.png\nOwn creation\n\n\n\ncitation_added.png\nOwn creation\n\n\n\ncitation_added2.png\nOwn creation\n\n\n\nclassification.png\nAn Introduction to Statistical Learning\n\n\n\nclusters.png\nTan, Steinbach, Karpatne, Kumar, 2018\n\n\n\nclusters_black.png\nTan, Steinbach, Karpatne, Kumar, 2018\n\n\n\ncrm.png\nhttps://localo.com/marketing-dictionary/what-is-crm\n\n\n\ncross_reference.png\nOwn creation\n\n\n\ncross_reference2.png\nOwn creation\n\n\n\ncsv_file.png\nhttps://rafalab.dfci.harvard.edu/dsbook-part-1/R/importing-data.html\n\n\n\ndashboard.png\nhttps://www.domo.com/learn/article/the-benefits-of-business-intelligence-dashboards\n\n\n\ndata-science-cycle.001.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.002.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.003.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.004.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.005.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.006.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.007.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.008.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.009.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata_wrangling.png\nhttps://airbyte.com/data-engineering-resources/data-wrangling\n\n\n\ndate_format_meme.jpg\nhttps://www.reddit.com/r/trippinthroughtime/comments/7axug6/the_perfect_date/?rdt=64406\n\n\n\ndb.png\nhttps://mockey.ai/blog/google-gemini-vs-chatgpt/\n\n\n\ndecision_boundaries.png\nAn Introduction to Statistical Learning\n\n\n\ndestatis.svg\nDestatis (Statistisches Bundesamt)\n\n\n\ndirty_excel.png\nhttps://www.sophieheloisebennett.com/posts/excel-sheet-cleaning/\n\n\n\nds_circle.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_1.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_2.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_3.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_4.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_5.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_6.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_7.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_8.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_importance_business.jpg\nhttps://www.jaroeducation.com/blog/data-science-and-analytics-importance/\n\n\n\nds_proglangs.jpg\nhttps://jelvix.com/blog/top-data-science-programming-languages\n\n\n\nds_vs_da.jpg\nhttps://www.qlik.com/us/data-analytics/data-science-vs-data-analytics\n\n\n\nds_vs_da2.png\nhttps://www.geeksforgeeks.org/difference-between-a-data-analyst-and-a-data-scientist/\n\n\n\ndsvenn.png\nhttps://www.cleanpng.com/png-data-science-venn-diagram-computer-science-machine-1831517/\n\n\n\nencoding_issue.png\nOwn creation\n\n\n\nencoding_issue2.png\nOwn creation\n\n\n\nerp.png\nhttps://koss.software/koss-lexikon/enterprise-resource-planning-system-erp\n\n\n\nerp2.png\nhttps://dmsiworks.com/faqs/is-business-central-a-good-erp-system\n\n\n\nexcel.png\nhttps://sta199-s24.github.io/\n\n\n\nfirst_quarto.png\nOwn creation\n\n\n\nfirst_quarto2.png\nOwn creation\n\n\n\nfirst_quarto3.png\nOwn creation\n\n\n\nfit_parametric.png\nAn Introduction to Statistical Learning\n\n\n\nfit_nonparametric.png\nAn Introduction to Statistical Learning\n\n\n\ngrammar_of_graphics.png\nhttps://r.qcbs.ca/workshop03/book-en/grammar-of-graphics-gg-basics.html\n\n\n\ngrammar_of_graphics_book.jpeg\nhttps://www.amazon.co.uk/Grammar-Graphics-Statistics-Computing/dp/0387987746\n\n\n\nHarvardX_Rbasics.png\nOwn creation\n\n\n\nHarvardX_Visualization.png\nOwn creation\n\n\n\nhbr_article1.png\nSource linked in slides\n\n\n\nhbr_article2.png\nSource linked in slides\n\n\n\nhp_json.png\nOwn creation\n\n\n\nhttp_get.jpg\nhttps://www.cloud4y.ru/en/blog/what-are-http-requests/\n\n\n\nhttp_response.jpg\nhttps://slideplayer.com/slide/5320369/\n\n\n\nISL_book.png\nhttps://www.statlearning.com/\n\n\n\njoins_full.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\njoins_inner.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\njoins_left.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\njoins_right.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\njoins_setup.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\nkfold_cv.png\nhttps://towardsdatascience.com/how-to-cross-validation-with-time-series-data-9802a06272c6/\n\n\n\nknn_reg.png\nAn Introduction to Statistical Learning\n\n\n\nknn_vs_lm.png\nAn Introduction to Statistical Learning\n\n\n\nknn_vs_lm2.png\nAn Introduction to Statistical Learning\n\n\n\nml_overview.png\nhttps://sanchittanwar75.medium.com/introduction-to-machine-learning-and-deep-learning-bd25b792e488\n\n\n\nnber.png\nhttps://www.nber.org/\n\n\n\nneighbor_unknown.png\nTan, Steinbach,Karpatne, Kumar, 2018\n\n\n\nnosql_types.png\nhttps://www.linkedin.com/pulse/types-nosql-databases-umang-agarwal\n\n\n\nnycflights13_data.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\noverfitting.png\nhttps://de.mathworks.com/discovery/overfitting.html\n\n\n\noverfitting.svg\nhttps://de.mathworks.com/discovery/overfitting.html\n\n\n\npalette_finder.png\nhttps://r-graph-gallery.com/color-palette-finder\n\n\n\npalmer_penguins.jpg\nhttps://education.rstudio.com/blog/2020/07/palmerpenguins-cran/\n\n\n\npaths_1.png\nhttps://dodona.be/en/activities/760650576/\n\n\n\npaths_2.png\nhttps://dodona.be/en/activities/760650576/\n\n\n\npenguins_beak.png\nhttps://education.rstudio.com/blog/2020/07/palmerpenguins-cran/\n\n\n\npenguins_cartoon.png\nhttps://allisonhorst.github.io/palmerpenguins/\n\n\n\npenguins_report.png\nOwn creation\n\n\n\npoints_symbols.png\nhttps://www.sthda.com/english/wiki/ggplot2-point-shapes\n\n\n\npresentation_quarto.png\nOwn creation\n\n\n\nquarto.png\nhttps://quarto.org/docs/get-started/hello/rstudio.html\n\n\n\nr.png\nhttps://sta199-s24.github.io/\n\n\n\nr_function_syntax.png\nhttps://www.learnbyexample.org/r-functions/\n\n\n\nr_help_round.png\nOwn creation\n\n\n\nr_help_sample.png\nOwn creation\n\n\n\nr_logo.png\nhttps://commons.wikimedia.org/wiki/File:R_logo.svg\n\n\n\nr_vs_rstudio.png\nhttps://moderndive.com/1-getting-started.html\n\n\n\nread_delim.png\nOwn creation\n\n\n\nreg_tree.png\nAn Introduction to Statistical Learning\n\n\n\nrel_db.jpg\nhttps://www.ddebose.com/projects1/movie-theater-database-psql\n\n\n\nrel_db2.svg\nhttps://en.wikipedia.org/wiki/Relational_database\n\n\n\nrender.png\nOwn creation\n\n\n\nrstudio.png\nhttps://sta199-s24.github.io/\n\n\n\nrstudio_logo.png\nhttps://techicons.dev/icons/rstudio\n\n\n\nrstudio_meme.jpg\nOwn creation\n\n\n\nrstudio_tour.png\nOwn creation\n\n\n\nrstudio_tour2.png\nhttps://sta199-s24.github.io/\n\n\n\nrvspython.jpg\nhttps://x.com/R_Amadio/status/1372624991485845510\n\n\n\nrvspython1.jpg\nhttps://x.com/R_Amadio/status/1372624991485845510\n\n\n\nrvspython2.jpg\nhttps://x.com/R_Amadio/status/1372624991485845510\n\n\n\nscale_funcs.png\nhttps://socviz.co/\n\n\n\nsh_json.png\nOwn creation\n\n\n\nspreadsheets_deaths.png\nhttps://r4ds.hadley.nz/spreadsheets.html\n\n\n\nsql_meme.jpg\nOwn creation\n\n\n\nstatista.png\nStatista\n\n\n\nstatistik_austria.png\nStatistik Austria\n\n\n\nsupervised_learning.png\nhttps://www.superannotate.com/blog/image-classification-basics\n\n\n\nth_json.png\nOwn creation\n\n\n\ntimezones.png\nhttps://en.wikipedia.org/wiki/Tz_database#/media/File:Timezone-boundary-builder_release_2023d.png\n\n\n\ntrain_test_mse.png\nAn Introduction to Statistical Learning\n\n\n\ntrain_test_mse2.png\nAn Introduction to Statistical Learning\n\n\n\ntrain_test_mse3.png\nAn Introduction to Statistical Learning\n\n\n\ntrain_test_split.jpg\nhttps://builtin.com/data-science/train-test-split\n\n\n\ntsv_file.png\nOwn creation\n\n\n\ntypes_of_data.png\nhttps://www.researchgate.net/publication/236860222_Developing_Dynamic_Packaging_Applications_Using_Semantic_Web-Based_Integration\n\n\n\ntype 1 and 2 errors.jpg\nhttps://learning.eupati.eu/mod/book/tool/print/index.php?id=362&chapterid=388\n\n\n\nUOS_Logo.jpg\nUniversity\n\n\n\nusdata.png\nhttps://data.gov/\n\n\n\nwb.png\nhttps://datacatalog.worldbank.org/home\n\n\n\nworkspace.png\nOwn creation\n\n\n\nX.jpg\nhttps://www.freepik.com/free-photos-vectors/twitter-x-logo",
    "crumbs": [
      "Image Sources"
    ]
  },
  {
    "objectID": "the-ds-workflow-iii.html",
    "href": "the-ds-workflow-iii.html",
    "title": "The Data Science Workflow III",
    "section": "",
    "text": "Download the PDF version here.",
    "crumbs": [
      "Slides",
      "The Data Science Workflow III"
    ]
  },
  {
    "objectID": "the-ds-workflow-ii.html",
    "href": "the-ds-workflow-ii.html",
    "title": "The Data Science Workflow II",
    "section": "",
    "text": "Download the PDF version here.",
    "crumbs": [
      "Slides",
      "The Data Science Workflow II"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Data Analytics",
    "section": "",
    "text": "Welcome to the course Data Science and Data Analytics in the summer semester 2025! This website contains all the lecture materials, i.e. the slides, exercises and the solutions to the exercises.\nIn this course, we will cover the following topics:\n\n\n\nDate(s)\nTopic\nSlides\nExercises\n\n\n\n\n14th March\nGetting Started\nIntroduction to Data Science\nGetting started\nIntroduction to Data Science\n\n\n\n21st March\nThe Essentials of R Programming\nThe Essentials of R Programming\nExercises I\n\n\n31st March\nThe Data Science Workflow – Part I: Import, Tidy and Transform\nThe Data Science Workflow – Part I\nExercises II\n\n\n4th April\n11th April\nThe Data Science Workflow – Part II: Visualize\nThe Data Science Workflow – Part II\nExercises III\n\n\n30th April\n6th May\n9th May\n16th May\n23rd May\nThe Data Science Workflow – Part III: Model\nThe Data Science Workflow – Part III\nExercises IV\n\n\n28th May\nThe Data Science Workflow – Part IV: Communicate\nThe Data Science Workflow – Part IV\n\n\n\n6th June\nQ&A Session\n\n\n\n\n13th June\nFinal presentations",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "The following table contains an overview of the datasets used in slides and exercises as well as the source of each data set. You can download the corresponding dataset by clicking on the provided link:\n\n\n\nName\nSource\nDirect link\n\n\n\n\nAdult Income\nUCI Machine Learning Repository\nAdult Income\n\n\nAdvertising\nISLR book\nAdvertising\n\n\ncalificaciones\nIntroduction to Data Science book\ncalificaciones\n\n\nclick\nKaggle\nclick\n\n\ncredit\nISLR book\ncredit\n\n\ndatasets\nreadxl\ndatasets\n\n\nfertility\nIntroduction to Data Science book\nfertility\n\n\ngm_cm\nGapminder\ngm_cm\n\n\ngm_gdp\nGapminder\ngm_gdp\n\n\ngm_geo\nGapminder\ngm_geo\n\n\ngm_lex\nGapminder\ngm_lex\n\n\ngm_pop\nGapminder\ngm_pop\n\n\ngm_tfr\nGapminder\ngm_tfr\n\n\ngoogle_search_ds\nGoogle Trends\ngoogle_search_ds\n\n\nmall_customers\nKaggle\nmall_customers\n\n\nmtcars\nR\nmtcars\n\n\nnative\nOwn creation\nnative\n\n\nnyc13_airlines\nnycflights13\nnyc13_airlines\n\n\nnyc13_airports\nnycflights13\nnyc13_airports\n\n\nnyc13_flights\nnycflights13\nnyc13_flights\n\n\nnyc13_planes\nnycflights13\nnyc13_planes\n\n\nnyc13_weather\nnycflights13\nnyc13_weather\n\n\nseeds_dataset\nUCI Machine Learning Repository\nseeds_dataset\n\n\nwinequality-red\nUCI Machine Learning Repository\nwinequality-red\n\n\nwinequality-white\nUCI Machine Learning Repository\nwinequality-white",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "slides/06_penguins_quarto.html",
    "href": "slides/06_penguins_quarto.html",
    "title": "Penguins in Antarctica",
    "section": "",
    "text": "For this analysis, we will use the penguins data set from the palmerpenguins package. The website belonging to this package can be found here.\nThe data set contains size measurements, clutch observations, and blood isotope ratios for three penguins species observed on three islands in the Palmer Archipelago, Antarctica, over a study period of three years (Gorman, Williams, and Fraser 2014).\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n\n\n\nFigure 1: A scatter plot of flipper length against body mass\n\n\n\n\n\nFigure 1 shows a scatter plot of the flipper length against the body mass of all penguins in the data set.\nThe data set comprises measurements of three types of penguins, namely:\n\nAdelie 🐧\nChinstrap 🐧\nGentoo 🐧\n\n\n\n\n\n\n\nFigure 2\n\n\n\nFigure 2 shows a cartoon of the three penguin species in the data set."
  },
  {
    "objectID": "slides/06_penguins_quarto.html#data",
    "href": "slides/06_penguins_quarto.html#data",
    "title": "Penguins in Antarctica",
    "section": "",
    "text": "For this analysis, we will use the penguins data set from the palmerpenguins package. The website belonging to this package can be found here.\nThe data set contains size measurements, clutch observations, and blood isotope ratios for three penguins species observed on three islands in the Palmer Archipelago, Antarctica, over a study period of three years (Gorman, Williams, and Fraser 2014).\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n\n\n\nFigure 1: A scatter plot of flipper length against body mass\n\n\n\n\n\nFigure 1 shows a scatter plot of the flipper length against the body mass of all penguins in the data set.\nThe data set comprises measurements of three types of penguins, namely:\n\nAdelie 🐧\nChinstrap 🐧\nGentoo 🐧\n\n\n\n\n\n\n\nFigure 2\n\n\n\nFigure 2 shows a cartoon of the three penguin species in the data set."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#the-data-science-workflow-communicate",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#the-data-science-workflow-communicate",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Communicate",
    "text": "The Data Science workflow – Communicate"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#putting-it-all-together",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#putting-it-all-together",
    "title": "Data Science and Data Analytics",
    "section": "Putting it all together…",
    "text": "Putting it all together…\n\nThe final product of a data science project is often a report, which can take multiple different forms:\n\na scientific publication (typically PDF)\na technical report for your company (typically Word, Pages or PDF)\na presentation (typically PowerPoint, Keynote or PDF)\na blog post (typically HTML)\n…\n\nDepending on the audience, such reports may include\n\ntext to describe project, methods and findings\nfigures and tables to describe and visualize data and results\ncode you ran to get to these results"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#putting-it-all-together-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#putting-it-all-together-1",
    "title": "Data Science and Data Analytics",
    "section": "Putting it all together…",
    "text": "Putting it all together…\n\nNow, imagine…\n\nyou have to re-run your entire analysis with a new data set because the initial one was wrong in some way.\nyou realize that a mistake was made and the code needs to be fixed and re-run entirely.\na colleague from another department wants to reproduce the results from your report using your data and code.\n\nSituations like these are very commonplace for a data scientist. It is for this reason that we need a framework that integrates text, figures, tables, code and data to create reproducible reports.\nSuch a framework is given by Quarto."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#what-is-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#what-is-quarto",
    "title": "Data Science and Data Analytics",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nQuarto is a format for so-called literate programming documents. Literate programming weaves textual documentation and comments together with code, producing a document that is optimized for human understanding.\nThe basic workflow is very simple:\n\nCreate a Quarto file (ends in .qmd)\nIn this file, specify the output format (PDF, HTML, Word, …) as well as metadata about the document, such as a title, the name of the author, date, etc.\nAdd your code and text.\nCompile the document into the final report. In this process, Quarto runs all your code, creates all the plots and tables and integrates everything with your text into the specified output format.\n\nQuarto is very powerful. The rest of this course gives a basic introduction."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#our-first-quarto-document",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#our-first-quarto-document",
    "title": "Data Science and Data Analytics",
    "section": "Our first Quarto document",
    "text": "Our first Quarto document\nIn RStudio, we can create a new Quarto file by clicking on File &gt; New File &gt; Quarto Document. Give the document a fitting title and author and select PDF as the output format (for now). RStudio then creates a template .qmd file:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-yaml-header",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-yaml-header",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – YAML header",
    "text": "Elements of a Quarto document – YAML header\n\nAt the very top of the document – demarcated by three dashes (---) on either end – is the so-called YAML header:\ntitle: \"My first Quarto document\"\nauthor: \"Julian Amon\"\nformat: pdf\nYAML is a widely used language mainly used for providing configuration data.\nWith Quarto, it is used to define metadata about the document, such as (in this case) title, author and (output) format.\nWe can define many other things in the header than what is included in the template, e.g. theme, fontcolor or fig-width. A complete list of available YAML fields for PDF documents can be found here."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – Code chunks",
    "text": "Elements of a Quarto document – Code chunks\n\nTo run code inside a Quarto document, you need to put it inside a so-called code chunk. Such a code consists of four elements:\n\nThree back ticks and the programming language in curly braces to start the chunk (since we only use R, for us, this is always ```{r}).\nChunk options (optional): these are key-value pairs that can be used for customizing code chunks, each on their own line starting with #|.\nThe actual code.\nThree back ticks again to signify the end of the code chunk.\n\nThe template contains an example of a code chunks with options:\n```{r}\n#| echo: false\n\n2 * 2\n```"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks-1",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – Code chunks",
    "text": "Elements of a Quarto document – Code chunks\n\nThe full list of code options is available here. The most important ones are:\n\necho: false prevents code, but not the results from appearing in the finished file. Use this when writing reports aimed at people who do not want to see the underlying R code.\neval: false prevents code from being run. This is useful for displaying example code, or for disabling a large block of code without commenting each line.\ninclude: false runs the code, but doesn’t show the code or results in the final document. Use this for setup code (like loading libraries) that you do not want cluttering your report.\nmessage: false or warning: false prevents messages or warnings from appearing in the finished file.\n\nBy default, all of these code chunk options are true."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks-2",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks-2",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – Code chunks",
    "text": "Elements of a Quarto document – Code chunks\n\nAs you work more with Quarto, you will discover that some of the default chunk options do not fit your needs and you want to change them.\nFor example, consider a situation where you are writing a report for a non-technical audience. To hide the underlying R code, we would have to set echo: false in each and every chunk.\nTo avoid this, we can set the preferred options globally in the YAML header under execute. So, to achieve global code suppression, we would set:\ntitle: \"My first Quarto document\"\nauthor: \"Julian Amon\"\nexecute:\n  echo: false\nformat: pdf\nThese global code chunk options can be overridden locally in any chunk."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-markdown-text",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-markdown-text",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – Markdown text",
    "text": "Elements of a Quarto document – Markdown text\n\nFinally, besides the YAML header and code chunks, the last (main) type of content we see in a Quarto file is text mixed with simple formatting instructions. This text is written in a so-called Markdown format.\nInstead of using buttons like in Word or Pages to format our text, Markdown lets us add headings, bold or italic text, lists, links and more, just by typing special characters. Consider the following examples:\n\n# Heading, ## Subheading and ### Subsubheading for section titles.\n*italic* for text in italics.\n**bold** for text in boldface.\n- Item 1\n- Item 2 for bullet point lists.\n[Link text](https://quarto.org) for clickable links."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#combining-all-three-elements-in-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#combining-all-three-elements-in-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Combining all three elements in Quarto",
    "text": "Combining all three elements in Quarto\n\nLet’s use our knowledge about the three elements of a Quarto document (YAML header, code chunks and Markdown text) to adapt the template that RStudio created for us in the following ways:\n\nAdd a date field to the header and set it (literally) to today.\nChange the title of the document to Penguins in Antarctica.\nChange the level 2 header ## Quarto to ## Data.\nImmediately after this header, add a code chunk that only loads the libraries palmerpenguins and ggplot2. For this chunk, set include: false.\nAfter that chunk, write a simple sentence in Markdown that points out that we use data from the palmerpenguins package. Make sure to include a link to the website of the package, which can be found here.\nFinally, in another chunk, create a scatter plot of flipper_length_mm length against body_mass_g. Remove all other content from the template."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#combining-all-three-elements-in-quarto-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#combining-all-three-elements-in-quarto-1",
    "title": "Data Science and Data Analytics",
    "section": "Combining all three elements in Quarto",
    "text": "Combining all three elements in Quarto\nAfter you did all that, your Quarto document should look something like this:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#compiling-our-first-quarto-report",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#compiling-our-first-quarto-report",
    "title": "Data Science and Data Analytics",
    "section": "Compiling our first Quarto report",
    "text": "Compiling our first Quarto report\n\nTo compile (or render) your final report from this Quarto file, first save the file into an appropriate location on your computer.\nThen, click on the  button at the top of the screen. This will run all the code, and combine text, code and results in the desired output format:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#the-visual-editor",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#the-visual-editor",
    "title": "Data Science and Data Analytics",
    "section": "The visual editor",
    "text": "The visual editor\n\nWe have successfully created our first Quarto report! 🎉\nWhile writing text in Markdown format is in principle easy to read and write, it still requires learning new syntax. For this reason, RStudio offers the visual editor to assist you in editing your Quarto document:\n\nThe idea of the visual editor is to provide a graphical user interface (GUI) that is more similar to working with, say, Microsoft Word.\nFor example, you can use the buttons on the menu bar for text formatting or to insert lists, images, tables, cross-references, etc. You can also paste an image from your clipboard directly into the visual editor and much more…\nWhile the visual editor displays your content with formatting, under the hood, it saves your content in plain Markdown. You can switch back and forth between visual and source editor to view and edit your content."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#the-visual-editor-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#the-visual-editor-1",
    "title": "Data Science and Data Analytics",
    "section": "The visual editor",
    "text": "The visual editor\n\nLet’s use the visual editor to make the following changes to our Quarto document on the penguins from the palmerpenguins package:\n\nWrite a sentence that explains that the data comprises three penguin species and list them with bullet points (Adelie, Chinstrap and Gentoo).\nAt the end of each bullet point, insert a penguin emoji (Insert &gt; Special Characters &gt; Insert Emoji...).\nBelow these bullet points, insert the cartoon displaying all three species from the package website. Make sure that it is aligned in the centre.\n\nAt the end, switch back to the source editor and see how your changes were implemented in Markdown. Finally, render your report again to see how the changes affect the final output."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#implementing-changes-in-the-visual-editor",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#implementing-changes-in-the-visual-editor",
    "title": "Data Science and Data Analytics",
    "section": "Implementing changes in the visual editor",
    "text": "Implementing changes in the visual editor\nWith all these changes, the visual editor should show something like this:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-and-citations-in-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-and-citations-in-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Cross-references and citations in Quarto",
    "text": "Cross-references and citations in Quarto\n\nTo write a scientific report or paper, two features are almost always required:\n\nCross references: referring to a figure or a table by a label, so that you can reference it in your text using its number (e.g., “see Figure 1”).\nCitations: citing academic literature using a .bib-file, so that references are properly formatted and listed automatically at the end of the document.\n\nFortunately, the visual editor makes both of these things very easy. To create a cross reference for a plot produced by a code chunk, we need to do two things:\n\nGive the chunk a label starting with fig-, e.g. fig-penguins1.\nProvide a caption for the plot in the fig-cap code chunk option, e.g. “A scatter plot of flipper length against body mass”.\n\nOnce, we have done this, we can then refer to this figure in the text with @fig-penguins1 or insert the cross reference via Insert &gt; Cross Reference."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-in-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-in-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Cross-references in Quarto",
    "text": "Cross-references in Quarto\n\n\nIn this way, it is ensured that wherever we move the chunk or its reference in the report, the reference will always go the correct plot.\nWe can create such references also for inserted images, of course:\n\nSimply click on the three dots next to the inserted image and go to the Attributes tab.\nGive the image a label starting with #fig-, e.g. #fig-cartoon\nYou can then refer to the image by @fig-cartoon (see next slide)."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-in-quarto-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-in-quarto-1",
    "title": "Data Science and Data Analytics",
    "section": "Cross-references in Quarto",
    "text": "Cross-references in Quarto"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#citations-in-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#citations-in-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Citations in Quarto",
    "text": "Citations in Quarto\n\nLet’s say we wanted to cite the paper in which the penguin data was originally published. It can be found here.\nTo insert a citation in the visual editor, simply select Insert &gt; Citation at the place where you want the citation to be. RStudio then gives you several options. The easiest way is to paste the DOI of the paper:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#citations-in-quarto-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#citations-in-quarto-1",
    "title": "Data Science and Data Analytics",
    "section": "Citations in Quarto",
    "text": "Citations in Quarto\nAt the next rendering, Quarto will create a .bib file holding the added references, add the citation at the place where you have inserted it and append a list of references to the end of your document."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats",
    "text": "Quarto formats\n\nOne of the many great things about Quarto is that it offers many different output formats in which to render our report.\nSwitching between output formats is very easy: simply change the format in the YAML header.\nFor reports, the most common formats are:\n\npdf: what we were using for our penguin report so far.\nhtml: an HTML document with more flexibility than a PDF (e.g. animations).\ndocx: a Microsoft Word document.\n\nFor presentations, the most common formats are:\n\nrevealjs: an HTML-based presentation format allowing for interactivity, animations, dynamic content and much more.\npptx: a Microsoft PowerPoint presentation."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-switching-to-html",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-switching-to-html",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Switching to HTML",
    "text": "Quarto formats – Switching to HTML\n\nAs you may have noticed, the penguin emojis we included into our Quarto document were not rendered into the output PDF. This is because the PDF output format cannot handle emojis.\nTo remedy that, let’s switch to html in the YAML header:\ntitle: \"Penguins in Antarctica\"\nauthor: \"Julian Amon\"\ndate: today\nformat: html\nbibliography: references.bib\nWhen we now click on , the report will be generated as an HTML and not as a PDF. Note how it changes in appearance slightly and now correctly displays the emojis we added earlier."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-switching-to-html-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-switching-to-html-1",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Switching to HTML",
    "text": "Quarto formats – Switching to HTML"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Presentation with revealjs",
    "text": "Quarto formats – Presentation with revealjs\n\nAs indicated before, Quarto allows us not only to generate reports, but also presentations.\nWhile in principle, we could also simply switch the format to one of the presentation formats (e.g. revealjs or pptx) to generate a presentation, there are some special things to consider when doing that.\nPresentations work by dividing your content into slides. To do this with Quarto, the different level headings receive a special meaning when the output format is revealjs or pptx:\n\nSecond level headings (starting with ##) demarcate slides, i.e. a new second level heading will create a new slide.\nFirst level headings (starting with #) indicate the beginning of a new section with a section title slide."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs-1",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Presentation with revealjs",
    "text": "Quarto formats – Presentation with revealjs\nA really basic presentation for the penguin example could look like this:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs-2",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs-2",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Presentation with revealjs",
    "text": "Quarto formats – Presentation with revealjs\nWhen rendered, this looks like this:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#presentations-with-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#presentations-with-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Presentations with Quarto",
    "text": "Presentations with Quarto\n\nQuarto has an incredible set of features for presentations. In fact, all of the slides for this course were created with Quarto!\nSome of these features include:\n\nCode highlighting and animations:\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\nSlide transitions\nTabsets (as in tabs splitting plot and code, for example)\nInteractive slides (see example on the next slide)\nand countless more…\n\nBasically, whatever you want to do, Quarto makes it possible! Thus, we cannot even remotely cover everything, features are best explored by yourself!"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#presentations-with-quarto-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#presentations-with-quarto-1",
    "title": "Data Science and Data Analytics",
    "section": "Presentations with Quarto",
    "text": "Presentations with Quarto"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#further-references-for-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#further-references-for-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Further references for Quarto",
    "text": "Further references for Quarto\n\nThe main resource is the Quarto website. There you can find:\n\nBasic tutorials for getting started.\nSpecific guides on topics like presentations, computations, books, tools, authoring, websites, publishing, etc.\nA reference that explains all options for the possible output formats.\nA demo presentation made with revealjs that showcases some more of the cool features that Quarto offers for presentations.\n\nA Markdown tutorial for practice with elementary Markdown.\nA Markdown table generator that gives you the option to create Markdown tables in a very convenient way (including importing from Excel or similar).\nThe R for Data Science book, which is freely available online. In its last two chapters, it also contains a great introduction to Quarto."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#the-data-science-workflow-model",
    "href": "slides/05_The_Data_Science_Workflow_III.html#the-data-science-workflow-model",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Model",
    "text": "The Data Science workflow – Model"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#time-to-put-the-data-to-use",
    "href": "slides/05_The_Data_Science_Workflow_III.html#time-to-put-the-data-to-use",
    "title": "Data Science and Data Analytics",
    "section": "Time to put the data to use…",
    "text": "Time to put the data to use…\n\nHaving imported, tidied, transformed and visualized our data, now it is time to actually put this data to good use…\nThe goal of the modeling stage of the data science workflow is therefore to detect patterns and extract information from data.\nThe tools and processes we use for this purpose stem from a field called machine learning (or statistical learning).\nTo motivate our study of this field, let’s begin with a simple example:\n\nWe are statistical consultants hired to investigate the association between advertising and sales of a particular product.\nWe are given a data set consisting of the sales of that product in 200 different markets, along with advertising budgets for three different media: social_media, radio and newspaper."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example",
    "title": "Data Science and Data Analytics",
    "section": "Machine learning – A motivating example",
    "text": "Machine learning – A motivating example\n\nOur client cannot directly increase sales of course… But, they can control advertising expenses in each of the three media.\nOur goal is therefore to develop an accurate model that can be used to predict sales on the basis of the three media budgets."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example-1",
    "title": "Data Science and Data Analytics",
    "section": "Machine learning – A motivating example",
    "text": "Machine learning – A motivating example\n\nIn this setting, the advertising budgets are input variables:\n\nInput variables are also called predictors or features.\nWe denote them by \\(X\\) with a subscript to distinguish them.\nIn the example, \\(X_1\\) is the social_media budget, \\(X_2\\) the radio budget and \\(X_3\\) the newspaper budget.\n\nOn the other hand, sales is an output variable:\n\nThe output variable is also called the response (variable) or label.\nWe denote it by \\(Y\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example-2",
    "title": "Data Science and Data Analytics",
    "section": "Machine learning – A motivating example",
    "text": "Machine learning – A motivating example\n\nUsing this notation more generally, suppose that we observe a quantitative response \\(Y\\) and \\(p\\) different predictors \\(X_1, X_2, \\ldots, X_p\\).\nWe assume that there is some relationship between \\(Y\\) and \\(X = (X_1, X_2, \\ldots, X_p)\\), which can be written in the very general form \\[Y = f(X) + \\epsilon,\\] where \\(f\\) is some fixed but unknown function of \\(X_1, \\ldots, X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero.\nWe can think of \\(f\\) as the systematic information that \\(X\\) provides about \\(Y\\).\nA large part of the machine learning literature revolves around different approaches for estimating \\(f\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#why-estimate-f-prediction",
    "href": "slides/05_The_Data_Science_Workflow_III.html#why-estimate-f-prediction",
    "title": "Data Science and Data Analytics",
    "section": "Why estimate \\(f\\)? – Prediction",
    "text": "Why estimate \\(f\\)? – Prediction\n\nEstimate \\(f\\) for prediction purposes: For inputs \\(X\\), we can predict \\(Y\\) using \\[\\hat{Y} = \\hat{f}(X),\\] where \\(\\hat{f}\\) represents our estimate for \\(f\\) and \\(\\hat{Y}\\) represents the resulting prediction for \\(Y\\).\nThe accuracy of \\(\\hat{Y}\\) as a prediction for \\(Y\\) depends on two quantities:\n\nReducible error: \\(\\hat{f}\\) will not be a perfect estimate for \\(f\\) and this inaccuracy will introduce some error. However, it is reducible by using more appropriate learning techniques or collecting more data.\nIrreducible error: Even if we knew \\(f\\), our prediction would have some error in it. This is because of the random error term \\(\\epsilon\\) affecting \\(Y\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#why-estimate-f-inference",
    "href": "slides/05_The_Data_Science_Workflow_III.html#why-estimate-f-inference",
    "title": "Data Science and Data Analytics",
    "section": "Why estimate \\(f\\)? – Inference",
    "text": "Why estimate \\(f\\)? – Inference\n\nWe might also estimate \\(f\\) for inference purposes, i.e. to understand the association between \\(Y\\) and \\(X_1, \\ldots, X_p\\).\nIn this context, one may be interested in answering the following questions:\n\nWhich predictors are associated with the response? Often only a small subset of the available predictors are important in explaining \\(Y\\).\nWhat is the relationship between the response and each predictor? Is it positive, negative, linear, non-linear, …?\n\nIn the advertising example, specific inference-related questions are:\n\nWhich media are associated with sales?\nWhich media generate the biggest boost in sales?\nHow large of an increase in sales is associated with a given increase in social media advertising?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#how-do-we-estimate-f",
    "href": "slides/05_The_Data_Science_Workflow_III.html#how-do-we-estimate-f",
    "title": "Data Science and Data Analytics",
    "section": "How do we estimate \\(f\\)?",
    "text": "How do we estimate \\(f\\)?\n\nWe assume that we have observed a set of \\(n\\) different data points. Let \\(x_{ij}\\) represent the value of the \\(j\\)th predictor for observation \\(i\\), where \\(i = 1, 2, \\ldots, n\\) and \\(j = 1, 2, \\ldots, p\\). Correspondingly, let \\(y_i\\) represent the response variable for observation \\(i\\).\nWith this, our data set consists of \\[\\{ (x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n) \\},\\] where \\(x_i = (x_{i1}, \\ldots, x_{ip})\\). These observations are called the training data because we will use them to train our method on how to estimate \\(f\\).\nOur goal with this is to estimate the unknown function \\(f\\), i.e. find a function \\(\\hat{f}\\) such that \\(Y \\approx \\hat{f}(X)\\) for any observation \\((X, Y)\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#parametric-methods-for-estimating-f",
    "href": "slides/05_The_Data_Science_Workflow_III.html#parametric-methods-for-estimating-f",
    "title": "Data Science and Data Analytics",
    "section": "Parametric methods for estimating \\(f\\)",
    "text": "Parametric methods for estimating \\(f\\)\n\nBroadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric.\nParametric methods involve a two-step model-based approach:\n\nMake an assumption about the function form of \\(f\\). For example, one very simple assumption is that \\(f\\) is linear in \\(X\\): \\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\\]\nFind and apply a procedure that uses training data to fit or train the model. In case of the linear model, that means finding values of parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) such that: \\[Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#non-parametric-methods-for-estimating-f",
    "href": "slides/05_The_Data_Science_Workflow_III.html#non-parametric-methods-for-estimating-f",
    "title": "Data Science and Data Analytics",
    "section": "Non-parametric methods for estimating \\(f\\)",
    "text": "Non-parametric methods for estimating \\(f\\)\n\nNon-parametric methods do not make explicit assumptions about the functional form of \\(f\\). Instead, they seek an estimate of \\(f\\) that gets as close to the training data points as possible without being too wiggly.\nAdvantage: can accommodate a much wider range of possible shapes for \\(f\\) compared to parametric approaches.\nDisadvantage: do not reduce the problem of estimating \\(f\\) to a small number of parameters \\(\\to\\) a very large number of observations is needed.\n\n\n\n\n\n\n\n\nParametric fit\n\n\n\n\n\n\nNon-parametric fit"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#estimating-f-as-part-of-supervised-learning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#estimating-f-as-part-of-supervised-learning",
    "title": "Data Science and Data Analytics",
    "section": "Estimating \\(f\\) as part of supervised learning",
    "text": "Estimating \\(f\\) as part of supervised learning\n\nFitting a model for the purposes of prediction or inference based on labeled training data \\((x_i, y_i)\\) characterizes a class of machine learning problems called supervised learning.\nFor these problems, there is an associated output \\(y_i\\) for each input \\(x_i\\). These labels act as a “supervisor” guiding the learning process, similar to a student learning under a teacher’s guidance."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-vs.-classification-problems",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-vs.-classification-problems",
    "title": "Data Science and Data Analytics",
    "section": "Regression vs. classification problems",
    "text": "Regression vs. classification problems\n\nSupervised learning problems can be further sub-categorized into regression and classification problems depending on the type of response variable:\n\nProblems with a quantitative response are called regression problems, e.g. modeling sales on the basis of the three media budgets.\nProblems with a categorical response are called classification problems, e.g. modeling consumer credit default based on economic variables. Depending on the number of categories, these problems are either binary (e.g. yes/no) or multiclass (e.g. Adelie/Chinstrap/Gentoo).\n\nWhile we select supervised learning methods on the basis of whether the response is quantitative or categorical, the type of the predictors is less important. Typically, methods can be applied with both quantitative and categorical predictors."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning",
    "title": "Data Science and Data Analytics",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nBy contrast, unsupervised learning describes the somewhat more challenging situation in which we only observe a vector of measurements \\(x_i\\), but no associated response \\(y_i\\).\nWe cannot fit a model because there is no response variable to predict.\nWhat sort of analysis can we do then?\n\nClustering: Assigning observations into relatively distinct groups, e.g. market segmentation: grouping customers into clusters based on socio-demographic characteristics.\nOutlier detection: Identifying data points that significantly deviate from the norm within a data set, potentially revealing errors, unusual events, or important discoveries.\n…"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-clustering",
    "href": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-clustering",
    "title": "Data Science and Data Analytics",
    "section": "Unsupervised learning – Clustering",
    "text": "Unsupervised learning – Clustering"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-overview-with-examples",
    "href": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-overview-with-examples",
    "title": "Data Science and Data Analytics",
    "section": "Machine learning – Overview with examples",
    "text": "Machine learning – Overview with examples"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-measuring-the-quality-of-fit",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-measuring-the-quality-of-fit",
    "title": "Data Science and Data Analytics",
    "section": "Regression – Measuring the quality of fit",
    "text": "Regression – Measuring the quality of fit\n\nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to quantify how far the predicted response values are from the true response values.\nFor regression problems, the most commonly-used measure is the mean squared error (MSE): \\[\\text{MSE}_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\]\n\nThe MSE will be small if the predicted responses are very close to the true responses.\nThe MSE will be large if the predicted responses and the true responses differ substantially."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test MSE",
    "text": "Training vs. test MSE\n\nAs we use our \\(n\\) training observations to compute the MSE on the previous slide, we call this the training MSE.\nHowever, in general, we do not really care about how well the method works on the training data. Rather, we are interested in the prediction accuracy on previously unseen test data.\nConsider the following examples:\n\nWhen fitting a model to predict a stock price, we do not really care about how well it does on historical stock prices. Instead, we care about how well it predicts tomorrow’s stock price.\nWhen estimating a model to predict whether someone has diabetes based on clinical measurements (e.g. weight, blood pressure, age, …), we want to use this model to predict diabetes risk for future patients, not for the ones used for training."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-1",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test MSE",
    "text": "Training vs. test MSE\n\nMathematically speaking, suppose we have a set of \\(n'\\) previously unseen test observations \\((x_i', y_i')\\) that were not used to train the machine learning method. Then we can compute the test MSE as: \\[\\text{MSE}_{\\text{test}} = \\frac{1}{n'} \\sum_{i=1}^{n'} (y_i' - \\hat{f}(x_i'))^2\\]\nWe can use this quantity to compare different methods: we want to select the method for which it is as small as possible.\nNow, how do we go about minimizing test MSE on unseen data? Can’t we just select the method that minimizes training MSE? Turns out this is not a good idea at all…"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-2",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test MSE",
    "text": "Training vs. test MSE\n\n\nSuppose we observe the data points in the left panel, which come from the true \\(f\\) shown in black. We estimate it using three different methods with increasing levels of flexibility (low: orange, medium: blue, high: green).\nThe right panel shows the corresponding training and test MSEs."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-3",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test MSE",
    "text": "Training vs. test MSE\n\nClearly, the best fit is achieved by the blue curve, the orange (linear) one is too inflexible, the green one is too flexible (too wiggly).\nThe comparison of train and test MSE as a function of flexibility reveals an interesting pattern:\n\nThe training MSE is monotonically decreasing: the more flexible the method, the better we fit our training data.\nHowever, the all-important test MSE exhibits a U-shape: it initially decreases with increasing flexibility, but then levels off and eventually increases again.\n\nThis is a fundamental property of statistical learning that holds regardless of data set and statistical method:\nAs model flexibility increases, the training MSE will decrease, but the test MSE may not."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting",
    "href": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting",
    "title": "Data Science and Data Analytics",
    "section": "Over- and underfitting",
    "text": "Over- and underfitting\n\nThe green fit has small training MSE, but high test MSE. We call this overfitting:\n\nWith its high flexibility, this method is fitting patterns in the training data that are just caused by random chance rather than by true properties of \\(f\\).\nThe test MSE will then be very large because the supposed patterns that the method found in the training data simply do not exist in the test data.\n\nConversely, the orange (linear) fit has large training MSE and large test MSE. We call this underfitting:\n\nWith its low flexibility, namely its assumption of linearity, this method is not able to fit the non-linearity of the true \\(f\\) well, even on the training data.\nBy increasing flexibility from here, we are able to use more information contained in \\(X\\) to improve the fit.\n\nConsequently, the blue fit with medium-level flexibility is close to optimal."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting-1",
    "title": "Data Science and Data Analytics",
    "section": "Over- and underfitting",
    "text": "Over- and underfitting\nLet’s see two more examples.\n\n\n\n\nHere, the true \\(f\\) is approximately linear.\nThe U-shape in test MSE is still there, but because the truth is close to linear, the test MSE only decreases slightly before increasing again.\n\n\n\n\nHere, the true \\(f\\) is highly non-linear.\nThe MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off",
    "href": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off",
    "title": "Data Science and Data Analytics",
    "section": "The bias-variance trade-off",
    "text": "The bias-variance trade-off\n\nThe U-shape observed in test MSE curves turns out to be the result of two competing properties of statistical learning methods: \\[\\text{Expected test MSE} = \\text{Var}(\\hat{f}) + \\text{Bias}(\\hat{f})^2 + \\text{Var}(\\epsilon)\\]\nSo we want methods that simultaneously achieve low variance and low bias. What is meant by these terms in this context?\n\nVariance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. In general, more flexible statistical methods have higher variance.\nBias refers to the error that is introduced by approximating a complicated real-life problem by a much simpler model. In general, more flexible statistical methods result in less bias."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off-1",
    "title": "Data Science and Data Analytics",
    "section": "The bias-variance trade-off",
    "text": "The bias-variance trade-off\nBelow, we decompose the three MSE curves from earlier into their three constituent parts: the variance of \\(\\hat{f}\\), its squared bias and \\(\\text{Var}(\\epsilon)\\), the irreducible error (represented by the dashed line):"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off-2",
    "title": "Data Science and Data Analytics",
    "section": "The bias-variance trade-off",
    "text": "The bias-variance trade-off\n\nThese plots reveal the following insights:\n\nAs the method’s flexibility increases, the variance increases and the bias decreases.\nThe relative rate of change of these two quantities determine whether the test MSE (as the sum of the two plus \\(\\text{Var}(\\epsilon)\\)) increases or decreases.\nThis is what we call the bias-variance trade-off: bias and variance need to be balanced optimally to minimize test MSE.\nThis trade-off is different for every data set and method applied to it.\n\nDue to the particularities of different data sets, there is no free lunch in statistical learning: no one method optimally trades off bias and variance on all possible data sets."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-measuring-the-quality-of-fit",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-measuring-the-quality-of-fit",
    "title": "Data Science and Data Analytics",
    "section": "Classification – Measuring the quality of fit",
    "text": "Classification – Measuring the quality of fit\n\nWith MSE, our discussion of model performance has so far focused on the regression setting. But many concepts we encountered easily transfer over to the classification setting.\nOne thing that does change is the metric for measuring the quality of fit. With \\(y_1, \\ldots, y_n\\) now categorical, the most common approach for quantifying model accuracy is the training error rate \\(ER\\): \\[ER_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n I(y_i \\neq \\hat{y}_i),\\] where \\(\\hat{y}_i\\) is the predicted class label for the \\(i\\)th observation using \\(\\hat{f}\\) and \\(I(y_i \\neq \\hat{y}_i)\\) is an indicator variable that equals 1 if \\(\\hat{y}_i \\neq y_i\\) and 0 if \\(\\hat{y}_i = y_i\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-error-rate",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-error-rate",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test error rate",
    "text": "Training vs. test error rate\n\nAs in the regression context, we are mostly interested in the prediction accuracy on previously unseen test data.\nTherefore, if we have a test set of \\(n'\\) observations \\((x_i',y_i')\\) that were not used to train our method, we can compute the test error rate as: \\[ER_{\\text{test}} = \\frac{1}{n'} \\sum_{i=1}^n I(y_i' \\neq \\hat{y}_i')\\]\nAgain, we can use this quantity to compare different classification methods: we want to select the method for which the test error rate is as small as possible.\nLet’s consider an example…"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-example",
    "title": "Data Science and Data Analytics",
    "section": "Classification example",
    "text": "Classification example\nSuppose we wanted to predict whether an individual will default on his or her credit card payment on the basis of annual income (\\(X_1\\)) and monthly credit card balance (\\(X_2\\)). Then we might use the \\(x\\) and the \\(y\\) axis to display the two predictors and colour to indicate default (orange) or no default (blue), like so:\n\n\n\n\n\n\n\n\n\nOrange and blue circles indicate training observations.\nFor each value of \\(X_1\\) and \\(X_2\\), there is a different probability of the response being orange or blue.\nThe orange shaded region reflects the set of points for which \\(\\Pr(Y = \\text{orange}|X) &gt; 0.5\\).\nThe purple line indicates the true decision boundary."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-example-1",
    "title": "Data Science and Data Analytics",
    "section": "Classification example",
    "text": "Classification example\n\nThe displayed data set is based on simulation, so that we know the true decision boundary.\nTypically, this is not the case of course and we only observe training data points. The goal then is to come as close as possible to the (unknown) true decision boundary.\nNote two parallels to the regression setting:\n\nEven if we knew the true decision boundary, we would commit errors. This is because there is some irreducible error.\nIn choosing a classification method, we also have to trade off bias and variance to avoid over-/underfitting."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting-regression-vs.-classification",
    "href": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting-regression-vs.-classification",
    "title": "Data Science and Data Analytics",
    "section": "Over- and underfitting – Regression vs. classification",
    "text": "Over- and underfitting – Regression vs. classification"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data",
    "href": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data",
    "title": "Data Science and Data Analytics",
    "section": "How to get test data",
    "text": "How to get test data\n\nTo assess when we are likely to overfit, we need some way of measuring the MSE (regression) or ER (classification) on unseen test data. How do we do that?\nAnswer: use the training data in a smart way!\nThere are many different approaches, two prominent ones are:\n\nTrain/test split: split the data randomly into a train and test sample and compute error on the test sample.\nK-fold cross-validation: split the entire data randomly into \\(K\\) folds. Fit the model using the \\(K-1\\) folds and evaluate the model using the remaining fold. Repeat this process until every fold has served as the test set."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data-traintest-split",
    "href": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data-traintest-split",
    "title": "Data Science and Data Analytics",
    "section": "How to get test data – Train/test split",
    "text": "How to get test data – Train/test split"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data-k-fold-cross-validation",
    "href": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data-k-fold-cross-validation",
    "title": "Data Science and Data Analytics",
    "section": "How to get test data – K-fold cross validation",
    "text": "How to get test data – K-fold cross validation"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression",
    "title": "Data Science and Data Analytics",
    "section": "Introduction to linear regression",
    "text": "Introduction to linear regression\n\nLet’s finally get to building our first machine learning model!\nOne of the most widely used tools for predicting a quantitative variable is linear regression.\nAs the name suggests, linear regression assumes that the dependence of a response variable \\(Y\\) on predictors \\(X_1, \\ldots, X_p\\) is linear.\nEven if true relationships are rarely linear, this simple approach is extremely useful both conceptually and practically.\nLet’s start with the simplest case: predicting a quantitative variable \\(Y\\) on the basis of a single predictor \\(X\\). We assume therefore: \\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon,\\] where \\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters or coefficients."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "Introduction to linear regression",
    "text": "Introduction to linear regression\n\nIn the case of simple linear regression, there are only two coefficients:\n\nThe slope coefficient \\(\\beta_1\\). It specifies how much \\(Y\\) changes on average for a given change in \\(X\\).\nThe intercept \\(\\beta_0\\). It specifies the value that \\(Y\\) takes on average, if \\(X = 0\\).\n\nGiven some estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we can predict future values of \\(Y\\) by using the regression line: \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x,\\] where \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of \\(X\\) being equal to \\(x\\).\nFor the purposes of illustration, let’s come back to the example with sales as a function of advertising budgets."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression-2",
    "title": "Data Science and Data Analytics",
    "section": "Introduction to linear regression",
    "text": "Introduction to linear regression\nSuppose we are only interested in the relationship between the social_media budget and sales. Our goal is to obtain estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), such that \\(y_i \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\) for \\(i = 1, \\ldots, n\\). There are multiple ways to do this."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-through-ols",
    "href": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-through-ols",
    "title": "Data Science and Data Analytics",
    "section": "Estimating coefficients through OLS",
    "text": "Estimating coefficients through OLS\n\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\) be the fitted value for \\(Y\\) based on the \\(i\\)th observation for \\(X\\). Then \\(e_i = y_i - \\hat{y}_i\\) represents the \\(i\\)th residual.\nOne possibility to choose the estimates for the coefficients is to minimize the residual sum of squares (RSS): \\[RSS = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2.\\]\nThe values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize this expression are called ordinary least squares (OLS) estimators and are given by: \\[\\hat{\\beta}_1 = \\frac{\\frac{1}{n} \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\frac{1}{n} \\sum_i (x_i - \\bar{x})^2}, \\quad \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x},\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) denote the means of the variables."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-example",
    "title": "Data Science and Data Analytics",
    "section": "Estimating coefficients – Example",
    "text": "Estimating coefficients – Example\n\nEstimating the parameters for the simple linear regression where social_media is \\(X\\) and sales is \\(Y\\), we get: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot X = 7.0326 + 0.0475 \\cdot X\\]\nInterpretation of the coefficients:\n\n\\(\\hat{\\beta}_0 = 7.0326\\): when no money is invested in social media advertising, the model expects average sales of 7.0326 units.\n\\(\\hat{\\beta}_1 = 0.0475\\): for every additional 100 dollars spent on social media advertising, the expected sales increase by around \\(100 \\cdot 0.0475 \\approx 4.75\\) units on average."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-example-1",
    "title": "Data Science and Data Analytics",
    "section": "Estimating coefficients – Example",
    "text": "Estimating coefficients – Example"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit",
    "href": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit",
    "title": "Data Science and Data Analytics",
    "section": "Assessing the goodness-of-fit",
    "text": "Assessing the goodness-of-fit\n\nHow well a regression line fits the data is described by its goodness-of-fit. It is typically assessed by the coefficient of determination \\(R^2\\).\nThe \\(R^2\\) measures the proportion of explained variance: \\[R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2},\\] where \\(TSS\\) is the total sum of squares as defined by the expression in the denominator.\nThe value of the \\(R^2\\) lies between 0 and 1:\n\n\\(R^2 = 1\\): 100% of the variance of \\(Y\\) can be explained by \\(X\\).\n\\(R^2 = 0\\): 0% of the variance of \\(Y\\) can be explained by \\(X\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-1",
    "title": "Data Science and Data Analytics",
    "section": "Assessing the goodness-of-fit",
    "text": "Assessing the goodness-of-fit"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#statistical-properties-of-ols-estimators",
    "href": "slides/05_The_Data_Science_Workflow_III.html#statistical-properties-of-ols-estimators",
    "title": "Data Science and Data Analytics",
    "section": "Statistical properties of OLS estimators",
    "text": "Statistical properties of OLS estimators\n\nUnder certain assumptions on the underlying true regression model, we can derive sampling distributions of the OLS estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), i.e. how these estimators will be distributed when computing them over several data sets.\nWe can use these sampling distributions use for testing hypotheses like \\[H_0: \\beta_1 = 0, \\quad \\quad H_1: \\beta_1 \\neq 0\\]\n… or to compute two-sided confidence intervals like \\[[\\hat{\\beta}_1 - t_{1-\\alpha/2;n-2} \\cdot \\hat{\\sigma}_{\\hat{\\beta}_1}; \\hat{\\beta}_1 + t_{1-\\alpha/2;n-2} \\cdot \\hat{\\sigma}_{\\hat{\\beta}_1}],\\] where \\(t_{1-\\alpha/2;n-2}\\) is the \\((1-\\alpha/2)\\)-quantile of a \\(t\\)-distribution with \\(n-2\\) degrees of freedom and \\(\\hat{\\sigma}_{\\hat{\\beta}_1}\\) is the standard error of the estimator of the slope coefficient \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\nHowever, this is a data science course and not a statistics course, so we will not compute these quantities by hand. Instead, we want to use R for all this.\nTo estimate a linear regression model in R, we need two things to pass into the function lm (which stands for “linear model”):\n\nA data.frame holding the variables we want included in the model.\nA formula that specifies the model. In simple linear regression, this will take the form of response ~ predictor.\n\nSo, in our advertising example, we would do the following:\n\nadvertising &lt;- read.csv(\"../data/advertising.csv\")\nadvertising &lt;- advertising[, -c(1, 2, 7)]\n\nsimple_reg &lt;- lm(sales ~ social_media, data = advertising)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\nWe can then have a look at the result using the summary function:\n\nsimple_reg_summary &lt;- summary(simple_reg)\nsimple_reg_summary\n\n\nCall:\nlm(formula = sales ~ social_media, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.032594   0.457843   15.36   &lt;2e-16 ***\nsocial_media 0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nThis output contains a lot of useful information. First of all:\n\nCall: summary of the estimated regression model\nResiduals: a five-point-summary of the residuals \\(e_i\\)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\n\nThen, it displays a table called Coefficients with six columns:\n\n1st column: name of the variable pertaining to the corresponding coefficient (here only (Intercept) and social_media).\n2nd column (Estimate): OLS estimate of the coefficient, so \\(\\hat{\\beta}_i\\) for \\(i = 0, 1\\).\n3rd column (Std. Error): Estimate of the coefficient standard errors, so \\(\\hat{\\sigma}_{\\hat{\\beta}_i}\\).\n4th column (t value): Test statistic \\(t\\) for the two-sided hypothesis test against 0, i.e. to test the null hypothesis \\(H_0: \\beta_i = 0\\) against the alternative \\(H_1: \\beta_i \\neq 0\\).\n5th column (Pr(&gt;|t|)): \\(p\\)-value of the corresponding hypothesis test.\n6th column: “stars of significance”\n\nFinally, additional information is displayed:\n\nResidual standard error: Estimate of the square root of \\(\\text{Var}(\\epsilon)\\).\nMultiple R-squared: \\(R^2\\)\nAdjusted R-squared / F-statistic: see example with multiple predictors."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-accessor-functions",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-accessor-functions",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R – Accessor functions",
    "text": "Linear Regression in R – Accessor functions\n\nFrom the created model object simple_reg, we can extract:\n\nThe coefficient estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) using the function coef:\n\n\ncoef(simple_reg)\n\n (Intercept) social_media \n  7.03259355   0.04753664 \n\n\n\nThe fitted values \\(\\hat{y}_i\\) using the function fitted:\n\n\nhead(fitted(simple_reg))\n\n        1         2         3         4         5         6 \n17.970775  9.147974  7.850224 14.234395 15.627218  7.446162 \n\n\n\nThe residuals \\(e_i = y_i - \\hat{y}_i\\) using the function residuals:\n\n\nhead(residuals(simple_reg))\n\n         1          2          3          4          5          6 \n 4.1292255  1.2520260  1.4497762  4.2656054 -2.7272181 -0.2461623 \n\n\n\nThe residual standard error using the function sigma:\n\n\nsigma(simple_reg)\n\n[1] 3.258656"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-accessor-functions-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-accessor-functions-1",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R – Accessor functions",
    "text": "Linear Regression in R – Accessor functions\n\n\nConfidence intervals for all regression coefficients at level \\(1 - \\alpha\\) using the function confint and its level argument:\n\n\nconfint(simple_reg, level = 0.95)\n\n                  2.5 %     97.5 %\n(Intercept)  6.12971927 7.93546783\nsocial_media 0.04223072 0.05284256\n\n\n\n\n\nAdditionally, we can extract the following quantities from the created model summary object simple_reg_summary:\n\nThe coefficient estimates, their standard errors, \\(t\\) and \\(p\\) values using the function coef:\n\n\ncoef(simple_reg_summary)\n\n               Estimate  Std. Error  t value    Pr(&gt;|t|)\n(Intercept)  7.03259355 0.457842940 15.36028 1.40630e-35\nsocial_media 0.04753664 0.002690607 17.66763 1.46739e-42\n\n\n\nThe \\(R^2\\) by accessing the r.squared field of the corresponding list:\n\n\nsimple_reg_summary$r.squared\n\n[1] 0.6118751"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-prediction",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-prediction",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R – Prediction",
    "text": "Linear Regression in R – Prediction\n\nIf we are interested in prediction rather than inference, we need a way to predict new data points. In R, this functionality is provided by the predict function.\nIt requires the estimated model we want to use for prediction and the values of the features for which predictions should be generated in a data.frame.\nSuppose, we want to predict sales for social_media budgets of 100 and 200:\n\nnew_data &lt;- data.frame(social_media = c(100, 200))\npredict(simple_reg, newdata = new_data)\n\n       1        2 \n11.78626 16.53992 \n\n\nNote that the names of the features (here only social_media) in newdata have to be identical to their names in the training data:\n\nnew_data &lt;- data.frame(social_med = c(100, 200))\npredict(simple_reg, newdata = new_data)\n\nError in eval(predvars, data, env): Objekt 'social_media' nicht gefunden"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r",
    "title": "Data Science and Data Analytics",
    "section": "K-fold cross-validation in R",
    "text": "K-fold cross-validation in R\n\nNow, we have all we need to estimate the test MSE of our model using K-fold cross validation! Let’s say for \\(K = 5\\).\nLet’s go step by step: first, we randomly assign each observation to one of the 5 folds. The randomness is once again introduced via the function sample:\n\nn &lt;- nrow(advertising)\nK &lt;- 5\n\nadvertising$fold &lt;- sample(rep(1:K, length.out = n))\nhead(advertising, 10)\n\n   social_media radio newspaper sales fold\n1         230.1  37.8      69.2  22.1    1\n2          44.5  39.3      45.1  10.4    1\n3          17.2  45.9      69.3   9.3    5\n4         151.5  41.3      58.5  18.5    5\n5         180.8  10.8      58.4  12.9    3\n6           8.7  48.9      75.0   7.2    4\n7          57.5  32.8      23.5  11.8    1\n8         120.2  19.6      11.6  13.2    3\n9           8.6   2.1       1.0   4.8    3\n10        199.8   2.6      21.2  10.6    2"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "K-fold cross-validation in R",
    "text": "K-fold cross-validation in R\n\nNext, we write a loop, in which for every fold \\(k = 1, ..., 5\\):\n\nWe fit our simple linear regression model on all data except the \\(k\\)th fold.\nUsing this model, we predict sales on the \\(k\\)th fold.\nSince we know the true sales for all observations, we can then compute the MSE on the \\(k\\)th fold.\nSave the MSE of the \\(k\\)th fold in a vector.\n\n\nmses_linear &lt;- numeric(K)\n\nfor(k in 1:K){\n  m &lt;- lm(sales ~ social_media, data = subset(advertising, fold != k))\n  preds &lt;- predict(m, newdata = subset(advertising, fold == k))\n  mse &lt;- mean((preds - advertising$sales[advertising$fold == k])^2)\n  mses_linear[k] &lt;- mse\n}\nmses_linear\n\n[1]  7.03702 10.98168 12.21465 10.85549 12.54239"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "K-fold cross-validation in R",
    "text": "K-fold cross-validation in R\n\nFinally, we can compute the cross-validated MSE by calculating the mean of the MSEs across the 5 folds:\n\nmean(mses_linear)\n\n[1] 10.72624\n\n\nSince the MSE lives on a squared scale (due to the squaring in its computation), we can bring the scale back to the original linear scale by taking the square root. The result is called the root mean squared error (RMSE):\n\nsqrt(mean(mses_linear))\n\n[1] 3.275095\n\n\nIn and of itself, these numbers do not tell us too much yet. However, we will be comparing them against the cross-validated (R)MSE of other methods, to see which one is the best."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#incorporating-non-linearities",
    "href": "slides/05_The_Data_Science_Workflow_III.html#incorporating-non-linearities",
    "title": "Data Science and Data Analytics",
    "section": "Incorporating non-linearities",
    "text": "Incorporating non-linearities\n\nThe term linear in “linear regression” actually refers only to linearity in parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nWe can therefore easily incorporate non-linearities by appropriately defining the dependent variable \\(Y\\) and the independent variable \\(X\\). For example:\n\n\\(\\log(Y) = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1 \\cdot \\log(X) + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1 \\cdot \\sqrt{X} + \\epsilon\\)\n\nLet’s estimate each of these models in our advertising example and assess their test MSEs using 5-fold cross validation again. First, we add the log and square root of the relevant variable to our data set:\n\nadvertising$log_sales &lt;- log(advertising$sales)\nadvertising$log_social_media &lt;- log(advertising$social_media)\nadvertising$sqrt_social_media &lt;- sqrt(advertising$social_media)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#incorporating-non-linearities-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#incorporating-non-linearities-1",
    "title": "Data Science and Data Analytics",
    "section": "Incorporating non-linearities",
    "text": "Incorporating non-linearities\n\nNow, let’s assess the test MSE of each of the three above models using 5-fold cross validation:\n\nmses_nonlinear &lt;- matrix(0, K, 3)\n\nfor(k in 1:K){\n  m1 &lt;- lm(log_sales ~ social_media, data = subset(advertising, fold != k))\n  m2 &lt;- lm(log_sales ~ log_social_media, data = subset(advertising, fold != k))\n  m3 &lt;- lm(sales ~ sqrt_social_media, data = subset(advertising, fold != k))\n\n  preds1 &lt;- exp(predict(m1, newdata = subset(advertising, fold == k)))\n  preds2 &lt;- exp(predict(m2, newdata = subset(advertising, fold == k)))\n  preds3 &lt;- predict(m3, newdata = subset(advertising, fold == k))\n\n  mses_nonlinear[k, 1] &lt;- mean((preds1 - advertising$sales[advertising$fold == k])^2)\n  mses_nonlinear[k, 2] &lt;- mean((preds2 - advertising$sales[advertising$fold == k])^2)\n  mses_nonlinear[k, 3] &lt;- mean((preds3 - advertising$sales[advertising$fold == k])^2)\n}\ncolMeans(mses_nonlinear)\n\n[1] 11.83003 10.71977 10.39436\n\n\nThe model \\(Y = \\beta_0 + \\beta_1 \\cdot \\sqrt{X}\\) gives the lowest test MSE out of the three and lower than the linear model as well, albeit not by much."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nTo improve our model even further, we probably have to include additional features into it. For instance, sales will probably also depend on the radio and newspaper advertising budget.\nIn the context of the linear model, we can include additional variables to create a multiple linear regression of the form \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\\]\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed (i.e. ceteris paribus).\nAs before, the regression coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are unknown and need to be estimated from the training data."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-ols-estimation",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-ols-estimation",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression – OLS estimation",
    "text": "Multiple linear regression – OLS estimation\n\nAs in the simple linear regression case, we choose the parameters to minimize the residual sum of squares (RSS): \\[RSS = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\ldots + \\hat{\\beta}_px_{pi}))^2\\]\nThe values of \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize this expression are complicated formulas of the data that are most easily represented using matrix algebra. These formulas do not really matter to us, as R computes them for us anyway.\nEstimating parameters for the multiple linear regression where sales is \\(Y\\), social_media is \\(X_1\\), radio is \\(X_2\\) and newspaper is \\(X_3\\), we get: \\[\\hat{Y} = 2.9389 + 0.0458 \\cdot X_1 + 0.1885 \\cdot X_2 - 0.0010 \\cdot X_3\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-ols-estimation-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-ols-estimation-1",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression – OLS estimation",
    "text": "Multiple linear regression – OLS estimation\n\nLet’s again go through the interpretation of each of those coefficients:\n\n\\(\\hat{\\beta}_0 = 2.9389\\): when no money is invested in advertising through any of the three channels, the model expects average sales of 2.9389 units.\n\\(\\hat{\\beta}_1 = 0.0458\\): for every additional 100 dollars spent on social media advertising ceteris paribus, the expected sales increase by around \\(100 \\cdot 0.0458 \\approx 4.58\\) units on average.\n\\(\\hat{\\beta}_2 = 0.1885\\): for every additional 100 dollars spent on radio advertising ceteris paribus, the expected sales increase by around \\(100 \\cdot 0.1885 \\approx 18.85\\) units on average.\n\\(\\hat{\\beta}_3 = -0.0010\\): for every additional 100 dollars spent on newspaper advertising ceteris paribus, the expected sales decrease by around \\(100 \\cdot 0.0010 \\approx 0.10\\) units on average."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-2",
    "title": "Data Science and Data Analytics",
    "section": "Assessing the goodness-of-fit",
    "text": "Assessing the goodness-of-fit\n\nThe \\(R^2\\) is still computed and interpreted in the same way as before: \\[R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\nIt now measures the proportion of the variance of \\(Y\\) explained jointly by the predictors \\(X_1, \\ldots, X_p\\). Alternatively, one can say that it measures the proportion of the variance of \\(Y\\) explained by the model.\nWhen adding additional variables to the model, the \\(R^2\\) never decreases and usually increases. This makes it a poor tool for deciding whether an additional variable should be added to the model."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-3",
    "title": "Data Science and Data Analytics",
    "section": "Assessing the goodness-of-fit",
    "text": "Assessing the goodness-of-fit\n\nAn alternative to the \\(R^2\\) that is the so-called adjusted \\(R^2\\): \\[R_{\\text{adj}}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\\]\nIt no longer has the interpretation of the \\(R^2\\), but it is more useful for comparing models. Consider the following example:\n\nThe simple linear regression of sales on social_media had an adjusted \\(R^2\\) of 0.6099 (see corresponding R output from earlier).\nThe multiple linear regression of sales on all three advertising budgets has an adjusted \\(R^2\\) of 0.8956 (see upcoming R output).\nTherefore, the adjusted \\(R^2\\) strongly suggests that the bigger model is superior to the smaller model."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#statistical-properties-of-ols-estimators-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#statistical-properties-of-ols-estimators-1",
    "title": "Data Science and Data Analytics",
    "section": "Statistical properties of OLS estimators",
    "text": "Statistical properties of OLS estimators\n\nUsing the standard assumptions of the multiple linear regression model, we can again derive sampling distributions of the OLS estimators \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\) and thus construct \\(t\\)-tests for hypotheses like \\[H_0: \\beta_j = 0, \\quad \\quad H_1: \\beta_j \\neq 0\\]\nAdditionally, we can test the null hypothesis that none of the coefficients are useful in predicting the response, i.e. \\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0, \\quad \\quad H_1: \\beta_j \\neq 0 \\text{ für min. ein } j,\\] using the \\(F\\)-statistic \\[F = \\frac{n-p-1}{p} \\cdot \\frac{TSS - RSS}{RSS} \\sim F_{p, n-p-1}\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression in R",
    "text": "Multiple linear regression in R\n\nHowever, again, this course focuses on how to use R to estimate these types of models and compute these types of statistics.\nTo estimate a multiple linear regression in R, we simply combine the predictors on the right side of the ~ using a + sign, i.e. response ~ predictor1 + predictor2 + ... + predictorp\nSo, to estimate the model with all three predictors in our advertising example, we would run the following line of code:\n\nmultiple_reg &lt;- lm(sales ~ social_media + radio + newspaper, data = advertising)\n\nFortunately, many of the other functionalities we have seen in simple linear regression neatly generalize to the multiple linear regression case!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression in R",
    "text": "Multiple linear regression in R\n\nAgain, we inspect the result using the summary function:\n\nsummary(multiple_reg)\n\n\nCall:\nlm(formula = sales ~ social_media + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.938889   0.311908   9.422   &lt;2e-16 ***\nsocial_media  0.045765   0.001395  32.809   &lt;2e-16 ***\nradio         0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper    -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\nAll predictors except for newspaper have a significant influence on sales on the 0.1% significance level.\nAround 89.7% of the variance of sales can be explained with this model."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#comparing-model-performance",
    "href": "slides/05_The_Data_Science_Workflow_III.html#comparing-model-performance",
    "title": "Data Science and Data Analytics",
    "section": "Comparing model performance",
    "text": "Comparing model performance\nBut how does its test MSE hold up to the models estimated so far? Let’s investigate this question using our cross-validation routine again:\n\n\n\nmses_big &lt;- numeric(K)\n\nfor(k in 1:K){\n  m &lt;- lm(sales ~ social_media + radio + newspaper,\n          data = subset(advertising, fold != k))\n  preds &lt;- predict(m, newdata = subset(advertising, fold == k))\n  mse &lt;- mean((preds - advertising$sales[advertising$fold == k])^2)\n  mses_big[k] &lt;- mse\n}\n\nresults &lt;- data.frame(model_id = factor(1:5),\n                      model_names = model_names,\n                      test_mse = c(mean(mses_linear),\n                                   colMeans(mses_nonlinear),\n                                   mean(mses_big)))\nknitr::kable(results[,-1], col.names = c(\"Model\", \"Test MSE\"),\n             digits = 2, row.names = TRUE)\n\n\n\n\n\n\nModel\nTest MSE\n\n\n\n\n1\n\\(Y \\sim X_1\\)\n10.73\n\n\n2\n\\(\\log(Y) \\sim X_1\\)\n11.83\n\n\n3\n\\(\\log(Y) \\sim \\log(X_1)\\)\n10.72\n\n\n4\n\\(Y \\sim \\sqrt{X_1}\\)\n10.39\n\n\n5\n\\(Y \\sim X_1 + X_2 + X_3\\)\n3.06\n\n\n\n\n\nUnsurprisingly, the model that uses all three advertising budgets as predictors is much better than any of the models that just uses the social media budget!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#comparing-model-performance-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#comparing-model-performance-1",
    "title": "Data Science and Data Analytics",
    "section": "Comparing model performance",
    "text": "Comparing model performance\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(results, aes(x = model_id, y = test_mse, fill = model_id)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Model ID\", y = \"Test MSE\",\n       title = \"Test MSE of the linear models estimated\",\n       subtitle = \"Test MSE determined via 5-fold cross validation\",\n       caption = \"Source: advertising data\") +\n  guides(fill = \"none\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#categorical-predictors",
    "href": "slides/05_The_Data_Science_Workflow_III.html#categorical-predictors",
    "title": "Data Science and Data Analytics",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nSo far, all the predictors we have looked at have been quantitative variables. In practice, we often have to deal with categorical predictors as well.\nTo see an example, let’s consider the Credit data set from the ISLR package. It contains socio-economic information on 400 credit card customers.\nThe data set contains seven quantitative variables:\n\nlibrary(ISLR)\nhead(Credit[, c(\"Income\", \"Limit\", \"Rating\", \"Cards\", \"Age\", \"Education\", \"Balance\")])\n\n   Income Limit Rating Cards Age Education Balance\n1  14.891  3606    283     2  34        11     333\n2 106.025  6645    483     3  82        15     903\n3 104.593  7075    514     4  71        11     580\n4 148.924  9504    681     3  36        11     964\n5  55.882  4897    357     2  68        16     331\n6  80.180  8047    569     4  77        10    1151\n\n\nThe goal here is to build a predictive model for balance, the average credit card balance of a given customer."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#categorical-predictors-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#categorical-predictors-1",
    "title": "Data Science and Data Analytics",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nHowever, aside from the quantitative variables, the data set also contains four qualitative (categorical) variables:\n\nhead(Credit[, c(\"Gender\", \"Student\", \"Married\", \"Ethnicity\")])\n\n  Gender Student Married Ethnicity\n1   Male      No     Yes Caucasian\n2 Female     Yes     Yes     Asian\n3   Male      No      No     Asian\n4 Female      No      No     Asian\n5   Male      No     Yes Caucasian\n6   Male      No      No Caucasian\n\n\nSo how can we include the information in these qualitative predictors to benefit our linear model for balance?\nThe answer lies in dummy variables, i.e. variables that only take on the values 0 or 1. For a categorical variable with \\(K\\) levels, we create \\(K-1\\) dummy variables. Let’s see an example of how this works."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables",
    "text": "Dummy variables\n\nThe variable Ethnicity takes on \\(K = 3\\) different values:\n\ntable(Credit$Ethnicity)\n\n\nAfrican American            Asian        Caucasian \n              99              102              199 \n\n\nSo, we can encode the information in the Ethnicity variable in \\(K - 1 = 2\\) dummy variables, called \\(d_1\\) and \\(d_2\\). For example, one could be used to indicate Asian ethnicity and the other to indicate Caucasian: \\[d_{1i} = \\begin{cases}\n1 & \\text{if person } i \\text{ is of Asian ethnicity} \\\\\n0 & \\text{otherwise}\n\\end{cases}\\] \\[d_{2i} = \\begin{cases}\n1 & \\text{if person } i \\text{ is of Caucasian ethnicity} \\\\\n0 & \\text{otherwise}\n\\end{cases}\\] Note that a third dummy variable is not necessary."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-1",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables",
    "text": "Dummy variables\n\nWe could manually add these dummy variables to the data set:\n\nCredit$Asian &lt;- ifelse(Credit$Ethnicity == \"Asian\", 1, 0)\nCredit$Caucasian &lt;- ifelse(Credit$Ethnicity == \"Caucasian\", 1, 0)\nhead(Credit[,c(\"Ethnicity\", \"Asian\", \"Caucasian\")], 8)\n\n         Ethnicity Asian Caucasian\n1        Caucasian     0         1\n2            Asian     1         0\n3            Asian     1         0\n4            Asian     1         0\n5        Caucasian     0         1\n6        Caucasian     0         1\n7 African American     0         0\n8            Asian     1         0\n\n\nWith the categorical data now numerically encoded, we can estimate the following model: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 d_{1i} + \\hat{\\beta}_2 d_{2i},\\] where \\(y_i\\) is the \\(i\\)th observation of the balance variable."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-2",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\nNote that since \\(d_{1i}\\) and \\(d_{2i}\\) can only take two possible values (0 or 1), this model can only predict three distinct values: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 d_{1i} + \\hat{\\beta}_2 d_{2i} = \\begin{cases}\n\\hat{\\beta}_0 + \\hat{\\beta}_1 & \\text{if } i \\text{ is Asian} \\\\\n\\hat{\\beta}_0 + \\hat{\\beta}_2 & \\text{if } i \\text{ is Caucasian} \\\\\n\\hat{\\beta}_0 & \\text{if } i \\text{ is African American} \\\\\n\\end{cases}\\]\nTherefore, we can interpret the regression coefficients as follows:\n\n\\(\\hat{\\beta}_0\\) is the average credit card balance of people of African American ethnicity. This category constitutes the so-called reference category.\n\\(\\hat{\\beta}_1\\) is the difference in average credit card balance between people of Asian ethnicity and the reference category (African American).\n\\(\\hat{\\beta}_2\\) is the difference in average credit card balance between people of Caucasian ethnicity and the reference category (African American)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables in R",
    "text": "Dummy variables in R\n\nLet’s estimate this model with R. Fortunately, as long as the categorical variables are properly encoded as factors, we do not have to create the dummy variables ourselves. Let’s check:\n\nclass(Credit$Ethnicity)\n\n[1] \"factor\"\n\nlevels(Credit$Ethnicity)\n\n[1] \"African American\" \"Asian\"            \"Caucasian\"       \n\n\nThis looks good! Note that R will always automatically use the first factor level as the reference category when estimating a linear model with factors:\n\ncredit_model1 &lt;- lm(Balance ~ Ethnicity, data = Credit)\n\nNote that with the proper factor encoding of Ethnicity, we do not need to make use of the dummy variables Asian and Caucasian we manually added to the data set."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables in R",
    "text": "Dummy variables in R\n\nLet’s have a look at the created model object:\n\nsummary(credit_model1)\n\n\nCall:\nlm(formula = Balance ~ Ethnicity, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-531.00 -457.08  -63.25  339.25 1480.50 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          531.00      46.32  11.464   &lt;2e-16 ***\nEthnicityAsian       -18.69      65.02  -0.287    0.774    \nEthnicityCaucasian   -12.50      56.68  -0.221    0.826    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.9 on 397 degrees of freedom\nMultiple R-squared:  0.0002188, Adjusted R-squared:  -0.004818 \nF-statistic: 0.04344 on 2 and 397 DF,  p-value: 0.9575\n\n\nThis shows that on average…\n\npeople of African American ethnicity have a balance of \\(531\\).\npeople of Asian ethnicity have a balance of \\(531-18.69=512.31\\).\npeople of Caucasian ethnicity have a balance of \\(531-12.50=518.50\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables in R",
    "text": "Dummy variables in R\n\nHowever, as the regression output shows, these differences in average credit card balances by ethnicity are clearly not significant.\nNote that the group averages inferred from the regression coefficients genuinely correspond to the group averages:\n\nround(tapply(Credit$Balance, Credit$Ethnicity, mean), 2)\n\nAfrican American            Asian        Caucasian \n          531.00           512.31           518.50 \n\n\nBased on this approach of dummy variable encoding, we can include any number of categorical variables into the regression equation.\nAs an example, the next slide displays the regression output from using all available predictors (quantitative and qualitative) to model credit card balance in the usual additive fashion."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#quantitative-and-qualitative-predictors",
    "href": "slides/05_The_Data_Science_Workflow_III.html#quantitative-and-qualitative-predictors",
    "title": "Data Science and Data Analytics",
    "section": "Quantitative and qualitative predictors",
    "text": "Quantitative and qualitative predictors\n\ncredit_model2 &lt;- lm(Balance ~ Income + Limit + Rating + Cards + Age + Education +\n                              Gender + Student + Married + Ethnicity,\n                    data = Credit)\nsummary(credit_model2)\n\n\nCall:\nlm(formula = Balance ~ Income + Limit + Rating + Cards + Age + \n    Education + Gender + Student + Married + Ethnicity, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-161.64  -77.70  -13.49   53.98  318.20 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -479.20787   35.77394 -13.395  &lt; 2e-16 ***\nIncome               -7.80310    0.23423 -33.314  &lt; 2e-16 ***\nLimit                 0.19091    0.03278   5.824 1.21e-08 ***\nRating                1.13653    0.49089   2.315   0.0211 *  \nCards                17.72448    4.34103   4.083 5.40e-05 ***\nAge                  -0.61391    0.29399  -2.088   0.0374 *  \nEducation            -1.09886    1.59795  -0.688   0.4921    \nGenderFemale        -10.65325    9.91400  -1.075   0.2832    \nStudentYes          425.74736   16.72258  25.459  &lt; 2e-16 ***\nMarriedYes           -8.53390   10.36287  -0.824   0.4107    \nEthnicityAsian       16.80418   14.11906   1.190   0.2347    \nEthnicityCaucasian   10.10703   12.20992   0.828   0.4083    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 98.79 on 388 degrees of freedom\nMultiple R-squared:  0.9551,    Adjusted R-squared:  0.9538 \nF-statistic: 750.3 on 11 and 388 DF,  p-value: &lt; 2.2e-16\n\n\nWith over 95% variance explained, this is a very good model for credit card balance. However, is it reasonable to leave all of the predictors in the model? Or are we better off removing some of them?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#variable-selection",
    "href": "slides/05_The_Data_Science_Workflow_III.html#variable-selection",
    "title": "Data Science and Data Analytics",
    "section": "Variable selection",
    "text": "Variable selection\n\nThe task of determining which predictors are associated with the response and are thus to be included in the model, is referred to as variable selection.\nWe can decide which variables to select on the basis of statistics like\n\ncross-validated MSE (the lower the better).\nadjusted \\(R^2\\) (the higher the better).\nAkaike information criterion (AIC, the lower the better)\n\nMost direct approach: best subset regression: compute OLS fit for every subset of variables and choose the best according to some metric:\n\nUnfortunately, there are a total of \\(2^p\\) models to consider.\nFor \\(p = 2\\), that’s 4 models, but for \\(p = 30\\) that is 1,073,741,824 models to estimate! This is not practical.\n\nInstead, we need a more efficient approach to select a smaller set of models."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#forward-selection",
    "href": "slides/05_The_Data_Science_Workflow_III.html#forward-selection",
    "title": "Data Science and Data Analytics",
    "section": "Forward selection",
    "text": "Forward selection\n\nOne approach to this task is forward selection (using AIC):\n\nBegin with a null model, i.e. a model that contains no predictors.\nFit \\(p\\) simple linear regressions (one for each predictor individually) and add to the null model the variable that results in the lowest AIC.\nAdd to that model the variable that results in the lowest AIC for the new two-variable model.\nContinue until some stopping rule is satisfied, e.g. no model with a smaller AIC can be fitted.\n\nIn R, forward selection can be achieved with the help of the step function. It requires the null model (as a lower bound) and the full model with all variables (as an upper bound) as input."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#forward-selection-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#forward-selection-1",
    "title": "Data Science and Data Analytics",
    "section": "Forward selection",
    "text": "Forward selection\n\nm0 &lt;- lm(Balance ~ 1, data = Credit)\ncredit_model_fw &lt;- step(m0, scope = Balance ~ Income + Limit + Rating + Cards + Age +\n                                    Education + Gender + Student + Married + Ethnicity,\n                        direction = \"forward\")\n\nStart:  AIC=4905.56\nBalance ~ 1\n\n            Df Sum of Sq      RSS    AIC\n+ Rating     1  62904790 21435122 4359.6\n+ Limit      1  62624255 21715657 4364.8\n+ Income     1  18131167 66208745 4810.7\n+ Student    1   5658372 78681540 4879.8\n+ Cards      1    630416 83709496 4904.6\n&lt;none&gt;                   84339912 4905.6\n+ Gender     1     38892 84301020 4907.4\n+ Education  1      5481 84334431 4907.5\n+ Married    1      2715 84337197 4907.5\n+ Age        1       284 84339628 4907.6\n+ Ethnicity  2     18454 84321458 4909.5\n\nStep:  AIC=4359.63\nBalance ~ Rating\n\n            Df Sum of Sq      RSS    AIC\n+ Income     1  10902581 10532541 4077.4\n+ Student    1   5735163 15699959 4237.1\n+ Age        1    649110 20786012 4349.3\n+ Cards      1    138580 21296542 4359.0\n+ Married    1    118209 21316913 4359.4\n&lt;none&gt;                   21435122 4359.6\n+ Education  1     27243 21407879 4361.1\n+ Gender     1     16065 21419057 4361.3\n+ Limit      1      7960 21427162 4361.5\n+ Ethnicity  2     51100 21384022 4362.7\n\nStep:  AIC=4077.41\nBalance ~ Rating + Income\n\n            Df Sum of Sq      RSS    AIC\n+ Student    1   6305322  4227219 3714.2\n+ Married    1     95068 10437473 4075.8\n+ Limit      1     94545 10437996 4075.8\n+ Age        1     90286 10442255 4076.0\n&lt;none&gt;                   10532541 4077.4\n+ Education  1     20819 10511722 4078.6\n+ Ethnicity  2     67040 10465501 4078.9\n+ Cards      1      2094 10530447 4079.3\n+ Gender     1       948 10531593 4079.4\n\nStep:  AIC=3714.24\nBalance ~ Rating + Income + Student\n\n            Df Sum of Sq     RSS    AIC\n+ Limit      1    194718 4032502 3697.4\n+ Age        1     44620 4182600 3712.0\n&lt;none&gt;                   4227219 3714.2\n+ Married    1     13083 4214137 3715.0\n+ Gender     1     12168 4215051 3715.1\n+ Cards      1     10608 4216611 3715.2\n+ Education  1      1400 4225820 3716.1\n+ Ethnicity  2     22322 4204897 3716.1\n\nStep:  AIC=3697.37\nBalance ~ Rating + Income + Student + Limit\n\n            Df Sum of Sq     RSS    AIC\n+ Cards      1    166410 3866091 3682.5\n+ Age        1     37952 3994549 3695.6\n&lt;none&gt;                   4032502 3697.4\n+ Gender     1     13345 4019157 3698.0\n+ Married    1      6660 4025842 3698.7\n+ Education  1      5795 4026707 3698.8\n+ Ethnicity  2     17704 4014797 3699.6\n\nStep:  AIC=3682.52\nBalance ~ Rating + Income + Student + Limit + Cards\n\n            Df Sum of Sq     RSS    AIC\n+ Age        1     44472 3821620 3679.9\n&lt;none&gt;                   3866091 3682.5\n+ Gender     1     11350 3854741 3683.3\n+ Education  1      5672 3860419 3683.9\n+ Married    1      3121 3862970 3684.2\n+ Ethnicity  2     14756 3851335 3685.0\n\nStep:  AIC=3679.89\nBalance ~ Rating + Income + Student + Limit + Cards + Age\n\n            Df Sum of Sq     RSS    AIC\n&lt;none&gt;                   3821620 3679.9\n+ Gender     1   10860.9 3810759 3680.7\n+ Married    1    5450.6 3816169 3681.3\n+ Education  1    5241.7 3816378 3681.3\n+ Ethnicity  2   11517.3 3810102 3682.7\n\n\nThe final model found with forward selection contains predictors Rating, Income, Student, Limit, Cards and Age. It has an AIC of 3679.9. The object credit_model_fw can be used like any other lm object."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#backward-selection",
    "href": "slides/05_The_Data_Science_Workflow_III.html#backward-selection",
    "title": "Data Science and Data Analytics",
    "section": "Backward selection",
    "text": "Backward selection\n\nAnother approach to the task of variable selection is backward selection:\n\nBegin with a full model, i.e. a model that contains all available predictors.\nFit \\(p\\) individual linear regressions, removing one variable from the full model at a time. Choose the model that has the lowest AIC.\nRepeat with the resulting model and continue until some stopping rule is satisfied, e.g. no model with a smaller AIC can be fitted.\n\nIn R, backward selection can also be achieved with the help of the step function. It only requires the full model (as an upper bound) and the direction backward as input."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#backward-selection-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#backward-selection-1",
    "title": "Data Science and Data Analytics",
    "section": "Backward selection",
    "text": "Backward selection\n\ncredit_model_bw &lt;- step(credit_model2, direction = \"backward\")\n\nStart:  AIC=3686.22\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student + Married + Ethnicity\n\n            Df Sum of Sq      RSS    AIC\n- Ethnicity  2     14084  3800814 3683.7\n- Education  1      4615  3791345 3684.7\n- Married    1      6619  3793349 3684.9\n- Gender     1     11269  3798000 3685.4\n&lt;none&gt;                    3786730 3686.2\n- Age        1     42558  3829288 3688.7\n- Rating     1     52314  3839044 3689.7\n- Cards      1    162702  3949432 3701.0\n- Limit      1    331050  4117780 3717.7\n- Student    1   6326012 10112742 4077.1\n- Income     1  10831162 14617892 4224.5\n\nStep:  AIC=3683.7\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student + Married\n\n            Df Sum of Sq      RSS    AIC\n- Married    1      4545  3805359 3682.2\n- Education  1      4757  3805572 3682.2\n- Gender     1     10760  3811574 3682.8\n&lt;none&gt;                    3800814 3683.7\n- Age        1     45650  3846464 3686.5\n- Rating     1     49473  3850287 3686.9\n- Cards      1    166806  3967621 3698.9\n- Limit      1    340250  4141064 3716.0\n- Student    1   6372573 10173387 4075.5\n- Income     1  10838891 14639705 4221.1\n\nStep:  AIC=3682.18\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student\n\n            Df Sum of Sq      RSS    AIC\n- Education  1      5399  3810759 3680.7\n- Gender     1     11019  3816378 3681.3\n&lt;none&gt;                    3805359 3682.2\n- Age        1     43545  3848904 3684.7\n- Rating     1     46929  3852289 3685.1\n- Cards      1    170729  3976088 3697.7\n- Limit      1    352112  4157472 3715.6\n- Student    1   6461978 10267338 4077.2\n- Income     1  10860901 14666261 4219.8\n\nStep:  AIC=3680.75\nBalance ~ Income + Limit + Rating + Cards + Age + Gender + Student\n\n          Df Sum of Sq      RSS    AIC\n- Gender   1     10861  3821620 3679.9\n&lt;none&gt;                  3810759 3680.7\n- Age      1     43983  3854741 3683.3\n- Rating   1     49520  3860279 3683.9\n- Cards    1    170898  3981656 3696.3\n- Limit    1    347654  4158413 3713.7\n- Student  1   6472072 10282831 4075.8\n- Income   1  10855780 14666539 4217.8\n\nStep:  AIC=3679.89\nBalance ~ Income + Limit + Rating + Cards + Age + Student\n\n          Df Sum of Sq      RSS    AIC\n&lt;none&gt;                  3821620 3679.9\n- Age      1     44472  3866091 3682.5\n- Rating   1     49263  3870883 3683.0\n- Cards    1    172930  3994549 3695.6\n- Limit    1    347898  4169518 3712.7\n- Student  1   6462599 10284218 4073.9\n- Income   1  10845018 14666638 4215.8\n\n\nThe final model found with backward selection also contains predictors Rating, Income, Student, Limit, Cards and Age. With an AIC of 3679.9, it is the same model that was found with forward selection. The object credit_model_bw can be used like any other lm object."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) regression",
    "text": "K-nearest neighbours (KNN) regression\n\nParametric methods like linear regression make strong assumptions about the form of \\(f(X)\\) (namely linearity in all predictors).\nBy contrast, non-parametric methods do not assume a parametric form for \\(f(X)\\), thereby providing a more flexible alternative for regression tasks.\nOne of the simplest non-parametric regression methods is K-nearest neighbours (KNN) regression, which works as follows:\n\nGiven a value for \\(K\\) and a prediction point \\(x_0\\), we first identify the \\(K\\) training observations that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nWe then estimate \\(f(x_0)\\) using the average of all the training responses in \\(\\mathcal{N}_0\\), i.e. formally we have \\[\\hat{f}(x_0) = \\frac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-2",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) regression",
    "text": "K-nearest neighbours (KNN) regression\n\nPlots of \\(\\hat{f}(X)\\) using KNN regression on a two-dimensional data set with 64 observations (orange dots). Left: \\(K = 1\\) results in a rough step function fit. Right: \\(K = 9\\) produces a much smoother fit."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-3",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) regression",
    "text": "K-nearest neighbours (KNN) regression\n\nThe number of neighbours \\(K\\) is what we call a hyperparameter. It changes the exact behaviour of the algorithm: a small value for \\(K\\) provides lots of flexibility (low bias, high variance), while a large value for \\(K\\) provide a smoother, less variable fit (high bias, low variance).\nThe optimal value for \\(K\\) will be different in every data set and can be determined via cross-validation, for instance.\nSo, how will the performance of KNN and linear regression compare? That depends on the true form of \\(f\\):\n\nIf \\(f\\) is close to linear, then linear regression will generally be better.\nIf \\(f\\) is highly non-linear, then KNN regression will generally be better.\nAs we do not know the true \\(f\\), we have to judge which one is better based on estimates of test MSE, individually for every data set under investigation."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-vs.-linear-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-vs.-linear-regression",
    "title": "Data Science and Data Analytics",
    "section": "KNN vs. linear regression",
    "text": "KNN vs. linear regression\n\nPlots of \\(\\hat{f}(X)\\) using KNN regression on a one-dimensional data set with 50 observations. Clearly, when the true relationship (black line) is linear, KNN regression is sub-optimal, regardless of whether \\(K = 1\\) (left) or \\(K = 9\\) (right)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-vs.-linear-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-vs.-linear-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "KNN vs. linear regression",
    "text": "KNN vs. linear regression\n\nFor a non-linear relationship between \\(X\\) and \\(Y\\), however, KNN outperforms linear regression. On the left, we see KNN fits for \\(K = 1\\) (blue) and \\(K = 9\\) (red) compared to the true relationship (black). On the right, we see that the test set MSE for linear regression (horizontal black line) is significantly higher than the test set MSE for KNN with different values of \\(K\\) (displayed as \\(1/K\\))."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#caveats-of-knn-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#caveats-of-knn-regression",
    "title": "Data Science and Data Analytics",
    "section": "Caveats of KNN regression",
    "text": "Caveats of KNN regression\n\nFirst caveat of KNN regression: The curse of dimensionality:\n\nAs we saw, with small \\(p\\), KNN typically performs slightly worse than linear regression when \\(f\\) is linear, but much better if \\(f\\) is non-linear.\nHowever, when \\(p\\) becomes larger (with \\(n\\) constant), KNN performance typically deteriorates.\nThis is because, in high dimensions (e.g. \\(p = 50\\)), the \\(K\\) observations that are nearest to \\(x_0\\) may still be very far away from \\(x_0\\), leading to a very poor prediction of \\(f(x_0)\\).\n\nAs a general rule therefore: parametric methods will tend to outperform non-parametric approaches when the number of observations per predictor (i.e. \\(n/p\\)) is small."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#caveats-of-knn-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#caveats-of-knn-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "Caveats of KNN regression",
    "text": "Caveats of KNN regression\n\nSecond caveat of KNN regression: feature scaling:\n\nSince KNN predicts a test observation by identifying the training observations that are nearest to it, the scale of the variables matter.\nIf all features are measured on the same scale (like in the advertising example), there is nothing to worry about.\nHowever, consider the features salary (in USD) and age (in years): for the KNN, a difference of 1000 USD in salary is enormous compared to a difference of 50 years in age.\nConsequently, all the predictions will be driven by salary rather than age.\n\nAs a general rule therefore: when feature scales differ, perform variable standardization before doing KNN regression."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r",
    "title": "Data Science and Data Analytics",
    "section": "KNN regression in R",
    "text": "KNN regression in R\nIn R, KNN regression can be achieved with the help of the FNN package. It provides a function called knn.reg, which requires a data frame of features, a vector of responses and a value for \\(K\\) to estimate a KNN model. Let’s apply that to our advertising example, say for \\(K = 3\\):\n\nlibrary(FNN)\n\nknn_ad &lt;- knn.reg(train = advertising[, 1:3], y = advertising$sales, k = 3)\nknn_ad\n\nPRESS =  373.3867 \nR2-Predict =  0.9310732 \n\n\n\nAs there are no coefficients to interpret or inferential statistics to analyse, the pure model fit is not that interesting.\nInstead, the value of KNN regression lies mostly in prediction.\nLet’s therefore see how KNN regression with different values of \\(K\\) performs in cross-validation compared to the best-performing linear regression model from earlier."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "KNN regression in R",
    "text": "KNN regression in R\nWith the knn.reg function, we can do fitting and predicting in one function call. This requires passing the prediction features as the test argument to the function. We use this functionality in our existing cross-validation scheme:\n\nn_neighbours &lt;- 1:10 # number of neighbours considered\nmses_knn &lt;- matrix(NA, K, length(n_neighbours))\ncolnames(mses_knn) &lt;- sprintf(\"K = %d\", n_neighbours)\n\nfor(k in 1:K){\n  preds &lt;- lapply(n_neighbours, function(x) knn.reg(train = subset(advertising, fold != k, 1:3),\n                                                    test = subset(advertising, fold == k, 1:3),\n                                                    y = advertising$sales[advertising$fold != k],\n                                                    k = x)$pred)\n  mses_knn[k, ] &lt;- colMeans((do.call(cbind, preds) - advertising$sales[advertising$fold == k])^2)\n}\nround(colMeans(mses_knn), 2)\n\n K = 1  K = 2  K = 3  K = 4  K = 5  K = 6  K = 7  K = 8  K = 9 K = 10 \n  2.33   1.94   2.27   2.37   2.49   2.68   2.92   3.24   3.61   3.73 \n\n\nThe best test MSE is achieved by setting \\(K = 2\\). The resulting model has a test MSE of 1.94, which is also considerably better than the best-performing linear model, which had test MSE 3.06."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "KNN regression in R",
    "text": "KNN regression in R"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#an-overview-of-classification",
    "href": "slides/05_The_Data_Science_Workflow_III.html#an-overview-of-classification",
    "title": "Data Science and Data Analytics",
    "section": "An overview of classification",
    "text": "An overview of classification\n\nSo far, we have dealt with situations in which the response variable \\(Y\\) was quantitative. However, often it is qualitative / categorical.\nPredicting a qualitative response is also referred to as classifying that observation, as it involves assigning the observation to a class / category.\nA classifier is an algorithm that maps input data (i.e. the predictors) to a category. Some of them first predict a probability for each of the categories. In this sense, they behave like regression methods.\nIn this first part, we will discuss the following two simple classifiers:\n\nLogistic regression\nKNN classification"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers",
    "href": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers",
    "title": "Data Science and Data Analytics",
    "section": "Evaluation of classifiers",
    "text": "Evaluation of classifiers\n\nIn the introduction, we already saw that a common way of evaluating classifiers is the error rate, i.e. the proportion of misclassified observations.\nHowever, there are many more ways to evaluate classifiers. One typically looks at the joint distribution of observed response and predicted response.\nFor binary classification, we can illustrate this distribution with the help of a so-called confusion matrix:\n\n\n\n\n\n\n\n\n\nobserved/true = 0\nobserved/true = 1\n\n\n\n\npredicted = 0\ntrue negatives (TN)\nfalse negatives (FN)\n\n\npredicted = 1\nfalse positives (FP)\ntrue positives (TP)\n\n\n\nFor an extended version of this table, see here."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-1",
    "title": "Data Science and Data Analytics",
    "section": "Evaluation of classifiers",
    "text": "Evaluation of classifiers"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-2",
    "title": "Data Science and Data Analytics",
    "section": "Evaluation of classifiers",
    "text": "Evaluation of classifiers\n\nFrom this confusion matrix, we can compute several metrics:\n\nAccuracy: percentage of correctly classified observations: \\[accuracy = (TN + TP) / (TN + TP + FN + FP)\\]\nRecall: percentage of correctly classified positives: \\[recall = TP/(FN + TP)\\]\nPrecision: percentage of correctly classified predicted positives: \\[precision = TP/(FP + TP)\\]\nF1 score: harmonic mean of precision and recall \\[F1 = 2 \\cdot recall \\cdot precision/(recall + precision)\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-3",
    "title": "Data Science and Data Analytics",
    "section": "Evaluation of classifiers",
    "text": "Evaluation of classifiers\n\nThe choice of the evaluation measure is application- and data-dependent. One typically evaluates the relative costs of false negatives and false positives.\nConsider the following example:\n\nSay, we have built a classifier that predicts whether someone has cancer based on an MRI image.\nFalse positive: telling someone they have cancer even though they do not.\nFalse negative: telling someone they don’t have cancer, although they do.\nIf the first is considered more problematic, optimize for precision.\nIf the second is considered more problematic, optimize for recall.\nIf both are important, optimize for F1 or accuracy.\n\nHowever, the sole use of accuracy can also be misleading…"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#using-accuracy-for-evaluation",
    "href": "slides/05_The_Data_Science_Workflow_III.html#using-accuracy-for-evaluation",
    "title": "Data Science and Data Analytics",
    "section": "Using accuracy for evaluation",
    "text": "Using accuracy for evaluation\n\nSuppose we have a data set of 100 credit card customers, of which 5 have defaulted on their credit card debt. We want to build a classifier that tells us whether someone will default or not.\nAfter training, the classifier produced the following confusion matrix:\n\n\n\n\n\n\n\n\n\n\nobserved/true = 0\nobserved/true = 1\nTotal\n\n\n\n\npredicted = 0\n94\n4\n98\n\n\npredicted = 1\n1\n1\n2\n\n\nTotal\n95\n5\n100\n\n\n\n\nOur classifier achieves 95% accuracy (5% error rate) on the training data!\nBut recall is only 1/5, only one default was correctly identified.\nPrecision is only 1/2, only one of the two default predictions was correct.\n\nAccuracy can be misleading for unbalanced class distributions!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#click-data-set",
    "href": "slides/05_The_Data_Science_Workflow_III.html#click-data-set",
    "title": "Data Science and Data Analytics",
    "section": "Click data set",
    "text": "Click data set\n\nAdvertising companies want to target advertisements to users based on a user’s likelihood to click.\nFor this purpose, we are using a data set from Kaggle, which contains information about the behaviour of 1000 users.\nA total of 9 variables are available for each user:\n\nThe binary variable clicked_on_ad indicates whether the user clicked the ad or not. This will be our response variable.\nPersonal information such as age, sex, time spent on the website.\nOther information, such as average annual income of the area where the user resides, city and country.\n\n\nclick &lt;- read.csv(\"../data/click.csv\")"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#simple-logistic-regression-model",
    "href": "slides/05_The_Data_Science_Workflow_III.html#simple-logistic-regression-model",
    "title": "Data Science and Data Analytics",
    "section": "Simple logistic regression model",
    "text": "Simple logistic regression model\nLet’s visualize the relationship between area_income and the response clicked_on_ad in a scatter plot (with a slight jitter):\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(click, aes(x = area_income, y = clicked_on_ad)) +\n  geom_jitter(height = 0.02) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#simple-logistic-regression-model-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#simple-logistic-regression-model-1",
    "title": "Data Science and Data Analytics",
    "section": "Simple logistic regression model",
    "text": "Simple logistic regression model\n\nNow, of course, we could simply lay a linear regression line into this scatter plot. This would amount to estimating a regression for the probability of a click (\\(Y = 1\\)) using area income as the explanatory variable \\(X\\): \\[p_{\\text{click}} = P(Y=1|X) = \\beta_0 + \\beta_1X + \\epsilon\\]\nHowever, this would not be appropriate as the possible values lie between \\(-\\infty\\) and \\(\\infty\\), but a probability should be between 0 and 1.\nSo, we need to transform the right-hand side in order to ensure that it lies between 0 and 1. In logistic regression, this transformation is given by the so-called logistic function: \\[p_{\\text{click}} = P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-vs.-logistic-regression-model",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-vs.-logistic-regression-model",
    "title": "Data Science and Data Analytics",
    "section": "Linear vs. logistic regression model",
    "text": "Linear vs. logistic regression model"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#interpretation-of-simple-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#interpretation-of-simple-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Interpretation of simple logistic regression",
    "text": "Interpretation of simple logistic regression\n\nNote that with some re-arrangement of the terms, the simple logistic regression model can also be written as \\[\\log\\left( \\frac{p_{\\text{click}}}{1 - p_{\\text{click}}} \\right) = \\beta_0 + \\beta_1X\\]\nThe left-hand side of this equation are the so-called log odds. In logistic regression, we model the log odds of the success probability as a linear function of the predictors. This implies:\n\nIncreasing \\(X\\) by one unit changes the log odds by \\(\\beta_1\\).\nEquivalently, it multiplies the odds by \\(e^{\\beta_1}\\).\nHowever, because the relationship between \\(p_{\\text{click}}\\) and \\(X\\) is not a straight line, \\(\\beta_1\\) does not correspond to the change in \\(p_{\\text{click}}\\) associated with a one-unit increase in \\(X\\). This is a difference to linear regression."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\nGiven multiple covariates \\(X_1, X_2, \\ldots, X_p\\), we can easily generalize the simple logistic regression model to the following: \\[p = P(Y=1|X_1, \\ldots, X_p) = \\frac{e^{\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p}}\\]\nAgain, we model log odds as a linear function of the predictors: \\[\\log\\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p\\]\n\n\\(\\beta_0\\) gives the baseline log odds for \\(Y=1\\) when \\(X_1 = X_2 = \\ldots = X_p = 0\\).\nA one unit increase in covariate \\(X_j\\) changes the log odds for \\(Y = 1\\) by \\(\\beta_j\\), keeping all other covariates constant (ceteris paribus)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-logistic-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-logistic-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\nIn logistic regression, the linear combination of the covariates \\(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p\\) is called the linear predictor.\nIt has the following interpretation:\n\nIf \\(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p = 0\\), then \\(P(Y=1|X_1, \\ldots, X_p) = 0.5\\), i.e. \\(Y=1\\) and \\(Y=0\\) are equally likely.\nIf \\(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p &gt; 0\\), then \\(P(Y=1|X_1, \\ldots, X_p) &gt; 0.5\\), i.e. \\(Y=1\\) is more likely.\nIf \\(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p &lt; 0\\), then \\(P(Y=1|X_1, \\ldots, X_p) &lt; 0.5\\), i.e. \\(Y=0\\) is more likely.\n\nIn practice, the coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are unknown and must be estimated on the basis of available training data.\nEstimation no longer happens with OLS, but with maximum likelihood (ML), a very common procedure in statistics."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#statistical-inference-in-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#statistical-inference-in-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Statistical inference in logistic regression",
    "text": "Statistical inference in logistic regression\n\nBroadly speaking, the idea of ML estimation is to choose parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) such that the probability of observing the data we observed is maximized. The details are beyond the scope of this course.\nFortunately, R has efficient algorithms for ML estimation of logistic regression models, which we will soon see how to use.\nBesides estimating the regression coefficients, ML estimation allows us to perform inferential statistics akin to the linear regression case:\n\nHypothesis tests for \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\).\nConfidence intervals for regression coefficients.\nGoodness-of-fit measures similar to the \\(R^2\\).\n\nHowever, when using logistic regression as a classification algorithm in machine learning, we are mostly interested in prediction, not inference."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#making-predictions",
    "href": "slides/05_The_Data_Science_Workflow_III.html#making-predictions",
    "title": "Data Science and Data Analytics",
    "section": "Making predictions",
    "text": "Making predictions\n\nOnce the coefficients have been estimated, we can compute the probability for \\(Y=1\\) for every combination of values of the predictors.\nWe get \\(\\hat{p}(x_1, \\ldots, x_p) = \\hat{P}(Y=1|X_1 = x_1, \\ldots, X_p = x_p)\\) by simply plugging in the coefficient estimates \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\) into the logistic regression formula: \\[\\hat{p}(x_1, \\ldots, x_p) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p}}\\]\nThe prediction of the conditional response is straightforward: \\[\\hat{Y}|(X_1 = x_1, \\ldots, X_p = x_p) = \\begin{cases}\n0 & \\text{ if } \\hat{p}(x_1, \\ldots, x_p) &lt; 0.5 \\\\\n1 & \\text{ if } \\hat{p}(x_1, \\ldots, x_p) \\geq 0.5\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nLet’s finally see an example of how to do all this in R!\nTo estimate a logistic regression in R, we need three things to pass into the function glm (which stands for “generalized linear model”):\n\nA data.frame holding the variables we want included in the model, especially the binary response as a factor or numeric.\nA formula that specifies the model, similar to the lm case.\nA family that specifies the type of generalized linear model. For logistic regression, this is always equal to binomial().\n\nSo, if we wanted to use area_income and age as predictors for the response clicked_on_ad, we would do the following:\n\nlogreg &lt;- glm(clicked_on_ad ~ area_income + age, data = click, family = binomial())"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nWe can then have a look at the result using the summary function:\n\nsummary(logreg)\n\n\nCall:\nglm(formula = clicked_on_ad ~ area_income + age, family = binomial(), \n    data = click)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  9.160e-02  5.399e-01    0.17    0.865    \narea_income -1.033e-04  8.207e-06  -12.59   &lt;2e-16 ***\nage          1.626e-01  1.261e-02   12.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1386.3  on 999  degrees of freedom\nResidual deviance:  880.0  on 997  degrees of freedom\nAIC: 886\n\nNumber of Fisher Scoring iterations: 5\n\n\nWhile the output looks somewhat different to the linear regression case, the all-important coefficients matrix with Estimate, Std. Error, z value and Pr(&gt;|z|) looks virtually identical."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nUsing \\(X_1\\) for area_income (in thousand USD) and \\(X_2\\) for age, the estimated model for the click probability \\(p\\) thus has the following form: \\[\\log\\left( \\frac{p}{1 - p} \\right) = 0.0916 - 0.1033 X_1 + 0.1626 X_2\\]\nInterpretation:\n\nCeteris paribus, increasing the area_income by 1000 USD decreases the log odds for clicking on the ad by 0.1033 on average.\nCeteris paribus, increasing the age by one year increases the log odds for clicking on the ad by 0.1626 on average.\n\nThe regression output also shows that both of these variables are highly significant with \\(p\\)-values very close to zero. Both area_income and age seem to be important for predicting whether or not a user will click on the ad."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-3",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nIn general, there is a high degree of consistency in how we can operate on glm objects compared to lm objects. For example, many accessor functions work in the same way as before:\n\nhead(residuals(logreg))\n\n         1          2          3          4          5          6 \n-0.9342594 -0.5191394 -0.5391085 -0.8425931 -0.5408458 -0.4287287 \n\nhead(fitted(logreg))\n\n        1         2         3         4         5         6 \n0.3536540 0.1260681 0.1352536 0.2988136 0.1360644 0.0878074 \n\ncoef(logreg)\n\n  (Intercept)   area_income           age \n 0.0915995255 -0.0001032967  0.1626462670 \n\nconfint(logreg)\n\n                    2.5 %        97.5 %\n(Intercept) -0.9649226490  1.154141e+00\narea_income -0.0001198858 -8.768127e-05\nage          0.1386819426  1.881543e-01\n\n\nDue to the different scales involved (probabilities vs. log odds), some of these functions come with additional options, as we shall now see for prediction."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-prediction",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-prediction",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R – Prediction",
    "text": "Logistic regression in R – Prediction\n\nUsing our model, what do we estimate the probability that a 29-year old living in an area with an average annual income of 50,000 USD clicks on the ad to be?\nLet’s simply plug in to our equation for the log odds: \\[\\log\\left( \\frac{p}{1 - p} \\right) = 0.0916 - 0.1033 \\cdot 50 + 0.1626 \\cdot 29 = -0.358\\]\nClearly, that is a log odds ratio and not a probability. To turn this into a probability, we need to plug into the other formula for logistic regression: \\[p = \\frac{e^{0.0916 - 0.1033 \\cdot 50 + 0.1626 \\cdot 29}}{1 + e^{0.0916 - 0.1033 \\cdot 50 + 0.1626 \\cdot 29}} = \\frac{e^{-0.358}}{1 + e^{-0.358}} \\approx 0.41\\]\nWe predict that this person will click on the ad with a probability of 41%!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-prediction-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-prediction-1",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R – Prediction",
    "text": "Logistic regression in R – Prediction\n\nIgnoring the rounding differences, both of these types of predictions are available through the predict function in R:\n\nWe get the linear predictor, i.e. the log odds for the click, by setting type to link (which is the default):\n\n\nnew_click &lt;- data.frame(area_income = 50000, age = 29)\npredict(logreg, newdata = new_click, type = \"link\")\n\n         1 \n-0.3564913 \n\n\n\nIn the machine learning context, we are typically only interested in the probability. This we get by setting type to response:\n\n\npredict(logreg, newdata = new_click, type = \"response\")\n\n        1 \n0.4118092 \n\n\nWith the ability to predict new observations secured, let’s evaluate the performance of our model as a classifier using 5-fold cross validation!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-evaluation",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-evaluation",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R – Evaluation",
    "text": "Logistic regression in R – Evaluation\n\nAgain, we randomly assign each observation to one of 5 folds:\n\nn &lt;- nrow(click)\nK &lt;- 5\nclick$fold &lt;- sample(rep(1:K, length.out = n))\n\nThis time, we save the class predictions on each hold-out fold together into one variable called class_preds_lr. This will then allow us to evaluate the out-of-sample performance of the classifier:\n\nclick$class_preds_lr &lt;- NA\n\nfor(k in 1:K){\n  m &lt;- glm(clicked_on_ad ~ area_income + age, data = subset(click, fold != k),\n           family = binomial())\n  prob_preds &lt;- predict(m, newdata = subset(click, fold == k), type = \"response\")\n  click$class_preds_lr[click$fold == k] &lt;- ifelse(prob_preds &gt;= 0.5, 1, 0)\n}\n\n\nWe classify an observation as a click if its probability is at least 0.5.\nWe classify an observation as no click if its probability is smaller than 0.5."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-evaluation-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-evaluation-1",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R – Evaluation",
    "text": "Logistic regression in R – Evaluation\n\nNow, we can compute the out-of-sample confusion matrix of our classifier:\n\ntable(click$class_preds, click$clicked_on_ad,\n      dnn = c(\"click_prediction\", \"click_truth\"))\n\n                click_truth\nclick_prediction   0   1\n               0 431 123\n               1  69 377\n\n\nFrom this, we can now calculate the aforementioned performance metrics:\n\nAccuracy: \\((431 + 377)/1000 = 0.808\\). Our logistic regression model correctly predicts the click decision in 80.8% of the observations.\nRecall: \\(377/(123 + 377) = 0.754\\). Our logistic regression model correctly identifies 75.4% of all clicks on an ad.\nPrecision: \\(377/(69 + 377) = 0.845\\). When our logistic regression model predicts a click, it is correct 84.5% of the time.\nF1 score: \\(2 \\cdot 0.754 \\cdot 0.845/(0.754 + 0.845) = 0.797\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#further-topics-on-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#further-topics-on-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Further topics on logistic regression",
    "text": "Further topics on logistic regression\n\nAs we saw, many concepts from linear regression generalize to logistic regression. Further examples of this include:\n\nCategorical predictors: we can use dummy variables to encode categorical variables as predictors also in logistic regression.\nVariable selection: the step function for forward and backward selection works equally on glm objects.\nLinearity: logistic regression is actually linear in the sense that it models a linear decision boundary, i.e. the curve separating observations of different classes is a straight line. It satisfies the following equation (see slide 107): \\[\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p = 0\\]\n\nIn our example with two predictors, we can nicely visualize the estimated decision boundary."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-decision-boundary-in-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-decision-boundary-in-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Linear decision boundary in logistic regression",
    "text": "Linear decision boundary in logistic regression\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(click, aes(x = area_income, y = age, color = factor(clicked_on_ad))) +\n  geom_point() +\n  geom_abline(slope = -coef(logreg)[2]/coef(logreg)[3],\n              intercept = -coef(logreg)[1]/coef(logreg)[3]) +\n  labs(title = \"Linear decision boundary of logistic regression\",\n       subtitle = \"Illustration of linear decision boundary estimated via logistic regression on click data\",\n       color = \"Clicked\") +\n  theme_minimal() +\n  scale_color_colorblind()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-classification-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-classification-1",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) classification",
    "text": "K-nearest neighbours (KNN) classification\n\nSimilar to the regression context, a non-parametric alternative for performing classification is given by K-nearest neighbours (KNN) classification.\nUnlike logistic regression, KNN classification can also be used for multi-class classification, i.e. for when there are more than two classes.\nIn principle, the procedure is straightforward:\n\nGiven a value for \\(K\\) and a prediction point \\(x_0\\), we again start by identifying the \\(K\\) training observations that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nWe then estimate the conditional probability for class \\(j\\) as the fraction of points in \\(\\mathcal{N}_0\\) whose response values equal \\(j\\): \\[\\hat{P}(Y=j|X=x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_0} I(y_i = j)\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-classification-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-classification-2",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) classification",
    "text": "K-nearest neighbours (KNN) classification\nSuppose we have two classes (\\(+\\) and \\(-\\)) and two features \\(X_1\\) and \\(X_2\\). The figure below illustrates how conditional probability would be estimated for a point \\(x\\) and \\(K=1,2,3\\) in a toy example data set:\n\n\\[K = 1, \\hat{P}(+|x) = 0/1 \\quad K = 2, \\hat{P}(+|x) = 1/2 \\quad K = 3, \\hat{P}(+|x) = 2/3\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-in-r",
    "title": "Data Science and Data Analytics",
    "section": "KNN classification in R",
    "text": "KNN classification in R\n\nIn R, KNN classification can also be achieved with the help of the FNN package:\n\nIt provides a function called knn, whose arguments are very similar to the knn.reg function we saw earlier.\nWith this function, we have to perform fitting and prediction in one function call. So let’s run KNN classification for different values of \\(K\\) directly in our existing cross-validation scheme.\nFirst though, we need to standardize our variables, since area_income is measured in USD and age is measured in years. To standardize a variable, we deduct the mean and divide by the standard deviation:\n\n\nclick$ai_scaled &lt;- (click$area_income - mean(click$area_income))/sd(click$area_income)\nclick$age_scaled &lt;- (click$age - mean(click$age))/sd(click$age)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "KNN classification in R",
    "text": "KNN classification in R\n\nNow, we can actually run the KNN classification for \\(K=1, \\ldots, 10\\) and predict each fold in the 5-fold cross validation scheme from earlier:\n\nn_neighbours &lt;- 1:10\n\nfor(nn in n_neighbours){\n  knn_colname &lt;- sprintf(\"class_preds_KNN%d\", nn)\n  click[[knn_colname]] &lt;- factor(NA, levels = c(0, 1))\n  for(k in 1:K){\n    preds &lt;- knn(train = click[click$fold != k, c(\"ai_scaled\", \"age_scaled\")],\n                 test = click[click$fold == k, c(\"ai_scaled\", \"age_scaled\")],\n                 cl = click$clicked_on_ad[click$fold != k],\n                 k = nn)\n    click[click$fold == k, knn_colname] &lt;- preds\n  }\n}\n\nLet’s compare the performance (in terms of classification accuracy on the test set) of these KNN models to the logistic regression model from earlier!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-vs.-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-vs.-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "KNN classification vs. logistic regression",
    "text": "KNN classification vs. logistic regression\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nres_knn &lt;- colMeans(as.matrix(click[, grep(\"_KNN\", names(click), fixed = TRUE)]) == click$clicked_on_ad)\nres_lr &lt;- mean(click$class_preds_lr == click$clicked_on_ad)\nres &lt;- data.frame(model = c(sub(\"class_preds_\", \"\", names(res_knn)), \"LR\"),\n                  accuracy = c(res_knn, res_lr))\nres$model &lt;- factor(res$model, levels = res$model)\nrownames(res) &lt;- NULL\n\nggplot(res, aes(x = model, y = accuracy)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue4\") +\n  labs(x = \"Model\", y = \"Test accuracy\",\n       title = \"Test accuracy of the KNN models and logistic regression\",\n       subtitle = \"Test accuracy via 5-fold cross validation\",\n       caption = \"Source: click data\") +\n  guides(fill = \"none\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-vs.-logistic-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-vs.-logistic-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "KNN classification vs. logistic regression",
    "text": "KNN classification vs. logistic regression\n\nIt appears that the logistic regression model (narrowly) outperforms all KNN models in terms of accuracy.\nTo get an even more detailed picture of the relative performances, one would look at precision, recall and F1 score of all presented classification approaches.\nWith its non-parametric flexibility, KNN classification can in principle model any shape of decision boundary. However, in the specific case of our click data set, it seems that the true decision boundary is close to linear, so the additional flexibility does not provide any benefit.\nTo illustrate this, consider the estimated decision boundaries for some of our KNN models on the next slide."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#decision-boundaries-in-knn-classification",
    "href": "slides/05_The_Data_Science_Workflow_III.html#decision-boundaries-in-knn-classification",
    "title": "Data Science and Data Analytics",
    "section": "Decision boundaries in KNN classification",
    "text": "Decision boundaries in KNN classification"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#tree-based-methods-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#tree-based-methods-1",
    "title": "Data Science and Data Analytics",
    "section": "Tree-based methods",
    "text": "Tree-based methods\n\nSo far, we know about the following regression methods:\n\nSimple and multiple linear regression\nLinear regression with forward and backward variable selection\nKNN regression\n\n… and the following classification methods:\n\nSimple and multiple logistic regression\nLogistic regression with forward and backward variable selection\nKNN classification\n\nNow, we will look into another class of machine learning methods called tree-based methods. In particular, we will discuss so-called decision trees that can be used for both regression and classification."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-regression-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-regression-trees",
    "title": "Data Science and Data Analytics",
    "section": "Introduction to regression trees",
    "text": "Introduction to regression trees\n\nWhen decision trees are used for regression tasks, they are referred to as regression trees.\nIn order to motivate regression trees, we begin with a simple example:\n\nThe Hitters data set in the ISLR package contains statistics of 322 baseball players in the American MLB from the 1986 and 1987 seasons.\nOur goal is to predict a player’s Salary based on Years (number of years in the MLB) and Hits (number of hits he made in the previous year)\nFirst, we remove observations that are missing Salary values and log transform the remaining salaries.\n\n\nHitters &lt;- Hitters[!is.na(Hitters$Salary), ]\nHitters$logSalary &lt;- log(Hitters$Salary)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#baseball-salary-data",
    "href": "slides/05_The_Data_Science_Workflow_III.html#baseball-salary-data",
    "title": "Data Science and Data Analytics",
    "section": "Baseball salary data",
    "text": "Baseball salary data\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(Hitters, aes(x = Years, y = Hits, color = logSalary)) +\n  geom_point() +\n  scale_color_gradientn(colors = rainbow(10)) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#a-simple-regression-tree-for-baseball-data",
    "href": "slides/05_The_Data_Science_Workflow_III.html#a-simple-regression-tree-for-baseball-data",
    "title": "Data Science and Data Analytics",
    "section": "A simple regression tree for baseball data",
    "text": "A simple regression tree for baseball data\n\nUsing this data set, let’s say, we wanted to find players who, based on their Hits and Years in the MLB, had similar salaries:\n\nWe could, for instance, group together all players with less than five years of experience.\nAmong those with more than five years of experience, we could again distinguish between those with less than 118 hits and those with more.\n\nWith these decision rules, we would thus have three groups in total:\n\nPlayers with Years &lt; 5\nPlayers with Years &gt;= 5 and Hits &lt; 118\nPlayers with Years &gt;= 5 and Hits &gt;= 118\n\nLet’s draw these three regions into the plot from earlier!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#a-simple-regression-tree-for-baseball-data-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#a-simple-regression-tree-for-baseball-data-1",
    "title": "Data Science and Data Analytics",
    "section": "A simple regression tree for baseball data",
    "text": "A simple regression tree for baseball data\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(Hitters) +\n  geom_point(aes(x = Years, y = Hits, color = logSalary)) +\n  geom_vline(xintercept = 5, linewidth = 1) +\n  geom_line(data = data.frame(x = c(5, 25), y = c(118, 118)),\n            mapping = aes(x = x, y = y),\n            linewidth = 1) +\n  scale_color_gradientn(colors = rainbow(10)) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-tree-illustration",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-tree-illustration",
    "title": "Data Science and Data Analytics",
    "section": "Regression tree illustration",
    "text": "Regression tree illustration\nWe have already built our first regression tree! The reason for the name is that we can illustrate our decisions on the groups in a tree structure:\n\n\nThe final three regions are known as terminal nodes or leaves of the tree.\nThe points along the tree where the predictor space is split are referred to as internal nodes."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#prediction-with-regression-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#prediction-with-regression-trees",
    "title": "Data Science and Data Analytics",
    "section": "Prediction with regression trees",
    "text": "Prediction with regression trees\n\nBut how do we use a regression tree for prediction?\nVery simple: our prediction for the log salary of a new player is the average log salary of all the players in his region!\nAs the plot shows, our regression tree can thus only predict 3 different values:\n\nA log salary of 5.107 for players with Years &lt; 5\nA log salary of 5.998 for players with Years &gt;= 5 and Hits &lt; 118\nA log salary of 6.740 for players with Years &gt;= 5 and Hits &gt;= 118\n\nInterpretation:\n\nFor a player with less than 5 years of experience, the number of hits that he made in the previous year seems to play little role in his salary.\nBut among more experienced players, the number of hits made in the previous year does (positively) affect salary."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-the-general-idea",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-the-general-idea",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – The general idea",
    "text": "Regression trees – The general idea\n\nWhile this is likely an oversimplification of the true relationship between experience, hits and salary, the regression tree has the advantage of being easy to interpret and to visualize.\nWe have now seen a basic example of a regression tree. Now, we will discuss how to build them. Roughly speaking, there are two steps:\n\nWe divide the predictor space, i.e. the set of possible values for \\(X_1, X_2, \\ldots, X_p\\), into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). In the previous example, we had \\(J = 3\\).\nFor every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\).\n\nAs we saw in the example, once we know the regions, step 2 is easy. But how do we get these regions \\(R_1, R_2, \\ldots, R_J\\)?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-constructing-regions",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-constructing-regions",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Constructing regions",
    "text": "Regression trees – Constructing regions\n\nIn theory, the regions could have any shape. However, in decision trees, we typically divide the predictor space into high-dimensional rectangles or boxes.\nThe goal is to find boxes \\(R_1, \\ldots, R_J\\) that minimize the RSS given by \\[\\sum_{j=1}^J \\sum_{i: x_i \\in R_j} (y_i - \\hat{y}_{R_j})^2,\\] where \\(\\hat{y}_{R_j}\\) is the mean response for the training observations within the \\(j\\)th box. Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes.\nInstead, an algorithm known as recursive binary splitting is used for the purpose of achieving this goal."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Recursive binary splitting",
    "text": "Regression trees – Recursive binary splitting\n\nRecursive binary splitting is based on the idea of, at every step, using a single predictor \\(X_j\\) at a node as a splitting variable.\nThis algorithm roughly works as follows:\n\nFirst, select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X|X_j &lt; s\\}\\) and \\(\\{X|X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS.\nNext, we repeat the process, choosing predictor and cutpoint so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.\nContinue splitting optimally in this way until some stopping criterion is reached, e.g. until no region contains more than five observations."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-1",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Recursive binary splitting",
    "text": "Regression trees – Recursive binary splitting\nIn the baseball example, we first consider all possible splits on Years:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-2",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Recursive binary splitting",
    "text": "Regression trees – Recursive binary splitting\nThen, we run the same procedure for Hits to check if we get any lower RSS:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-3",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Recursive binary splitting",
    "text": "Regression trees – Recursive binary splitting\n\nBased on these results, we can determine the first split:\n\nFor the variable Years, the lowest RSS was achieved for \\(s = 5\\). The lowest achievable RSS was 115.06.\nFor the variable Hits, the lowest RSS was achieved for \\(s = 118\\). The lowest achievable RSS was 160.97.\nTherefore, the first split will be based on Years for \\(s = 5\\). We split the predictor space into two rectangles: one where Years &lt; 5 and one where Years &gt;= 5, both times irrespective of the number of Hits.\n\nNext, we again check which split (across either of those two rectangles) gives the lowest RSS. Remember that we only split one rectangle at a time."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split",
    "href": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split",
    "title": "Data Science and Data Analytics",
    "section": "Recursive binary splitting – Second split",
    "text": "Recursive binary splitting – Second split"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-1",
    "title": "Data Science and Data Analytics",
    "section": "Recursive binary splitting – Second split",
    "text": "Recursive binary splitting – Second split"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-2",
    "title": "Data Science and Data Analytics",
    "section": "Recursive binary splitting – Second split",
    "text": "Recursive binary splitting – Second split"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-3",
    "title": "Data Science and Data Analytics",
    "section": "Recursive binary splitting – Second split",
    "text": "Recursive binary splitting – Second split\n\nThe next best split is splitting the rectangle where Years &gt;= 5 into two smaller rectangles: one where Hits &lt; 118 and one where Hits &gt;= 118. This gives \\(RSS = 91.33\\).\nStopping after these two steps gave us the regression tree we saw in the introduction. However, we could continue until some stopping criterion is reached, e.g. no region contains more than five observations.\nRecursive binary splitting is known as a top-down, greedy algorithm:\n\nIt is top-down because it begins at the top of the tree (all observations in a single region) and then successively splits the predictor space, each split indicated as two new branches further down on the tree.\nIt is greedy because at each step, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#prediction-surface-in-regression-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#prediction-surface-in-regression-trees",
    "title": "Data Science and Data Analytics",
    "section": "Prediction surface in regression trees",
    "text": "Prediction surface in regression trees\n\nWith recursive binary splitting, the prediction surface of regression trees looks like a (irregular) stair case. Below is an example with five terminal nodes. Notice the difference to linear regression which would be a plane."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#pruning-a-tree",
    "href": "slides/05_The_Data_Science_Workflow_III.html#pruning-a-tree",
    "title": "Data Science and Data Analytics",
    "section": "Pruning a tree",
    "text": "Pruning a tree\n\nNow, performing recursive binary splitting, until only a few observations are left in each region is likely to overfit the data:\n\nA high number of splits produces a very complex tree, which will have low training error, but (probably) high test error.\nThus, a smaller tree might lead to lower variance and better interpretation at the cost of only a little bias.\n\nOne way to mitigate this is to grow a very large tree \\(T_0\\) and then to prune it back (cut branches of the tree) in order to obtain a subtree.\nThe way we do this in practice is through an approach called cost complexity pruning (aka weakest link pruning)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#cost-complexity-pruning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#cost-complexity-pruning",
    "title": "Data Science and Data Analytics",
    "section": "Cost complexity pruning",
    "text": "Cost complexity pruning\n\nThe idea of cost complexity pruning is to choose a subtree \\(T\\) of \\(T_0\\) that optimally trades off a subtree’s complexity against its fit to the training data.\nFor this purpose, we set a tuning parameter \\(\\alpha &gt; 0\\) and then find \\(T\\) so that \\[\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha \\cdot |T|\\] is minimized. \\(|T|\\) is the size of the tree, i.e. the number of terminal nodes.\nThe tuning parameter \\(\\alpha\\) controls the aforementioned trade-off:\n\nWhen \\(\\alpha = 0\\), then \\(T = T_0\\) because the expression just measures the training error.\nHowever, when \\(\\alpha &gt; 0\\), there is a price to pay for having a tree with many terminal nodes, so the expression will “prefer” a smaller subtree."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#cost-complexity-pruning-choosing-alpha",
    "href": "slides/05_The_Data_Science_Workflow_III.html#cost-complexity-pruning-choosing-alpha",
    "title": "Data Science and Data Analytics",
    "section": "Cost complexity pruning – Choosing \\(\\alpha\\)",
    "text": "Cost complexity pruning – Choosing \\(\\alpha\\)\n\nIn general, the higher the value of \\(\\alpha\\), the more the initial fully grown tree \\(T_0\\) will be pruned. Thus, as we increase \\(\\alpha\\) from 0, the optimal subtree will become smaller and smaller.\nSo how do we choose the optimal \\(\\alpha\\)?\n\nWe perform \\(K\\)-fold cross-validation!\nMore specifically, we assess the predictive performance of the subtrees on each of the \\(K\\) hold-out folds as a function of \\(\\alpha\\) and then pick the \\(\\alpha\\) that minimizes the cross-validated test MSE.\n\nHaving determined the optimal \\(\\alpha\\) in this way, we can then apply it to obtain the optimally pruned tree on the entire training data set.\nFortunately, this process is implemented in a highly optimized R package called rpart that does most of this work for us. Let’s have a look at it!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-fitting-and-visualization",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-fitting-and-visualization",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Fitting and visualization",
    "text": "Regression trees in R – Fitting and visualization\n\nThe main function in the rpart package is also called rpart. Besides fitting a regression tree, it performs the cross-validation analysis for \\(\\alpha\\) automatically in the background.\nTo use it, we need to pass it a model formula and data (like in lm) as well as the minimum \\(\\alpha\\) value, which is referred to as cp (complexity parameter) in the package (0.01 by default).\nUsing all the variables in the baseball data, we can thus fit a “full” tree like this:\n\nlibrary(rpart)\n\nrt_full &lt;- rpart(logSalary ~ . - Salary, data = Hitters,\n                 control = list(cp = 0))\n\nThe output from print(rt) is not very user-friendly so we will visualize the tree using the rpart.plot package (result on the next slide):\n\nlibrary(rpart.plot)\nrpart.plot(rt_full)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-visualization",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-visualization",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Visualization",
    "text": "Regression trees in R – Visualization"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nAs expected, this is quite a large tree! So how do we go about optimally pruning it?\nAs indicated before, rpart has done the necessary cross-validation (10-fold by default) exercise during fitting. We can look at the result by calling\n\n\nprintcp(rt_full)\n\n\nRegression tree:\nrpart(formula = logSalary ~ . - Salary, data = Hitters, control = list(cp = 0))\n\nVariables actually used in tree construction:\n [1] AtBat   CAtBat  CHits   CHmRun  CRBI    CRuns   Hits    PutOuts Walks  \n[10] Years  \n\nRoot node error: 207.15/263 = 0.78766\n\nn= 263 \n\n          CP nsplit rel error  xerror     xstd\n1  0.5689379      0   1.00000 1.00594 0.065303\n2  0.0612877      1   0.43106 0.46493 0.052709\n3  0.0577844      2   0.36977 0.44229 0.055377\n4  0.0307862      3   0.31199 0.36508 0.061041\n5  0.0219449      4   0.28120 0.34821 0.059012\n6  0.0130968      5   0.25926 0.35770 0.060186\n7  0.0117008      6   0.24616 0.36255 0.060416\n8  0.0106994      7   0.23446 0.35751 0.059687\n9  0.0082164      8   0.22376 0.35799 0.060102\n10 0.0054925      9   0.21555 0.34999 0.058923\n11 0.0052416     10   0.21005 0.34842 0.059175\n12 0.0048933     14   0.18886 0.35715 0.062130\n13 0.0043587     15   0.18397 0.35914 0.062019\n14 0.0043174     17   0.17525 0.36168 0.062240\n15 0.0032817     18   0.17094 0.36331 0.062404\n16 0.0029170     19   0.16766 0.36551 0.064355\n17 0.0021653     20   0.16474 0.36653 0.064503\n18 0.0019773     21   0.16257 0.36330 0.064039\n19 0.0000000     22   0.16060 0.36789 0.068038"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-1",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nThe main output of that function is a table with five columns:\n\nCP: The complexity parameter, a scaled version of \\(\\alpha\\). In each row, it is the smallest penalty parameter, for which the number of splits is still equal to nsplit, e.g. 0.06129 is the smallest value for which there is only one split.\nnsplit: The number of splits induced by the value of CP.\nrel error: Scaled training RSS. More precisely, it is \\(RSS/TSS\\) in-sample. Will always decrease as the number of splits (i.e. the size of the tree) increases.\nxerror: Scaled test RSS. More precisely, it is \\(RSS/TSS\\) out-of-sample (i.e. averaged across the hold-out folds). Primary metric for assessing optimal number of splits.\nxstd: Standard deviation of xerror over the different folds."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-2",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nWith this table, we can now plot the scaled RSS (in train and test sample) on the y-axis against the number of splits on the x-axis and then choose the number of splits optimally.\nIn choosing the number of splits, we want to balance to conflicting goals:\n\nChoose as large a tree as necessary to deliver a good fit.\nAmong those with similar goodness-of-fit, choose the smallest tree possible.\n\nA good, yet inevitably somewhat subjective heuristic is therefore: Choose the number of splits where the scaled test RSS starts to flatten out.\nLet’s create the plot to see where that point would be in our example (Note: we do not have to use ggplot2 to create this plot, a quick way to do it is to simply run the function rsq.rpart on our tree)!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-3",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_error &lt;- data.frame(nsplit = rep(rt_full$cptable[, 2], 2),\n                       rss_scaled = as.vector(rt_full$cptable[, 3:4]),\n                       type = rep(c(\"train\", \"test\"), each = nrow(rt_full$cptable)))\n\nggplot(df_error, aes(x = nsplit, y = rss_scaled, group = type, color = type)) +\n  geom_vline(xintercept = 4, linetype = 2, linewidth = 0.25) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Number of splits\", y = \"Scaled RSS (RSS/TSS)\", color = \"Sample\",\n       title = \"Using cost complexity pruning to choose the optimal number of splits\",\n       subtitle = \"Scaled train RSS and scaled test RSS (from 10-fold cross-validation) as a function of the number of splits\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-4",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-4",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nSeems like a good value for the number of splits in the baseball example is 4. Increasing the number of splits to any point beyond that does not deliver any (meaningful) reduction in the scaled test RSS.\nNow, looking back to the table, we have to find the complexity parameter cp corresponding to nsplit = 4:\n\nrt_full$cptable[rt_full$cptable[,\"nsplit\"] == 4, ]\n\n        CP     nsplit  rel error     xerror       xstd \n0.02194488 4.00000000 0.28120373 0.34820532 0.05901179 \n\n\nIt is 0.02194488. To finally optimally prune the tree, we now have to apply the prune function to our full tree rt_full together with the optimal cp parameter:\n\nrt_pruned &lt;- prune(rt_full, cp = 0.02194488)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-optimally-pruned-tree",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-optimally-pruned-tree",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Optimally pruned tree",
    "text": "Regression trees in R – Optimally pruned tree\n\n# Plotting pruned tree with some extra information:\nrpart.plot(rt_pruned, extra = 101, digits = 4)\n\n\nThis is now a much more parsimonious model!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees",
    "text": "Classification trees\n\nWhen decision trees are used for classification tasks, they are referred to as classification trees.\nAlmost all of the logic regarding recursive binary splitting, tree growing and pruning carries over from regression trees, but there are two crucial differences:\n\nIn regression trees, the predicted response is the mean response of the training observations belonging to the same terminal node. In classification trees, we use the most commonly occurring class.\nIn regression trees, we use RSS as a criterion for making the binary splits. In classification trees, we use the so-called Gini index instead.\n\nLet’s grow a tree on the click data set from earlier (with predictors area_income and age) to dive deeper into these differences."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Example",
    "text": "Classification trees – Example"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example-1",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Example",
    "text": "Classification trees – Example\n\nThe grown classification tree has five internal nodes and six terminal nodes (leaves). From left to right, we label the six resulting rectangular regions by \\(R_1, R_2, R_3, R_4, R_5\\) and \\(R_6\\).\nThe predicted class in each leaf is the one having the majority among the observations in there: green for click and blue for no click with the darkness indicating the purity of the node.\nEach node displays the following pieces of information:\n\nThe majority class at the top (0 or 1 for no click or click).\nThe number of observations falling into each of these classes in that node.\nThe percentage of all observations in the node.\n\nThe rectangular regions and resulting decision boundary can be highlighted in the two-dimensional scatter plot from earlier."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example-2",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Example",
    "text": "Classification trees – Example"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-gini-index",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-gini-index",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Gini index",
    "text": "Classification trees – Gini index\n\nClassification trees are still built with recursive binary splitting, only the criterion changes as RSS cannot be used for classification.\nInstead, at each step, we choose the split that minimizes the Gini index \\[G = \\sum_{k=1}^K \\hat{p}_{mk} (1 - \\hat{p}_{mk}),\\] where \\(K\\) is the number of classes and \\(\\hat{p}_{mk}\\) is the proportion of training observations in the mth region that are from the kth class.\nThe Gini index is a measure of node purity:\n\nIt is low when most observations belong to the same class.\nIt is high when they are evenly distributed among classes."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Recursive binary splitting",
    "text": "Classification trees – Recursive binary splitting\nLet’s consider all possible splits on the variable area_income:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting-1",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Recursive binary splitting",
    "text": "Classification trees – Recursive binary splitting\nThen, we run the same procedure for age to check if we get any lower Gini:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting-2",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Recursive binary splitting",
    "text": "Classification trees – Recursive binary splitting\n\nBased on these results, we can determine the first split:\n\nFor the variable area_income, the lowest Gini was achieved for \\(s = 50337.93\\). The lowest achievable Gini index was 0.3983.\nFor the variable age, the lowest Gini was achieved for \\(s = 38\\). The lowest achievable Gini index was 0.3947.\nTherefore, the first split will be based on age for \\(s = 38\\). We split the predictor space into two rectangles: one where age &lt; 38 and one where age &gt;= 38, both times irrespective of the area_income.\n\nThis of course again corresponds to the result we already saw. In recursive binary splitting, we would now go on splitting like this until some stopping criterion is reached."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-fitting",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-fitting",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Fitting",
    "text": "Classification trees in R – Fitting\n\nIn R, not much changes when we want to fit a classification tree compared to a regression tree. As long as the response is converted to a factor, the rpart function will automatically fit a classification tree.\nSo, let’s make sure the response in the click data set is a factor:\n\nclick$clicked_on_ad &lt;- factor(click$clicked_on_ad)\n\nNow, we can grow a “full” tree again in the same way as before in the click data. We want to use predictors age, area_income, male and daily_time_spent_on_site:\n\nct_full &lt;- rpart(clicked_on_ad ~ age + area_income + male + daily_time_spent_on_site,\n                 data = click, control = list(cp = 0))\n\nWe can again visualize the resulting classification tree using rpart.plot (result on the next slide):\n\nrpart.plot(ct_full)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-visualization",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-visualization",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Visualization",
    "text": "Classification trees in R – Visualization"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-pruning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-pruning",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Pruning",
    "text": "Classification trees in R – Pruning\nAll functionality for pruning also carries over from regression trees. The metric used in the cptable is classification error rate:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_error &lt;- data.frame(nsplit = rep(ct_full$cptable[, 2], 2),\n                       error_rate = as.vector(ct_full$cptable[, 3:4]),\n                       type = rep(c(\"train\", \"test\"), each = nrow(ct_full$cptable)))\n\nggplot(df_error, aes(x = nsplit, y = error_rate, group = type, color = type)) +\n  geom_vline(xintercept = 2, linetype = 2, linewidth = 0.25) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Number of splits\", y = \"Error rate\", color = \"Sample\",\n       title = \"Using cost complexity pruning to choose the optimal number of splits\",\n       subtitle = \"Classification error rate for train and test data (from 10-fold cross-validation) as a function of the number of splits\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-pruning-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-pruning-1",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Pruning",
    "text": "Classification trees in R – Pruning\n\nSeems like a good value for the number of splits in the click example is 2. Increasing the number of splits to any point beyond that does not deliver any (meaningful) reduction in error rate.\nNow, looking at the cptable, we find the corresponding value of cp\n\nct_full$cptable[ct_full$cptable[,\"nsplit\"] == 2, ]\n\n       CP    nsplit rel error    xerror      xstd \n0.0130000 2.0000000 0.1960000 0.2320000 0.0202528 \n\n\nIt is 0.013. To finally optimally prune the tree, we again simply apply the prune function to the full tree ct_full together with the optimal cp parameter:\n\nct_pruned &lt;- prune(ct_full, cp = 0.013)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-optimal-pruning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-optimal-pruning",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Optimal pruning",
    "text": "Classification trees in R – Optimal pruning\n\n# Plotting pruned tree with some extra information:\nrpart.plot(ct_pruned, extra = 101)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#final-assessment-of-decision-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#final-assessment-of-decision-trees",
    "title": "Data Science and Data Analytics",
    "section": "Final assessment of decision trees",
    "text": "Final assessment of decision trees\n\nWhether used for regression or classification, decision trees have many advantages:\n\nVery easy to explain intuitively. Even easier than linear or logistic regression.\nCan be nicely visualized and easily interpreted as long as they are only moderately large, even by non-experts.\nCan handle qualitative predictors without the need for dummy variables.\n\nHowever, there are some downsides of course:\n\nDue to their simplistic nature, decision trees typically do not have the same level of predictive accuracy as other methods.\nTrees struggle with linearity, e.g. linear decision boundaries (see next slide)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#final-assessment-of-decision-trees-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#final-assessment-of-decision-trees-1",
    "title": "Data Science and Data Analytics",
    "section": "Final assessment of decision trees",
    "text": "Final assessment of decision trees"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-1",
    "title": "Data Science and Data Analytics",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nSo far, this course has focused on supervised learning methods, such as regression and classification.\nIn that setting, we observe both a set of features \\(X_1, \\ldots, X_p\\) for each observation and a response variable \\(Y\\). The goal then is to predict \\(Y\\) using \\(X_1, \\ldots, X_p\\).\nNow, we will instead focus on unsupervised learning, where we only have set a of features \\(X_1, \\ldots, X_p\\) measured on \\(n\\) observations.\nWe are not interested in prediction because we do not have an associated response variable \\(Y\\). Rather, the goal is to discover interesting things about the measurements on \\(X_1, \\ldots, X_p\\).\nWe will focus on one particular such interesting thing, namely: can we find subgroups / clusters in the data?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-clustering-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-clustering-2",
    "title": "Data Science and Data Analytics",
    "section": "Unsupervised learning – Clustering",
    "text": "Unsupervised learning – Clustering\n\nClustering refers to a broad set of techniques for finding subgroups or clusters in a data set. Using clustering, we ideally group the observations such that…\n\nobservations within each group are quite similar to each other and\nobservations in different groups are quite different from each other.\n\nTo make this concrete, we must define what it means for two or more observations to be “similar” or “different”. This is a domain-specific consideration that depends on the data being studied.\nConsider the following application of clustering for market segmentation:\n\nSay we have access to a large number of measurements of (potential) customers, e.g. income, occupation, past purchase behaviour, etc.\nWe can perform clustering to identify subgroups of people who may be more receptive to a particular form of advertising."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#what-is-a-cluster",
    "href": "slides/05_The_Data_Science_Workflow_III.html#what-is-a-cluster",
    "title": "Data Science and Data Analytics",
    "section": "What is a cluster?",
    "text": "What is a cluster?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#what-is-a-cluster-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#what-is-a-cluster-1",
    "title": "Data Science and Data Analytics",
    "section": "What is a cluster?",
    "text": "What is a cluster?\nClearly, the notion of a cluster can be ambiguous. There is subjective judgement required in deciding what constitutes a cluster:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#clustering-methods",
    "href": "slides/05_The_Data_Science_Workflow_III.html#clustering-methods",
    "title": "Data Science and Data Analytics",
    "section": "Clustering methods",
    "text": "Clustering methods\n\nSince clustering is popular in many fields, there exist a great number of clustering methods. Some of the most popular ones are:\n\nK-means clustering\nHierarchical clustering\nDBSCAN\nModel-based clustering\n…\n\nAs with supervised learning, there is no free lunch: clustering methods perform differently on different data sets.\nAs an introduction to clustering, we will only be looking more deeply into the first of these methods: K-means clustering."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering",
    "text": "K-means clustering\n\nGiven a number of clusters \\(K\\), K-means clustering divides the observations into \\(K\\) distinct, non-overlapping clusters, i.e. each data point is in exactly one subset.\nThe idea is pretty simple:\n\nEach cluster is associated with a so-called centroid. It is essentially a cluster’s midpoint, calculated as the vector of feature means for all the observations in that cluster.\nEach data point is assigned to the cluster with the closest centroid, where “close” is usually defined based on Euclidean distance, i.e. \\[d(x, y) = \\sqrt{(x_1 - y_1)^2 + \\ldots + (x_p - y_p)^2}\\]\n\nThe algorithm is best understood when illustrating it with an example."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nIn the following data set, we have \\(p = 2\\) features named X1 and X2. Based on visual inspection, choosing \\(K = 3\\) for clustering seems appropriate."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-1",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nStep 1: generate \\(K\\) random centroids in the range of the data. We plot the three generated random initial cluster centroids as stars into the plot below:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-2",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nStep 2: Assign each observation to the centroid that it is closest to. Below, we colour each point in accordance with the colour of its assigned centroid:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-3",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nStep 3: Now, re-compute the centroid of each cluster as the mean (in X1 and X2, respectively) of all observations in a given cluster:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-4",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-4",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nStep 4: Finally, iterate the assignment of observations to the cluster with the closest centroid and the re-computation of centroids until the clusters are stable:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-5",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-5",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#notes-on-k-means-clustering",
    "href": "slides/05_The_Data_Science_Workflow_III.html#notes-on-k-means-clustering",
    "title": "Data Science and Data Analytics",
    "section": "Notes on K-means clustering",
    "text": "Notes on K-means clustering\n\nDue to the randomness of the initial cluster centroids, K-means clustering is non-deterministic, i.e. the final clustering outcome can be different every time the algorithm is run:\n\nTo mitigate this, we typically run the algorithm on multiple (random) starting values and see which run gives the best result.\nThe clusters labels (1, 2, 3) are also random, so they can be different for different runs even when the same observations are clustered together.\n\nIn general, K-means clustering…\n\nis a simple, easy-to-understand algorithm.\nworks well also in higher dimensions.\ntends to build spherical clusters of equal size."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#notes-on-k-means-clustering-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#notes-on-k-means-clustering-1",
    "title": "Data Science and Data Analytics",
    "section": "Notes on K-means clustering",
    "text": "Notes on K-means clustering\nWith these properties, K-means clustering can be expected to perform well on some data sets, whilst performing badly on others:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R\n\nLet’s see how to run this algorithm on an actual real-world data set in R!\nWe use the mall_customers data set from Kaggle. This data set contains information on 200 customers of a large mall.\nBesides information on the age, sex and annual income (in thousand USD) of these customers, the data also contains a spending score variable based on customer behaviour and purchasing data:\n\nmall &lt;- read.csv(\"../data/mall_customers.csv\")\nnames(mall) &lt;- c(\"customer_id\", \"sex\", \"age\", \"annual_income\", \"spending_score\")\nhead(mall)\n\n  customer_id    sex age annual_income spending_score\n1           1   Male  19            15             39\n2           2   Male  21            15             81\n3           3 Female  20            16              6\n4           4 Female  23            16             77\n5           5 Female  31            17             40\n6           6 Female  22            17             76\n\n\nUsing K-means clustering, we want to perform market segmentation based on the variables annual_income and spending_score."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R\nLet’s start by plotting these two variables in a scatter plot. Based on this plot, a choice of \\(K = 5\\) seems reasonable (more on how to choose \\(K\\) later)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R\n\nImportant: before we apply K-means clustering, we should always scale / standardize our variables!\nThis is because – as with KNN regression or classification – K-means clustering revolves around computing distances between observations, which is highly driven by the scale on which the features are measured.\nTo scale our observations, we can either manually subtract the mean and divide by the standard deviation, or use the R function scale:\n\nmall_scaled &lt;- scale(mall[, c(\"annual_income\", \"spending_score\")])\n\nNow, to apply K-means clustering in R, we use the function kmeans. Besides the (scaled) data, it requires the number of clusters \\(K\\) (argument centers) and the number of random starting points (argument nstart):\n\nkm_mall &lt;- kmeans(mall_scaled, centers = 5, nstart = 10)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-3",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R\n\nAmong other things, the resulting kmeans object holds the coordinates of the centroids (centers) and the clustering vector (cluster), which gives the cluster to which each of the observations was assigned.\nLet’s start by looking at the sizes of the clusters:\n\ntable(km_mall$cluster)\n\n\n 1  2  3  4  5 \n22 39 81 35 23 \n\n\nNext, we want to highlight the detected clusters in the original scatter plot using colour (result on the next slide):\n\nmall$Cluster &lt;- factor(km_mall$cluster)\n\nggplot(mall, aes(x = annual_income, y = spending_score, color = Cluster)) +\n  geom_point() +\n  labs(title = \"Annual income (k$) and spending score (1-100) of mall customers\",\n       x = \"Annual income (k$)\", y = \"Spending score (1-100)\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-4",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-4",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k",
    "href": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k",
    "title": "Data Science and Data Analytics",
    "section": "Choosing the number of clusters \\(K\\)",
    "text": "Choosing the number of clusters \\(K\\)\n\nSo far, we have determined a suitable number of clusters \\(K\\) by inspecting the data. While this may work when there are 2 or 3 features, it certainly no longer works in higher dimensions.\nTherefore, we need some other metric to assess clustering quality. One commonly used metric is the within-cluster sum of squares (WCSS): \\[WCSS = \\sum_{k=1}^K \\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2\\]\nThe WCSS measures how much the observations within a given cluster differ from each other:\n\nIt is small when the observations a very tightly packed around the centroid.\nIt is large when they are widely spread around the centroid."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k-1",
    "title": "Data Science and Data Analytics",
    "section": "Choosing the number of clusters \\(K\\)",
    "text": "Choosing the number of clusters \\(K\\)\n\nTherefore, we want a clustering with as small a \\(WCSS\\) as possible. In fact, K-means clustering is designed to minimize \\(WCSS\\).\nTypically (although not necessarily), the \\(WCSS\\) decreases as we increase the number of clusters.\nConsequently, one way of choosing \\(K\\) is a heuristic called the elbow method:\n\nRun K-means with different values of \\(K\\) and record the \\(WCSS\\) for each.\nPlot the number of clusters against the \\(WCSS\\) and find the “elbow” in the plot, i.e. the point where the reduction in \\(WCSS\\) for an increase in \\(K\\) becomes very small. Choose \\(K\\) accordingly.\n\nWhile this heuristic is easy to understand, it is not always unambiguous and does require some subjective judgement."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k-2",
    "title": "Data Science and Data Analytics",
    "section": "Choosing the number of clusters \\(K\\)",
    "text": "Choosing the number of clusters \\(K\\)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn_clusters &lt;- 1:10\nwcss &lt;- sapply(n_clusters, function(k) kmeans(mall_scaled, centers = k, nstart = 10)$tot.withinss)\nwcss &lt;- data.frame(k = n_clusters, wcss = wcss)\n\nggplot(wcss, aes(x = k, y = wcss)) + \n  geom_line() + \n  geom_point() +\n  geom_vline(xintercept = 5, linetype = 2) + \n  scale_x_continuous(breaks = n_clusters) + \n  labs(title = \"Choosing K in the mall_customers data\",\n       subtitle = \"The elbow in the WCSS plot is at K = 5\",\n       x = \"K\", y = \"WCSS\") + \n  theme_minimal()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#the-data-science-workflow-visualize",
    "href": "slides/04_The_Data_Science_Workflow_II.html#the-data-science-workflow-visualize",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Visualize",
    "text": "The Data Science workflow – Visualize"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data",
    "href": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data",
    "title": "Data Science and Data Analytics",
    "section": "Why even look at data?",
    "text": "Why even look at data?\n\nGiven the multitude of ways we can describe data numerically (using descriptive statistics), why do we even bother to present data visually?\nThe answer is that descriptive statistics can never tell us the full story about our data. Consider the following artificial data set from Anscombe (1973):\n\n\n\n\n\n\nIt is a collection of four data sets, each with 11 observations for variables x and y. Here is the first 2 rows of each:\n\n\n   data_id  x     y\n1        1 10  8.04\n2        1  8  6.95\n12       2 10  9.14\n13       2  8  8.14\n24       3  8  6.77\n25       3 13 12.74\n36       4  8  7.71\n37       4  8  8.84\n\n\n\nAll these four data sets have: same mean and SD in x and y, and same correlation between them:\n\n\n  data_id mean_x  sd_x mean_y  sd_y   cor\n1       1      9 3.317  7.501 2.032 0.816\n2       2      9 3.317  7.501 2.032 0.816\n3       3      9 3.317  7.500 2.030 0.816\n4       4      9 3.317  7.501 2.031 0.817\n\n\nSo, seems like these data sets should look pretty similar, right?"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data-1",
    "title": "Data Science and Data Analytics",
    "section": "Why even look at data?",
    "text": "Why even look at data?"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data-2",
    "title": "Data Science and Data Analytics",
    "section": "Why even look at data?",
    "text": "Why even look at data?\n\nWhile this is an extreme, manufactured example, it does prove a point: visualizing data offers insight that the study of pure statistics does not.\nBut that does not mean looking at data is all one needs to do:\n\nReal data sets are often complicated and messy, displaying them graphically presents problems of its own.\nThe core problem of visualization is how to map relations between observations to visual representations in the plot, like shape, lines, colour, size, etc.\nHow to do this optimally for each given data set is subject to considerable debate. There is no simple recipe to follow.\n\nFortunately, however, there is software that provides a lot of support in our data visualization endeavours."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#data-visualization-in-r",
    "href": "slides/04_The_Data_Science_Workflow_II.html#data-visualization-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Data visualization in R",
    "text": "Data visualization in R\n\n\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.”\nJohn Tukey (Statistician, 1915 - 2000)\n\n\n\n\nThere are many tools for visualizing data. Of course, we use R 😉\nEven with R, there are many approaches / systems / packages for creating data visualizations. By far the most common one is the package ggplot2.\nThe gg in ggplot2 stands for the grammar of graphics, which is a coherent system for describing and building graphs developed by statistician Leland Wilkinson in his aptly named 2005 book.\nAs we will see in great detail, this system constructs plots in layers that are built on top of one another in an additive fashion."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#the-grammar-of-graphics",
    "href": "slides/04_The_Data_Science_Workflow_II.html#the-grammar-of-graphics",
    "title": "Data Science and Data Analytics",
    "section": "The grammar of graphics",
    "text": "The grammar of graphics\n\nThe grammar is a set of rules for producing graphics from data, taking pieces of data and mapping them to geometric objects (like points and lines) that have aesthetic attributes (like position, colour and size), together with further rules for transforming the data if needed, adjusting scales, adapting coordinate system and themes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: a grammar limits the structure of what you can say, but it does not automatically make what you say meaningful, i.e. your code just being “grammatically” correct does not make the resulting plot sensible."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins",
    "href": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins",
    "title": "Data Science and Data Analytics",
    "section": "An introductory example – Palmer penguins",
    "text": "An introductory example – Palmer penguins\nLet’s see an example of how this works with ggplot2 in practice.\n\n\nThe data set we will use for our first ggplot is the palmerpenguins data set that can be obtained from CRAN in a package that bears exactly that name.\nIt contains size measurements, clutch observations, and blood isotope ratios for three penguin species observed on three islands in the Palmer Archipelago, Antarctica over a study period of three years."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins-1",
    "title": "Data Science and Data Analytics",
    "section": "An introductory example – Palmer penguins",
    "text": "An introductory example – Palmer penguins\nMake sure the required packages palmerpenguins and ggplot2 are installed. Then we can load them and start by inspecting the data set:\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\npenguins &lt;- as.data.frame(penguins)\nhead(penguins)\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n6  Adelie Torgersen           39.3          20.6               190        3650\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n6   male 2007\n\n\n\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins-2",
    "title": "Data Science and Data Analytics",
    "section": "An introductory example – Palmer penguins",
    "text": "An introductory example – Palmer penguins\nAs zoologists, we might be interested in the following questions about the different types of Palmer penguins:\n\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers?\nWhat does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Non-linear?\nDoes the relationship vary by the species of the penguin? How about by the island where the penguin lives?\n\nLet’s answer all of these questions using a single visualization."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-ultimate-goal",
    "href": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-ultimate-goal",
    "title": "Data Science and Data Analytics",
    "section": "An introductory example – Ultimate goal",
    "text": "An introductory example – Ultimate goal\nUltimately, we want to create the following plot:"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#first-step-telling-ggplot-about-our-data-set",
    "href": "slides/04_The_Data_Science_Workflow_II.html#first-step-telling-ggplot-about-our-data-set",
    "title": "Data Science and Data Analytics",
    "section": "First step – Telling ggplot about our data set",
    "text": "First step – Telling ggplot about our data set\nLet’s recreate this plot step-by-step.\n\nWith ggplot2, we begin a plot with the function ggplot. Its first argument is the dataset to use in the graph.\n\n\n\nggplot(data = penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor now, we have not told ggplot how to visualize the data, so we only have an empty canvas. Now, we will “paint” onto this canvas in layers."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#second-step-specifying-aesthetic-mappings",
    "href": "slides/04_The_Data_Science_Workflow_II.html#second-step-specifying-aesthetic-mappings",
    "title": "Data Science and Data Analytics",
    "section": "Second step – Specifying aesthetic mappings",
    "text": "Second step – Specifying aesthetic mappings\n\nThe second argument of ggplot is called mapping. It defines how variables in our data set are mapped to visual properties (aesthetics) of our plot. This argument is always defined in the aes function, and the x and y arguments of aes specify which variables to map to the x- and y-axes. We want flipper length on the x and body mass on the y axis.\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, our empty canvas has more structure: x- and y-axis have a range, ticks and labels. But the penguins themselves are not yet on the plot."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#third-step-additively-layer-on-geoms",
    "href": "slides/04_The_Data_Science_Workflow_II.html#third-step-additively-layer-on-geoms",
    "title": "Data Science and Data Analytics",
    "section": "Third step – Additively layer on geoms",
    "text": "Third step – Additively layer on geoms\n\nThis is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot. To do so, we need a geom, a geometrical object used for data representation. In the example, we want our data represented by points, so we use geom_point:\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we start to have an actual scatter plot. Note that R warns us about missing data points. We will suppress this warning in the plots to come."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-note-about-geoms",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-note-about-geoms",
    "title": "Data Science and Data Analytics",
    "section": "A note about geoms",
    "text": "A note about geoms\nIn ggplot2, geometric objects are made available with functions that start with geom_. People often describe plots by the type of geom that the plot uses, for example:\n\nScatter plots use point geoms (geom_point) as we just saw.\nBar charts use bar geoms (geom_bar)\nLine charts use line geoms (geom_line)\nBoxplots use boxplot geoms (geom_boxplot)\n…\n\nWe layer a geom onto a ggplot literally in an additive way, i.e. by combining the two with a + sign. We will learn how to deal with many different geoms in this way."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers",
    "title": "Data Science and Data Analytics",
    "section": "Fourth step – Adding aesthetics and layers",
    "text": "Fourth step – Adding aesthetics and layers\n\nDoes the relationship between flipper length and body mass differ by species? To answer this, let’s represent species with different coloured points. To achieve this, we modify the aesthetic to additionally map species to colour:\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn mapping a categorical variable to the colour aesthetic, ggplot2 automatically assigns a unique colour to each factor level (i.e. each species), a process known as scaling. ggplot2 will also add a legend that explains which values correspond to which levels."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-1",
    "title": "Data Science and Data Analytics",
    "section": "Fourth step – Adding aesthetics and layers",
    "text": "Fourth step – Adding aesthetics and layers\n\nNow, let’s add one more layer, a smooth curve displaying the relationship between body mass and flipper length. Since this is a new geometric object representing our data, we will add a new geom, namely geom_smooth based on a linear model (lm):\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have successfully added lines, but this plot does not look like our ultimate goal. Instead of having only one line for the entire data set, we have separate lines for each of the three penguin species. What went wrong?"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-2",
    "title": "Data Science and Data Analytics",
    "section": "Fourth step – Adding aesthetics and layers",
    "text": "Fourth step – Adding aesthetics and layers\n\nWhen aesthetics are defined in ggplot(), at the top level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function can also take a mapping argument, which allows for aesthetics at the local level. Here, we want point colours, but not lines separated by species, so we should specify color = species locally for geom_point only:\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are getting close to our ultimate goal… Some minor details are still missing though."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-3",
    "title": "Data Science and Data Analytics",
    "section": "Fourth step – Adding aesthetics and layers",
    "text": "Fourth step – Adding aesthetics and layers\n\nDue to differences in colour perception (e.g. colour blindness), it is generally not a good idea to represent information using only colours on a plot. Therefore, in addition to colour, we can also map species to the shape aesthetic:\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point(mapping = aes(color = species,\n                           shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the legend is automatically updated to reflect the different shapes of the points as well."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fifth-step-adjust-scales-labels-titles",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fifth-step-adjust-scales-labels-titles",
    "title": "Data Science and Data Analytics",
    "section": "Fifth step – Adjust scales, labels, titles, …",
    "text": "Fifth step – Adjust scales, labels, titles, …\n\nThe “clean-up” work involves setting appropriate labels for title, subtitle, axes, and legend using the labs function and using the colourblind safe colour palette scale_color_colorblind from the ggthemes package (an extension of ggplot2):\n\n\n\n\nlibrary(ggthemes)\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point(mapping = aes(color = species,\n                           shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       title = \"Body mass and flipper length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       color = \"Species\",\n       shape = \"Species\") +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we are done, we have built the plot of our ultimate goal step-by-step from the ground up. The example highlights the core principles of how ggplot2 allows us to build nice-looking informative graphs with relatively little coding effort."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#steps-in-creating-a-plot-with-ggplot2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#steps-in-creating-a-plot-with-ggplot2",
    "title": "Data Science and Data Analytics",
    "section": "Steps in creating a plot with ggplot2",
    "text": "Steps in creating a plot with ggplot2\nConceptually, the steps we went through are the same for every plot created with ggplot2. Based on a tidy data set, we always proceed as follows:\n\nTelling ggplot about our data set.\nSpecifying aesthetic mappings, i.e. what relationships we want to see and by what we want them represented.\nAdditively layer on geoms as needed.\nAdd and adapt aesthetics and layers until all relationships are displayed.\nAdjust scales, labels, titles, …\n\n\nThese steps are always the same. We will now only learn in greater detail about how to tell ggplot what to do by going through several examples of different data sets, aesthetic mappings, geoms and more. As with anything, the key to improvement is LOTS of trial and error…"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Histogram",
    "text": "Visualizing a numerical variable – Histogram\nLet’s say we want to visualize the distribution of body mass (in grams) across all penguins in the sample using a histogram. For this, we map the variable body_mass_g to x and use geom_histogram:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-1",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Histogram",
    "text": "Visualizing a numerical variable – Histogram\nA histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. ggplot2 by default creates 30 such bins. We can control this by setting bins ourselves:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-2",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Histogram",
    "text": "Visualizing a numerical variable – Histogram\nWe do not like the colour of this histogram. Previously, we set the color aesthetic, but here we do not want to map colour to a variable. Instead we want to set it to a specific colour. This we do in the geom, not in the aes:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(color = \"deepskyblue4\", bins = 15)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-3",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Histogram",
    "text": "Visualizing a numerical variable – Histogram\nHm, this is not what we wanted… ggplot only coloured the border line of the bars. Turns out that if we want to fill the bars in a specific colour, we need to use the fill argument of the geom_histogram function:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"deepskyblue4\", bins = 15)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-density-plot",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-density-plot",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Density plot",
    "text": "Visualizing a numerical variable – Density plot\nAn alternative visualization for distributions of numerical variables is a density plot, which is a smoothed-out version of a histogram. To create one, we simply switch out our geom to geom_density:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-density-plot-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-density-plot-1",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Density plot",
    "text": "Visualizing a numerical variable – Density plot\nThat’s a bit boring, let’s change the colour again (both of the line itself using color and the area below the curve using fill):\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density(color = \"deepskyblue4\", fill = \"deepskyblue4\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a categorical variable – Barplot",
    "text": "Visualizing a categorical variable – Barplot\nA simple bar chart visualizes the (absolute or relative) frequencies of the levels of a categorical variable. While we could use ggplot directly on the data, it is generally preferable to compute frequencies explicitly ourselves, e.g. for species:\n\nCodePlot\n\n\n\nspecies_freq &lt;- as.data.frame(table(penguins$species))\nnames(species_freq)[1] &lt;- \"species\"\n\n# As we have already computed frequencies, we have to tell the bar geom\n# that it should use the values as they are, i.e. use the \"identity\":\nggplot(species_freq, aes(x = species, y = Freq)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue4\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot-1",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a categorical variable – Barplot",
    "text": "Visualizing a categorical variable – Barplot\nIf the variable has a nominal (and not ordinal) scale, then it is usually preferable to order the bars based on their frequency. For this, we have to reorder the factor levels for the species:\n\nCodePlot\n\n\n\nggplot(species_freq, aes(x = reorder(species, Freq, decreasing = TRUE), y = Freq)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue4\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot-2",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a categorical variable – Barplot",
    "text": "Visualizing a categorical variable – Barplot\nOften, we want the bars to be horizontal rather than vertical. To achieve this, we simply flip the axes using coord_flip. Additionally, we add nice axis labels:\n\nCodePlot\n\n\n\nggplot(species_freq, aes(x = reorder(species, Freq, decreasing = TRUE), y = Freq)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue4\") +\n  labs(x = \"Penguin Species\", y = \"Observed frequency\") +\n  coord_flip()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nTo visualize the relationship between a categorical and a numerical variable, we can create parallel boxplots. For example, for the distribution of body weight by species, we could try:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-1",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nAn alternative to boxplots are so-called violin plots. In their simplest form, these are rotated density plots of the numerical variable in each category mirrored to create a symmetric, “violin-like” object:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-2",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nSince density plots can also mask certain aspects of the distribution of the data, it is often advisable to additionally plot the actual data points on top of the violin plots. We can do this by layering a point geom on top:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, color = species)) +\n  geom_violin() +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-3",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nIf we have a lot of points close together, then they overlap and it becomes impossible to tell their distribution. To avoid this, we can slightly perturb the points in the x direction. This is achieved by switching to geom_jitter.\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, color = species)) +\n  geom_violin() +\n  geom_jitter()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-4",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-4",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nAnother alternative to displaying the distribution of body weights in the three penguin species is to combine their densities into one plot in the usual way, i.e. with the numerical variable on the x axis.\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g, fill = species)) +\n  geom_density()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-5",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-5",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nNow, the densities are overlapping. To avoid this, we could of course not map the fill aesthetic. A nice alternative is to set the alpha aesthetic, which controls transparency. 0 is max transparency, 1 is max opaqueness (default).\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g, fill = species)) +\n  geom_density(alpha = 0.5)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#two-categorical-variables",
    "href": "slides/04_The_Data_Science_Workflow_II.html#two-categorical-variables",
    "title": "Data Science and Data Analytics",
    "section": "Two categorical variables",
    "text": "Two categorical variables\nWe can use grouped or stacked bar plots to visualize the relationship between two categorical variables. For example, we might want to see how often the different species occur on the different islands.\n\nCodePlot\n\n\n\nspecies_freq &lt;- as.data.frame(table(penguins$species, penguins$island))\nnames(species_freq)[1:2] &lt;- c(\"species\", \"island\")\n\nggplot(species_freq, aes(x = island, y = Freq, fill = species)) +\n  geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#two-categorical-variables-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#two-categorical-variables-1",
    "title": "Data Science and Data Analytics",
    "section": "Two categorical variables",
    "text": "Two categorical variables\nWe can create variations of this plot by changing the position argument of geom_bar. The default is stack. Most useful for comparing distributions are relative frequency plots achieved with position = \"fill\":\n\nCodePlot\n\n\n\nggplot(species_freq, aes(x = island, y = Freq, fill = species)) +\n  geom_bar(stat = \"identity\", position = \"fill\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#two-numerical-variables",
    "href": "slides/04_The_Data_Science_Workflow_II.html#two-numerical-variables",
    "title": "Data Science and Data Analytics",
    "section": "Two numerical variables",
    "text": "Two numerical variables\nIn the introductory example, we have already seen scatter plots and smooth curves as ways to illustrate the relationship between two numerical variables. Scatter plots are an absolute staple in a data scientist’s toolbox:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables",
    "text": "Three or more variables\nThe introductory example displayed three variables: flipper_length_mm (x), body_mass_g (y) and species (color and shape). In general, we have the following options to introduce additional variables into a scatter plot:\n\nUse additional aesthetics:\n\nColour \\(\\to\\) color (categorical or numerical variables)\nShape \\(\\to\\) shape (categorical)\nSize \\(\\to\\) size (categorical or numerical variables)\nTransparency \\(\\to\\) alpha (categorical or numerical variables)\n\nSplit plot into facets, subplots that each display one subset of the data (categorical variables).\nAnimate the plot (categorical or numerical variables, especially time).\nCreate a 3D scatter plot (categorical or numerical variables)."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-1",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables",
    "text": "Three or more variables\n\nWhen using aesthetics to represent additional variables, we must be aware that different sorts of variables can be represented more or less well by different kinds of visual representations.\nWhile there are no one-size-fits-all recipes, consider the following order as a heuristic when introducing additional variables into a 2D scatter plot:\n\nFor continuous variables: size, colour, transparency.\nFor categorical variables: colour, shape.\n\nFor example, in our scatter plot of penguin flipper lengths vs. body masses, we might want to introduce categorical information on species and island. By the heuristic, we use colour for the first and shape for the second."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-colour-and-shape",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-colour-and-shape",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Colour and shape",
    "text": "Three or more variables – Colour and shape\nIn the default setting, the points are quite small, which makes it hard to distinguish the shapes. To mitigate this, we additionally set the size aesthetic to something larger, namely 3.\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-colour-and-size",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-colour-and-size",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Colour and size",
    "text": "Three or more variables – Colour and size\nLet’s say we wanted a scatter plot of penguin bill length vs. bill depth for different species. Additionally, we want information on body mass included as well. A logical choice to represent the latter would be size:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     color = species, size = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-multiple-aesthetics",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-multiple-aesthetics",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Multiple aesthetics",
    "text": "Three or more variables – Multiple aesthetics\nIf we now really wanted to go mad, we could add the island back in via the shape aesthetic and additionally include the flipper length via the alpha aesthetic, so that longer flippers are represented by more opaque points:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     color = species, size = body_mass_g,\n                     shape = island, alpha = flipper_length_mm)) +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Faceting",
    "text": "Three or more variables – Faceting\n\nClearly, there is now too much information in this plot.\nAdding too many aesthetic mappings to a plot makes it cluttered and difficult to make sense of.\nAnother way to introduce additional categorical variables is to split your plot into several facets.\nA facet is a subplot that displays one subset of the data. For example, there could be one facet for each factor level of a categorical variable.\nWe can use facets to explore conditional relationships, like the relationship between flipper length and body weight on each of the three islands."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting-1",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Faceting",
    "text": "Three or more variables – Faceting\nTo facet a plot by a single variable, we use facet_wrap. It takes a so-called formula as its first argument, which we create with ~ followed by the name of a categorical variable.\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species)) +\n  geom_point() +\n  facet_wrap(~island)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting-2",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Faceting",
    "text": "Three or more variables – Faceting\nTo facet a plot by two variables, we use facet_grid. We specify the variable to be faceted in the rows before the ~ and the variable to be faceted in the columns after it. For example, we could additionally facet sex in the rows:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species)) +\n  geom_point() +\n  facet_grid(sex~island)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Labels",
    "text": "Three or more variables – Labels\n\nFor some data sets, we can also display an additional variable in a scatter plot by creating informative labels for the points.\nOur penguin data set is not a good example to illustrate this, so we consider the elections_historic data set from the socviz package instead:\n\nlibrary(socviz)\nelections_historic &lt;- as.data.frame(elections_historic)\nhead(elections_historic[, c(\"year\", \"winner\", \"win_party\", \"ec_pct\",\n                            \"popular_pct\", \"winner_label\")])\n\n  year                 winner win_party ec_pct popular_pct  winner_label\n1 1824      John Quincy Adams     D.-R. 0.3218      0.3092    Adams 1824\n2 1828         Andrew Jackson      Dem. 0.6820      0.5593  Jackson 1828\n3 1832         Andrew Jackson      Dem. 0.7657      0.5474  Jackson 1832\n4 1836       Martin Van Buren      Dem. 0.5782      0.5079    Buren 1836\n5 1840 William Henry Harrison      Whig 0.7959      0.5287 Harrison 1840\n6 1844             James Polk      Dem. 0.6182      0.4954     Polk 1844\n\n\nThis data set contains information on US presidential elections from 1824 to 2016. We will have a look at the winner’s share of the electoral college vote and the popular vote."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-1",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Labels",
    "text": "Three or more variables – Labels\nIn a scatter plot, we want to have the popular vote share on the x axis and the electoral college vote share on the y axis. If we only map these two aesthetics, the plot looks quite uninformative…\n\nCodePlot\n\n\n\nggplot(elections_historic, aes(x = popular_pct, y = ec_pct)) +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-2",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Labels",
    "text": "Three or more variables – Labels\nWe do not know who any of these points represent. This is a perfect use case for labeling the points. For this, we can use the label aesthetic together with the text geom (there is also a label geom, which would work as well):\n\nCodePlot\n\n\n\nggplot(elections_historic, aes(x = popular_pct, y = ec_pct, label = winner_label)) +\n  geom_text() +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-3",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Labels",
    "text": "Three or more variables – Labels\nThat is much better already. However, now, there is a lot of overlap. To remedy this, we need an additional library called ggrepel, which provides us with a text_repel geom to avoid overlapping labels:\n\nCodePlot\n\n\n\nlibrary(ggrepel)\nggplot(elections_historic, aes(x = popular_pct, y = ec_pct, label = winner_label)) +\n  geom_text_repel() +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#customizing-our-plots",
    "href": "slides/04_The_Data_Science_Workflow_II.html#customizing-our-plots",
    "title": "Data Science and Data Analytics",
    "section": "Customizing our plots",
    "text": "Customizing our plots\n\nSo far, we have seen how to create and extend basic plots, like bar plots, density plots, histograms, and scatter plots by tailoring aesthetic mappings and layering on suitable geoms according to our needs.\nHowever, the customization usually does not stop there. We might want to …\n\n… add informative titles, subtitles, captions and axis labels to the plot.\n… use colours and shapes different from the ones ggplot uses by default.\n… adapt the scale of the x and / or y axis.\n… change the font type and size for labels and titles.\n… set the whole plot in a different theme to suit corporate guidelines.\n… and so much more…\n\nIn ggplot2, virtually every aspect of a plot is customizable. We will only have a look at how to customize some of the most important ones."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-guides-and-themes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-guides-and-themes",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales, guides and themes",
    "text": "Understanding scales, guides and themes\n\nOutside of aesthetic mappings and geoms, there are three types of functions that do most of the heavy lifting when it comes to customizing our plot:\n\nThe family of scale_ functions\nThe guides function and\nThe theme function.\n\nDue to the sheer amount of options for customizability and the partial overlap between some of these functions, things can become confusing very quickly when working through these functions.\nLet’s start with a basic delineation of scales, guides and themes."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-guides-and-themes-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-guides-and-themes-1",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales, guides and themes",
    "text": "Understanding scales, guides and themes\n\nA rough guide on the use of these functions is the following:\n\nScales: every aesthetic mapping has a scale. If we want to adjust how that scale is marked or graduated, we use a scale_ function.\nGuides: Many scales come with a legend to help us interpret the graph. These are called guides and can be adjusted with the help of the guides function.\nThemes: graphs have other features not strictly connected to the structure of the data being displayed. These include background colour, font size, legend placement, etc. To adjust these, we use the theme function.\n\nWe will now dig deeper into each of these three domains of customization and see examples of how the corresponding functions are used to change the appearance of the data displayed."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales",
    "text": "Understanding scales\nLet’s come back to an earlier example from the penguin data set:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-1",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales",
    "text": "Understanding scales\nThis plot has four aesthetic mappings:\n\nThe variable flipper_length_mm is mapped to x.\nThe variable body_mass_g is mapped to y.\nThe variable species is mapped to color.\nThe variable island is mapped to shape.\n\n\nAnd each of these mappings has a scale:\n\nflipper_length_mm is a continuous variable, so the x scale is continuous.\nbody_mass_g is a continuous variable, so the y scale is continuous.\nspecies is an unordered categorical variable, so the color scale is discrete.\nisland is an unordered categorical variable, so the shape scale is discrete."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-2",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales",
    "text": "Understanding scales\n\nScales for these mappings may have labels, axis tick marks at particular positions, or specific colours or shapes. If we want to adjust them, we use one of the scale_ functions.\nThese functions have the following general structure:\n\n\n\n\n\nSo, for example:\n\nscale_x_continuous controls x scales for continuous variables\nscale_y_continuous controls y scales for continuous variables\nscale_color_discrete controls color scales for discrete variables\nscale_shape_discrete controls shape scales for discrete variables"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#scales-in-action",
    "href": "slides/04_The_Data_Science_Workflow_II.html#scales-in-action",
    "title": "Data Science and Data Analytics",
    "section": "Scales in action",
    "text": "Scales in action\nLet’s see this in action. Say we wanted to override the default for tick marks by having a mark every 5 mm on the x axis and every 500 g on the y axis. We can use the breaks argument to control the position of the tick marks:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(170, 230, by = 5)) +\n  scale_y_continuous(breaks = seq(3000, 6000, by = 500))"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#scales-in-action-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#scales-in-action-1",
    "title": "Data Science and Data Analytics",
    "section": "Scales in action",
    "text": "Scales in action\nNow, let’s say we also wanted to change the colours and shapes used to represent species and island, respectively. For manually adapting them, we use scale_color_manual and scale_shape_manual:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(170, 230, by = 5)) +\n  scale_y_continuous(breaks = seq(3000, 6000, by = 500)) +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\")) +\n  scale_shape_manual(values = c(17, 18, 8))"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-shapes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-shapes",
    "title": "Data Science and Data Analytics",
    "section": "A word on shapes",
    "text": "A word on shapes\n\nThe following shapes are most commonly used in R:\n\n\n\n\n\nYou can reference them by using the numbers indicated and passing them to scale_shape_manual in the order of the factor levels.\nIn the example before, we had Biscoe = 17, Dream = 18 and Torgersen = 8."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour",
    "text": "A word on colour\n\nWhenever we map color or fill as an aesthetic, ggplot2 uses default colour or fill scales that do a fine enough job for exploratory data analysis, but typically have to be tweaked for publication-ready plots.\nIn the example before, we have manually overridden the default using scale_color_manual and the names of three colours (red, blue and green).\nChoosing the right colour(s) for data visualization is actually a very complex problem that has to consider adequate representation of the underlying data as well as differences in colour perception.\nWe will only briefly skim the surface of colour choice by looking into some of the excellent options for setting colours that already exist out of the box in ggplot2."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-1",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour",
    "text": "A word on colour\n\nA collection of colours used for representing data is called a palette.\nA very popular family of such palettes available in ggplot2 are the colorbrewer palettes. For discrete variables, they come in three different types:\n\nQualitative: best suited to represent unordered categorical data, e.g. penguin species.\nSequential: best suited to represent ordered data that progresses from low to high, e.g. school grades or counts.\nDiverging: best suited to represent ordered data with a neutral midpoint and extremes diverging in both directions, e.g. Likert scales.\n\nDepending on the type of data we want represented, we can choose a palette from the corresponding family."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-qualitative-palettes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-qualitative-palettes",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – Qualitative palettes",
    "text": "A word on colour – Qualitative palettes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-sequential-palettes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-sequential-palettes",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – Sequential palettes",
    "text": "A word on colour – Sequential palettes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-diverging-palettes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-diverging-palettes",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – Diverging palettes",
    "text": "A word on colour – Diverging palettes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – How to use colorbrewer",
    "text": "A word on colour – How to use colorbrewer\n\nWe can access all of these palettes in our plots by referencing them using the names given in the functions scale_color_brewer and scale_fill_brewer, respectively.\nIn our example, we have the species variable mapped to colour, which is an unordered categorical variable. A suitable palette to represent it might therefore be Dark2, for example.\nTo do this, we would run the following code (result on the next slide):\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(170, 230, by = 5)) +\n  scale_y_continuous(breaks = seq(3000, 6000, by = 500)) +\n  scale_color_brewer(palette = \"Dark2\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-1",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – How to use colorbrewer",
    "text": "A word on colour – How to use colorbrewer"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-2",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – How to use colorbrewer",
    "text": "A word on colour – How to use colorbrewer\n\nWith these colorbrewer palettes, we can represent data from discrete variables, like categorical or count data.\nHowever, what if we want to represent a continuous variable using colour, such as penguin bill length?\nAll sequential and diverging colorbrewer palettes can also be used for continuous scales. Depending on the aesthetic, we only have to pass them into the functions scale_color_distiller or scale_fill_distiller.\nPenguin bill length does not have a neutral midpoint, so we use a sequential palette like YlOrRd to represent it (result on the next slide):\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = bill_length_mm)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(170, 230, by = 5)) +\n  scale_y_continuous(breaks = seq(3000, 6000, by = 500)) +\n  scale_color_distiller(palette = \"YlOrRd\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-3",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – How to use colorbrewer",
    "text": "A word on colour – How to use colorbrewer"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-final-word-on-colour",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-final-word-on-colour",
    "title": "Data Science and Data Analytics",
    "section": "A final word on colour",
    "text": "A final word on colour\n\nThe options for colour choice are virtually endless. The goal here was merely to introduce colour scales and how to change them.\nIf you find yourself unhappy with the colour palettes provided by colorbrewer, have a look at the Palette Finder in the R Graph Gallery:"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides",
    "title": "Data Science and Data Analytics",
    "section": "Understanding guides",
    "text": "Understanding guides\nNow let’s discuss guides. For this, consider again our violin plot from earlier:\n\nThis plot contains redundant information: the species can be inferred from the axis labels, we do not need the additional legend for species."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides-1",
    "title": "Data Science and Data Analytics",
    "section": "Understanding guides",
    "text": "Understanding guides\nTo “switch off” the guide for a particular aesthetic, we can simply call the guides function with the name of the aesthetic as a named argument set to \"none\":\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin() +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides-2",
    "title": "Data Science and Data Analytics",
    "section": "Understanding guides",
    "text": "Understanding guides\n\nOf course, the guides function can do much more than simply to “switch off” legends for individual scales.\nIn fact, it can be used to customize virtually every aspect of a legend such as:\n\nwhere the title of the legend should go\nwhere the labels should be placed\nin which orders the labels should be displayed\nand much more…\n\nAs with many aspects of ggplot2, this bridge is best crossed, when you first come to it…"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-themes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-themes",
    "title": "Data Science and Data Analytics",
    "section": "Understanding themes",
    "text": "Understanding themes\n\nFinally, a very important domain of plot customization is opened up by the theme function. It allows us to customize all aspects that are not directly related to the data being displayed.\nA very common basic use of the theme function is to place the legend at a different location in the plot and to change font sizes of different labels (result on the next slide):\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3) +\n  labs(title = \"Body mass and flipper length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       caption = \"Source: palmerpenguins package\",\n       color = \"Species\",\n       shape = \"Island\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = rel(1.75)),\n        plot.subtitle = element_text(size = rel(1.5)),\n        plot.caption = element_text(size = rel(0.75)))"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-themes-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-themes-1",
    "title": "Data Science and Data Analytics",
    "section": "Understanding themes",
    "text": "Understanding themes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#using-pre-built-themes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#using-pre-built-themes",
    "title": "Data Science and Data Analytics",
    "section": "Using pre-built themes",
    "text": "Using pre-built themes\n\nWhile customizing every single aspect of a plot is definitely possible in ggplot2, it is also incredibly cumbersome and usually not necessary.\nInstead, we can simply layer on a pre-built theme that changes the overall look of a plot all at once.\nThemes that come shipped with ggplot2 are: bw, classic, dark, gray (default), light, linedraw, minimal, test and void.\nThe following slide gives an overview of our plot would look like in each of these themes. If none of those suit your needs, you can have a look at the ggthemes package that has plenty more to offer."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#using-pre-built-themes-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#using-pre-built-themes-1",
    "title": "Data Science and Data Analytics",
    "section": "Using pre-built themes",
    "text": "Using pre-built themes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#saving-your-work",
    "href": "slides/04_The_Data_Science_Workflow_II.html#saving-your-work",
    "title": "Data Science and Data Analytics",
    "section": "Saving your work",
    "text": "Saving your work\n\nNow that we know how to create and customize our own plots, we might want to save them to include them in a presentation or send them to a colleague.\nThe easiest way to do this is to use the function ggsave. By default, it will save the most recently created plot into the file name you provide:\n\nggsave(filename = \"my_figure.png\")\n\nYou can save the plot as PDF, JPG, PNG (or several other formats) by changing the file ending accordingly.\nYou can also change the dimensions and the resolution of the plot by changing arguments width, height and dpi, respectively.\nAs always, for more details, refer to the documentation via ?ggsave."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-sneak-peek-into-what-is-possible-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-sneak-peek-into-what-is-possible-1",
    "title": "Data Science and Data Analytics",
    "section": "A sneak peek into what is possible",
    "text": "A sneak peek into what is possible"
  },
  {
    "objectID": "the-ds-workflow-i.html",
    "href": "the-ds-workflow-i.html",
    "title": "The Data Science Workflow I",
    "section": "",
    "text": "Download the PDF version here.",
    "crumbs": [
      "Slides",
      "The Data Science Workflow I"
    ]
  },
  {
    "objectID": "exercises/exercises-ii.html",
    "href": "exercises/exercises-ii.html",
    "title": "Exercises II",
    "section": "",
    "text": "The Data Science Workflow – Import\n\nOpen RStudio and run an R command in the console to determine your working directory. Set your working directory to your Downloads folder.\nDownload the Wine Quality data set from the UCI Machine Learning Repository. It contains physicochemical properties of red and white vinho verde wine samples, from the north of Portugal. The data set comprises two csv files, one for red and one for white wine samples. Inspect the file with a text editor to determine the correct import function and the arguments to be used. Then read both files into R using the appropriate function call and save the resulting data.frames in objects named white_wines and red_wines, respectively. Inspect the first 10 rows of the two data.frames using head and verify that the columns have the correct data types.\nThe file native.csv contains the names and favourite foods of three individuals. Inspect the file using a text editor and read the data into R using the appropriate function. What do you observe? How can you resolve this issue? Implement the necessary adaptations to read in the data without the occurring issue.\nThe Excel spreadsheet datasets.xls contains four worksheets, each with a different data set. Read all four work sheets into a list, such that each list element contains the data.frame holding the data of one work sheet. Inspect the head of each of the four data.frames to verify that the data has been read in correctly. Hint: you could use the excel_sheets function of the readxl package to identify the names of the four sheets and then read them into R using the appropriate function in a for loop.\nSearch the data documentation of the Gapminder project for data on GDP per capita in constant PPP dollars (PPP$2017 or PPP$2021). Gapminder offers all of their data sets in the form of publicly available Google Sheets documents. Open the corresponding Google Sheets document for the GDP per capita data and identify the sheet ID from the URL. Find the worksheet for regions (i.e. Africa, Asia, Europe and The Americas) and read the data from that worksheet into R as a data.frame with the help of the googlesheets4 package. Inspect the first couple of rows using head and verify that the columns have the correct data types.\n\n\n\nThe Data Science Workflow – Tidy and Transform\n\nThe flights data set:\n\nImport the nyc13_flights.csv data set and give it the name flights.\nUse the code from the lecture to give appropriate names to the columns of flights and inspect the first 10 rows of the data.frame using head.\nRemove all rows from flights that have an NA in any column (Hint: have a look at the function na.omit). How many rows in the data.frame are lost as a consequence of this NA removal step?\nThe departure times are given in a weird format distributed over several columns in the data.frame. To make this information usable, create a column called dep_time_dt, which contains the actual departure time as a POSIXct date-time. Hint: use paste to bring together the date information from the three relevant columns. Then use the %/% and %% operators to extract hour and minute from the dep_time column and paste it to the date. Finally, use as.POSIXct and the correct time zone to turn this string into a date-time.\nFilter the flights data set to only include:\n\nflights operated by United Air Lines or US Airways (carrier codes UA and US, respectively).\nflights from LaGuardia (LGA) to Fort Lauderdale (FLL).\nflights operated by Delta Air Lines (carrier code DL) with a departure delay of more than three hours.\nflights from Newark (EWR) to Chicago (ORD) operated by Envoy Air (carrier code MQ) on 1st June 2013.\nflights with an air time of at most half an hour operated by American Airlines (carrier code AA).\nflights from LaGuardia (LGA) or JFK to Nashville (BNA) with an air time of more than 2.5 hours.\nflights that departed after 16th May 2013 (careful: be aware of the time zone!).\n\nUsing appropriate functions in R, answer the following questions:\n\nWhat is the most frequent destination airport flown to from LaGuardia (LGA)?\nWhich carrier has the most departures out of JFK? Which proportion of the total number of departures do they cover there?\nWhich flight had the longest air time out of the ones contained in the data set?\nHow often is the flight with the shortest distance flown in the time period covered by the data set?\nWhat is the average departure delay at each of the three NYC origin airports?\nWhat is the average arrival delay of flights from JFK to Los Angeles (LAX) in each month?\nWhat is the tail number of the airplane that has covered the most distance over all flights in the data set?\nWhat is the distance of the longest flight (in terms of distance) offered out of LaGuardia (LGA) for each carrier?\n\nImport the nyc13_airports.csv data set and give it the name airports.\nThe airports data set contains the FAA code, the full name, latitude and longitude and time zone information of 1458 airports in the United States. Using an inner join, join the name, lat and lon of the destination airport into the flights data set (mind the differences in key column name between flights and airports!). Record the number of rows of the flights data set before and after the join. How many rows were lost due to the inner join? How could that have been avoided?\nRename the new columns in flights to dest_name, dest_lat and dest_lon, respectively.\nImport the nyc13_planes.csv data set and give it the name planes.\nThe planes data set contains the tail number, manufacturing year, type, manufacturer, model, number and type of engines and average cruising speed of 3322 airplanes in commercial service in the United States. Using a left join, join the manufacturer and model of the plane into the flights data set. For how many flights was no plane information found in planes?\nRename the new columns in flights to plane_manufacturer and plane_model, respectively.\n\nThe UCI Adult Income data set:\n\nDownload the Adult Income data set from the UCI Machine Learning Repository. It contains socio-economic data on individuals from the 1994 US census. When you download the data from the provided link, you will receive a zip archive that contains five files. Import the file adult.data into R using read.table and give the resulting data.frame the name income. Hint: the file can be opened with any text editor to inspect the delimiter and the presence of a header.\nUsing the information provided under the link above, give appropriate names to the columns of income.\nHave a look at the head of some individual columns. What do you notice?\nAdditionally to the leading spaces, this data set uses a question mark ? to indicate missing values. Both of these things would require manual work to handle. Fortunately, the read.table function has arguments strip.white to remove leading and trailing white space and na.strings to specify strings that are to be interpreted as NA. Use these two arguments to read adult.data into R again, redefining the income object you just created. Repeat exercise b. to rename the columns appropriately. Now, inspect the columns to verify that the leading spaces are gone and that the question marks have been turned into NAs.\nVerify that the variables in the data.frame have the correct data types for the data stored in them. Decide which of the included variables should be turned into a factor and go ahead with their transformation.\nA detail on factors left out from the lecture is that there are actually two types of them: unordered and ordered. Unordered factors should be used for nominal scales, while ordered factors should be used for ordinal scales. Turn the income variable in the data set into an ordered factor (you can use as.ordered) and verify that the two levels are in the correct order by inspecting head head of that variable.\nThe variable marital_status uses three different levels to mean “married”, namely Married-AF-spouse, Married-civ-spouse and Married-spouse-absent. This distinction is not relevant for us, we want to subsume all of these levels under the category Married. To do this, redefine the factor for marital_status and use the labels argument of factor function to map all three “married”-levels to the level Married. Verify that redefining the factor worked by computing the frequency table for the different marital statuses.\nUsing appropriate functions in R, answer the following questions:\n\nWhat is the average age of individuals broken down by education level?\nWhich proportion of men are married broken down by race? (Hint: you can create two-dimensional contingency tables by passing more than one vector into table and compute row or column percentages with prop.table.)\nHow many hours per week do people with a doctorate work on average? Is that the highest average work load of all education levels?\nWhich proportion of people whose native country is the US make more than 50k?\nHow many people in the data set are black and have a masters or a doctoral degree?",
    "crumbs": [
      "Exercises",
      "Exercises II"
    ]
  },
  {
    "objectID": "exercises/exercises-iii.html",
    "href": "exercises/exercises-iii.html",
    "title": "Exercises III",
    "section": "",
    "text": "The Data Science Workflow – Visualize\n\nInvestigating the relationship between age and number of children based on the gss_sm data set:\n\nThe socviz package contains a data set called gss_sm, which is a small subset of the questions from the 2016 General Social Survey (GSS). The GSS is a long-running survey of American adults that asks about a range of topics of interest to social scientists. To access this data set, start by installing and loading the socviz package.\nStart by creating a simple scatter plot of age (on the x axis) against childs (on the y axis), i.e. illustrate how the number of children varies with age.\nThe plot from task b. suffers from overplotting, i.e. many data points are in the same position and overlap. To avoid this, see what happens when you set the position argument of the point geom to \"jitter\".\nNow, add a smooth line describing the relationship between age and number of children into the plot.\nFacet the plot by sex (rows) and race (columns).\nChange the theme of the plot to theme_minimal. Also, add axis tick marks on the x axis every 10 years. Finally, choose your favourite colour from here and use its name to colour the points in the plot. Also set their size to 0.25 to make them smaller and further reduce the overplotting issue. Finally, label the axes with Age and Number of Children, respectively.\n\nAnalyzing the relationship between education and political views based on the gss_sm data set:\n\nCreate a frequency table of the degree variable and save it as a variable called degree_freqs. Turn it into a data.frame with column names degree and freq.\nUse the degree_freqs object to create a bar plot indicating the absolute frequencies of the different levels of education in the data set.\nReorder the degrees in the bar plot to reflect the order of their absolute frequencies from highest to lowest. Also, relabel the axes to Education Level and Frequency, respectively.\nNow, create a contingency table of the variables degree and polviews and save it as a variable called degrees_polview. Turn it into a data.frame with column names degree, polviews and freq.\nUsing degrees_polview, create a stacked bar plot of degree, where the fill aesthetic is used to indicate polviews. Afterwards, also create a bar plot of polviews, where the fill aesthetic is used to indicate degree.\nTurn both bar plots from task e. into ones that show conditional relative frequencies instead of absolute frequencies to show the relationship between education and political views.\nFor the first of the two bar plots, add the title “Education and political views”, a subtitle “Data from the 2016 General Social Survey”, a caption “Source: socviz package”, appropriate axes labels and a header for the legend saying “Political views”. Moreover, choose a suitable color palette from the colorbrewer set and apply it to the graph. Finally, use theme_economist_white from the ggthemes package.\nFinally, use the code you have already written to also have a look at the relationship between religion and polviews. For the most part, this should just involve some copying and pasting as well as adapting certain labels accordingly. Note that you have to create a new contingency table, of course.\n\nAnalyzing the relationship between economic well-being and life expectancy for countries in the world in 2020 based on the gapminder data set:\n\nIn the course materials, you should find the data sets called gm_gdp.csv, gm_lex.csv, gm_pop.csv and gm_geo.csv. These data sets all stem from the Gapminder Project, a Swedish non-profit venture that promotes understanding of information about social, economic, and environmental development through the use of data visualization and statistics. Start by loading all these four data sets into R under the names of gdp, lex, pop and geo, respectively. Inspect these four data sets and familiarize yourself with their structure. Refer to gm_description.csv for additional information if you need it. Rename the following columns:\n\nthe fourth column of gdp to gdp_percap.\nthe third column of lex to time and the fourth column to life_expectancy.\nthe fourth column of pop to population.\n\nPerform the following joins in the given order:\n\nan inner join between columns 1, 3 and 4 of both gdp and lex on variables geo and time. Call the resulting object gm_df.\nan inner join between gm_df and columns 1, 3 and 4 of pop on variables geo and time. Redefine gm_df as the resulting object.\nan inner join between gm_df and columns 1, 2 and 3 of geo on variable geo. Redefine gm_df as the resulting object.\n\nVerify that gm_df now has columns geo, time, gdp_percap, life_expectancy, population, name and four_regions.\nWe will start by illustrating the relationship between GDP per capita and life expectancy at birth in a single year. For this, create a data.frame called gm_df2020 that only includes data points from the year 2020. Using gm_df2020, then create a simple scatter plot of gdp_percap on the x axis and life_expectancy on the y axis.\nIn the plot, you can see a point at life_expectancy 0. This is because in the year 2020, we do not have life expectancy information for Hong Kong and Liechtenstein. The corresponding values are set to 0, which is not sensible. To avoid this being visible, we can simply restrict the plot range of the y axis. Set it, using scale_y_continuous, to the range from 40 to 95.\nAnother problem of this plot is that the GDP per capita spans a very wide range of values. To mitigate this for visualization, apply log2-scaling to the x axis. Hint: check the documentation for possible values to pass to the transform argument of the scale_x_continuous function.\nTo match the log2-scaling of the x axis, set the tick marks at powers of 2 starting from 256 and ending at 131,072.\nNow, we want the size of the points to represent the sizes of the country populations. Add the corresponding aesthetic for population to your plot. Control the minimal and maximal sizes of the points via the range argument of scale_size_continuous, the minimum should be 0.5 and the maximum should be 20.\nThe big size of some points now means that many points are not visible due to overlap. To mitigate this, increase the transparency of the points by setting alpha to 0.4.\nNext, it would be nice to represent the four world regions (Africa, Americas, Asia and Europe) by colour. Add the corresponding aesthetic for the variable four_regions and apply a suitable colorbrewer palette of your choosing.\nAdd the label GDP per capita (log scale) to the x axis and the label Life expectancy at birth to the y axis. Furthermore, add a title saying “Economic well-being and life expectancy”, a subtitle saying “Data from 2020” and a caption saying “Source: gapminder” to the plot. Rename the header of the legend for the world regions to World Region.\nFinally, remove the guide for the size aesthetic and change the theme to theme_test.\n\nAnalyzing the relationship between child mortality and total fertility rate over time based on the gapminder data set:\n\nWe will now merge additional data into the data.frame called gm_df created in exercise 3. First load the data sets gm_tfr.csv and gm_cm.csv into R under the names of tfr and cm. The first contains data on the total fertility rate (i.e. number of babies per woman) and the second contains data on child mortality (i.e. number of 0-5 year-olds dying per 1000 born children). After import, rename their fourth columns to tfr and cm, respectively.\nPerform the following joins in the given order:\n\nan inner join between gm_df and columns 1, 3 and 4 of tfr on variables geo and time. Redefine gm_df as the resulting object.\nan inner join between gm_df and columns 1, 3 and 4 of cm on variables geo and time. Redefine gm_df as the resulting object.\n\nVerify that the variables tfr and cm are now available in the data.frame called gm_df.\nWe want to look at the relationship between child mortality and total fertility rate in six years, namely 1800, 1900, 1930, 1950, 1980 and 2010. Restrict gm_df to only include data from these six years.\nCreate a scatter plot with cm on the x axis and tfr on the y axis. Facet the plot by time into a 2 x 3 grid using facet_wrap, such that each subplot represents one of the six years we have pre-selected.\nNow, map four_regions and population to the color and size aesthetics, respectively, exactly in the same way as in exercise 3. To prevent too large points dominating the subplots, the only difference should be setting the maximum point size to 10.\nThis time, we want to be able to read off at least some of the individual countries from the plot. To do this, load the ggrepel package, map the name variable to the label aesthetic and use geom_text_repel with arguments size = 2 and max_overlaps = 15 to add the labels.\nLabel the plot appropriately and adapt the overall plot design to conform with the appearance of the final plot from exercise 3.\n\nAnalyzing the relationship between chemical properties of wine and its perceived quality:\n\nIn exercise set II, you downloaded and imported the Wine Quality data sets from the UCI Machine Learning Repository. Import both of these data sets under the names white_wines and red_wines again. To both data.frames, add a column called type, which is white for all wines in the first one and red for all wines in the second one. Then, using the function rbind, combine the two data frames into one called wines. Finally, turn the variables quality and type into a factor.\nFirst, we want to investigate the relationship between alcohol percentage of the wines and their sensory quality. Create parallel boxplots of the alcohol variable for all observed levels of the quality variable. Use the fill aesthetic to represent the two different types of wine.\nUse coord_flip to have quality on the y axis and alcohol on the x axis.\nLet’s represent the wines in appropriate colours. Use scale_fill_manual to manually adapt the colours of the fill aesthetic. One way to specify colours in R is with the help of so-called hex codes, use “#7B0323” for red wines and “#F1EAA3” for white wines. To make these colours visible, choose a theme with a white background, such as theme_test. Finally, label your plot appropriately.\nCreate further similar plots relating the wine quality to different chemical properties. In particular:\n\nvolatile.acidity. Try using a jitter geom instead of the boxplot geom. Experiment with the point size to reduce the overplotting issue.\nsulphates. Try using a violin geom this time.\n\n\nAnalyzing flipped counties by demographics in the 2016 US presidential election:\n\nIn the socviz package, there is a data set called county_data, which contains county-level data from the 2016 US presidential election. In this data set, the variable flipped indicates, whether a given county flipped to Republican or Democrat compared to the 2012 election. Start by creating a simple scatter plot of county population (pop) on the x axis and the percentage of black population (black/100) on the y axis, but only for the counties that did not flip in the 2016 election. Use a log10 scale on the x axis, color the points in grey and increase their transparency by setting alpha = 0.25.\nNow, add points for the counties that did flip in the 2016 election. You do this simply by adding another point geom, but with its own data set and aesthetic mapping. Map the variable partywinner16 (indicating the party to which the county flipped in the election) to the color aesthetic and use the typical Republican and Democrat party colours (which is “#CB454A” for Republican and “#2E74C0” for Democrat) as the colours.\nFor nicely labeled tick marks, we can use the scales package. Having installed the package, load it and pass the function comma as the labels argument to scale_x_log10 and the function percent as the labels argument to scale_y_continuous. Notice how that changes the labels for the tick marks in the plot.\nAdd the following labels your plot:\n\nx: County population (log scale)\ny: Percent black population\ntitle: Flipped counties, 2016\nsubtitle: Counties in grey did not flip.\ncaption: Source: socviz package\ncolor: County flipped to…\n\nUse a text_repel geom to label all counties that flipped and have a black population of more than 25% with a label indicating the state they belong to. Set the size of the text to 2. Finally, set the theme to theme_economist from the ggthemes package.",
    "crumbs": [
      "Exercises",
      "Exercises III"
    ]
  },
  {
    "objectID": "exercises/exercise-solutions-iv.html",
    "href": "exercises/exercise-solutions-iv.html",
    "title": "Exercises IV",
    "section": "",
    "text": "The Data Science Workflow – Model\n\nThe ISLR2 package contains the Boston data set, which records medv (median house value) for 506 suburbs of Boston. We will seek to predict medv using 12 predictors such as rm (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940) and lstat (percent of households with low socio-economic status).\n\nInstall and load the ISLR2 package. Investigate the description of all variables in the Boston data set via ?Boston.\n\n\ninstall.packages(\"ISLR2\")\nlibrary(ISLR2)\n?Boston\n\n\nWe will first do a random train-test split. Using the function sample, randomly choose 405 (i.e. approximately 80%) of the observations and assign them to a new data.frame called train. Assign all others to a data.frame called test.\n\n\nlibrary(ISLR2)\nn &lt;- nrow(Boston)\nn_train &lt;- 405\ntrain_ind &lt;- sample(n, n_train)\n\ntrain &lt;- Boston[train_ind, ]\ntest &lt;- Boston[-train_ind, ]\n\n\nUsing the training data set, estimate a simple linear regression model with medv as the response and lstat as the predictor. Inspect the created lm object using summary and interpret regression coefficients, their statistical significance and the \\(R^2\\) statistic. Furthermore determine a 95%-confidence interval for the slope coefficient \\(\\beta_1\\).\n\n\nm1 &lt;- lm(medv ~ lstat, data = train)\nsummary(m1)\n\n\nCall:\nlm(formula = medv ~ lstat, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.065  -3.883  -1.405   1.721  24.609 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  34.4851     0.6170   55.89   &lt;2e-16 ***\nlstat        -0.9543     0.0427  -22.35   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.148 on 403 degrees of freedom\nMultiple R-squared:  0.5534,    Adjusted R-squared:  0.5523 \nF-statistic: 499.4 on 1 and 403 DF,  p-value: &lt; 2.2e-16\n\n# Note: exact values will be different for every run due to the random selection of\n# train and test data.\n# Interpretation of regression coefficients:\n# slope: for an increase in the percentage of households with low socio-economic status\n# by one percentage point, the median house value drops on average around 954.30$.\n# intercept: in a suburb with no households of low socio-economic status, the median\n# house value is around 34,485$.\n# Both intercept and slope are significantly different from zero at all usual significance\n# levels (e.g. also 0.1%).\n\n# Interpretation of R^2:\n# A model containing only lstat as an explanatory variable is able to explain around\n# 55.3% of the variance in median house values between suburbs.\n\nconfint(m1, \"lstat\")\n\n          2.5 %     97.5 %\nlstat -1.038243 -0.8703427\n\n\n\nVisualize the simple linear regression model you just fitted on the training data using a scatter plot. Hint: simply use geom_smooth with the method equal to lm and se set to FALSE to add the regression line into the scatter plot. Does the linear fit appear appropriate for the data at hand?\n\n\nlibrary(ggplot2)\nggplot(train, aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE) +\n  theme_minimal()\n\n\n\n\n\n\n\n# For higher values of lstat, the relationship with medv seems to flatten\n# out, which indicates that a linear fit might not be appropriate to\n# model this relationship.\n\n\nNow, estimate a new model that uses the logarithm of lstat as an explanatory variable. Inspect its \\(R^2\\) and compare to the one of the simple model that is linear in lstat. Visualize the corresponding regression line in another scatter plot. Hint: for the visualization of the non-linear regression line, use geom_function. This requires you to specify a function to be drawn into the scatter plot, for which you can use predict.\n\n\nm2 &lt;- lm(medv ~ log(lstat), data = train)\nsummary(m2)\n\n\nCall:\nlm(formula = medv ~ log(lstat), data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.3012  -3.3930  -0.5836   1.9879  26.1718 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  51.9673     1.0507   49.46   &lt;2e-16 ***\nlog(lstat)  -12.4816     0.4312  -28.94   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.243 on 403 degrees of freedom\nMultiple R-squared:  0.6752,    Adjusted R-squared:  0.6744 \nF-statistic: 837.8 on 1 and 403 DF,  p-value: &lt; 2.2e-16\n\n# With this model, approximately 67.5% of the variance in the median house values\n# can be explained, which is considerably higher than the 55.3% of the model before.\n# This indicates that the log fit is indeed more suitable here.\n\nggplot(train, aes(x = lstat, y = medv)) + \n  geom_point() + \n  geom_function(fun = function(x) predict(m2, newdata = data.frame(lstat = x)),\n                color = \"blue\", linewidth = 1) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe now want to compare the performance of different linear regression models using 10-fold cross validation. For this, first assign each training observation randomly to a fold between 1 and 10. Next, using these folds, determine the test MSE for each of the following linear regression models.\n\nM1: medv ~ lstat\nM2: medv ~ log(lstat)\nM3: medv ~ log(lstat) + crim\nM4: medv ~ log(lstat) + crim + dis\nM5: medv ~ log(lstat) + dis + rm\nM6: medv ~ log(lstat) + dis + ptratio\nM7: medv ~ lstat + dis + ptratio + crim + nox\nM8: medv ~ log(lstat) + dis + ptratio + crim + nox\n\n\n\nK &lt;- 10\ntrain$fold &lt;- sample(rep(1:K, length.out = n_train))\nmses &lt;- matrix(NA, K, 8)\ncolnames(mses) &lt;- sprintf(\"M%d\", 1:8)\n\nfor(k in 1:K){\n  m1 &lt;- lm(medv ~ lstat, data = subset(train, fold != k))\n  m2 &lt;- lm(medv ~ log(lstat), data = subset(train, fold != k))\n  m3 &lt;- lm(medv ~ log(lstat) + crim, data = subset(train, fold != k))\n  m4 &lt;- lm(medv ~ log(lstat) + crim + dis, data = subset(train, fold != k))\n  m5 &lt;- lm(medv ~ log(lstat) + dis + rm, data = subset(train, fold != k))\n  m6 &lt;- lm(medv ~ log(lstat) + dis + ptratio, data = subset(train, fold != k))\n  m7 &lt;- lm(medv ~ lstat + dis + ptratio + crim + nox, data = subset(train, fold != k))\n  m8 &lt;- lm(medv ~ log(lstat) + dis + ptratio + crim + nox, data = subset(train, fold != k))\n\n  predsm1 &lt;- predict(m1, newdata = subset(train, fold == k))\n  predsm2 &lt;- predict(m2, newdata = subset(train, fold == k))\n  predsm3 &lt;- predict(m3, newdata = subset(train, fold == k))\n  predsm4 &lt;- predict(m4, newdata = subset(train, fold == k))\n  predsm5 &lt;- predict(m5, newdata = subset(train, fold == k))\n  predsm6 &lt;- predict(m6, newdata = subset(train, fold == k))\n  predsm7 &lt;- predict(m7, newdata = subset(train, fold == k))\n  predsm8 &lt;- predict(m8, newdata = subset(train, fold == k))\n\n  mses[k, 1] &lt;- mean((predsm1 - train$medv[train$fold == k])^2)\n  mses[k, 2] &lt;- mean((predsm2 - train$medv[train$fold == k])^2)\n  mses[k, 3] &lt;- mean((predsm3 - train$medv[train$fold == k])^2)\n  mses[k, 4] &lt;- mean((predsm4 - train$medv[train$fold == k])^2)\n  mses[k, 5] &lt;- mean((predsm5 - train$medv[train$fold == k])^2)\n  mses[k, 6] &lt;- mean((predsm6 - train$medv[train$fold == k])^2)\n  mses[k, 7] &lt;- mean((predsm7 - train$medv[train$fold == k])^2)\n  mses[k, 8] &lt;- mean((predsm8 - train$medv[train$fold == k])^2)\n}\ncolMeans(mses)\n\n      M1       M2       M3       M4       M5       M6       M7       M8 \n38.18925 27.75549 27.40817 24.42848 23.38412 22.49007 29.82197 21.32557 \n\n\n\nWhich of the models investigated in f. gives the lowest test MSE? Fit the corresponding model to the entire training data and investigate the model more deeply using summary.\n\n\nwhich.min(colMeans(mses))\n\nM8 \n 8 \n\n# Clearly, model M8 is the best. So we fit it to the entire training data:\nm8_full &lt;- lm(medv ~ log(lstat) + dis + ptratio + crim + nox, data = train)\nsummary(m8_full)\n\n\nCall:\nlm(formula = medv ~ log(lstat) + dis + ptratio + crim + nox, \n    data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.6082  -2.7435  -0.3968   1.9661  25.5635 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.95769    3.11192  25.373  &lt; 2e-16 ***\nlog(lstat)  -11.63807    0.50507 -23.043  &lt; 2e-16 ***\ndis          -1.37921    0.16825  -8.197 3.39e-15 ***\nptratio      -0.91311    0.11961  -7.634 1.70e-13 ***\ncrim         -0.08108    0.02848  -2.847 0.004646 ** \nnox         -11.85103    3.35019  -3.537 0.000452 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.564 on 399 degrees of freedom\nMultiple R-squared:  0.7563,    Adjusted R-squared:  0.7533 \nF-statistic: 247.7 on 5 and 399 DF,  p-value: &lt; 2.2e-16\n\n\n\nUsing your cross-validation setup from f., compare the test MSE performance of the best-performing linear regression model now to the performance of KNN regression models that uses the same variables, i.e. lstat, dis, ptratio, crim and nox. Let \\(K\\) vary between 2 and 10 and only compare the best-performing KNN approach to model M8. Is the non-parametric approach able to outperform the parametric one?\n\n\nlibrary(FNN)\nn_neighbours &lt;- 2:10 # number of neighbours considered\nmses_knn &lt;- matrix(NA, K, length(n_neighbours))\ncolnames(mses_knn) &lt;- sprintf(\"K = %d\", n_neighbours)\n\nfor(k in 1:K){\n  preds &lt;- lapply(n_neighbours, function(x) knn.reg(train = subset(train, fold != k,\n                                                                   c(12, 8, 11, 1, 5)),\n                                                    test = subset(train, fold == k,\n                                                                  c(12, 8, 11, 1, 5)),\n                                                    y = train$medv[train$fold != k],\n                                                    k = x)$pred)\n  mses_knn[k, ] &lt;- colMeans((do.call(cbind, preds) - train$medv[train$fold == k])^2)\n}\nround(colMeans(mses_knn), 2)\n\n K = 2  K = 3  K = 4  K = 5  K = 6  K = 7  K = 8  K = 9 K = 10 \n 22.06  22.39  22.26  21.31  22.06  21.73  21.63  21.98  21.75 \n\n# The best-performing KNN regression model has K = 5. This achieves a test MSE of\n# 21.31, which virtually identical to the performance of M8, which had 21.33.\n# Hence, when using only the 5 predictors under investigation, there is no major\n# difference in performance between linear regression and KNN regression.\n\n\nUsing all variables available in the data set, can you find an even better model? Fit your model, the model M8 and the KNN regression model with \\(K = 5\\) from before on the entire train data set. Using these fits, compare the performance of the three models on the initial hold-out test data set from b. Which of the three approaches has the lowest MSE on that data set?\n\n\n# After investigating some more linear models in the context of cross validation,\n# the following model appeared particularly promising:\nnew_m &lt;- lm(medv ~ log(lstat) + log(dis) + ptratio + crim + nox + rm + tax + rad,\n            data = train)\nm8 &lt;- lm(medv ~ log(lstat) + dis + ptratio + crim + nox, data = train)\nknn &lt;- knn.reg(train = train[, c(12, 8, 11, 1, 5)],\n               test = test[, c(12, 8, 11, 1, 5)],\n               y = train$medv,\n               k = 5)\n\n# Predict the test response with the three models:\nnew_m_pred &lt;- predict(new_m, newdata = test)\nm8_pred &lt;- predict(m8, newdata = test)\nknn_pred &lt;- knn$pred\n\n# Compute MSEs:\nnew_m_mse &lt;- mean((new_m_pred - test$medv)^2)\nm8_mse &lt;- mean((m8_pred - test$medv)^2)\nknn_mse &lt;- mean((knn_pred - test$medv)^2)\n\n# Display MSEs:\nnew_m_mse\n\n[1] 19.12033\n\nm8_mse\n\n[1] 26.08365\n\nknn_mse\n\n[1] 21.34268\n\n# Clearly, the model new_m delivers the lowest MSE on the test set out of the 3.\n\n\nNow, fit a regression tree that uses predictors lstat, dis, ptratio, crim, nox, rm, tax and rad to the entire training data. Make sure to set the complexity parameter cp to zero in the fitting process. Visualize the resulting tree using corresponding functionality from the rpart.plot package.\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\nrt &lt;- rpart(medv ~ lstat + dis + ptratio + crim + nox + rm + tax + rad, data = train,\n            control = list(cp = 0))\nrpart.plot(rt)\n\n\n\n\n\n\n\n\n\nUsing the created rpart object, determine the optimal complexity parameter cp and create an optimally pruned tree. Finally, use both the unpruned and the optimally pruned tree to predict the response in the initial hold-out test data set from b. How do their MSEs compare to the models investigated in i.? Hint: Prediction with decision trees from rpart works just like it does for linear regression, so that you can use the predict function.\n\n\n# Graphic analysis (run rsq.rpart(rt)) shows that a reasonable number of splits\n# is 9 or 14 (subjective judgement required). Going for 14 splits, the corresponding\n# cp parameter is 0.00371291, so we use that to prune the tree:\nrt_pruned &lt;- prune(rt, cp = 0.00371291)\n\n# Prediction on the test data:\nrt_pred &lt;- predict(rt, newdata = test)\nrt_pruned_pred &lt;- predict(rt_pruned, newdata = test)\n\n# Compute MSEs:\nrt_mse &lt;- mean((rt_pred - test$medv)^2)\nrt_pruned_mse &lt;- mean((rt_pruned_pred - test$medv)^2)\n\n# Display MSEs:\nrt_mse\n\n[1] 22.19599\n\nrt_pruned_mse\n\n[1] 22.58178\n\n# In this case, pruning did not improve performance. Both models are worse\n# than the best model from i.\n\nThe ISLR2 package also contains the Default data set, which contains information on ten thousand credit card customers. More specifically, the data.frame holds the following four variables: default (a factor with levels No and Yes indicating whether the customer defaulted on their debt), student (a factor indicating whether the customer is a student), balance (average outstanding balance at the end of the month), income (income of the customer in USD).\n\nStart by appropriately visualizing the relationship between default (as the response) and each of the other three variables (as the predictors). Also create a scatter plot of balance against income, where the default cases are highlighted via colour and shape.\n\n\nlibrary(ggplot2)\n\n# Default and balance: parallel violin plots\nggplot(Default, aes(x = default, y = balance, fill = default)) + \n  geom_violin() + \n  guides(fill = \"none\")\n\n\n\n\n\n\n\n# Default and income: parallel violin plots\nggplot(Default, aes(x = default, y = income, fill = default)) + \n  geom_violin() + \n  guides(fill = \"none\")\n\n\n\n\n\n\n\n# Default and student: normalized stacked bar charts\ndef_by_student &lt;- as.data.frame(with(Default, table(default, student)))\nggplot(def_by_student, aes(x = student, y = Freq, fill = default)) + \n  geom_bar(stat = \"identity\", position = \"fill\")\n\n\n\n\n\n\n\n# Scatter plot of balance and income with default indicated by colour:\nggplot(Default, aes(x = balance, y = income, colour = default, shape = default)) + \n  geom_point(size = 0.8) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSince the data set is quite big (\\(n = 10000\\)), we can afford to again set some observations aside as a test set for final evaluation. Using the function sample, randomly assign 90% of the observations to a new data.frame called train and assign all others to a data.frame called test.\n\n\nn &lt;- nrow(Default)\ntrain_ind &lt;- sample(n, floor(0.9*n))\n\ntrain &lt;- Default[train_ind, ]\ntest &lt;- Default[-train_ind, ]\n\n\nWrite a function called binary_class_metrics that takes two factor arguments y_pred and y_true, both with two levels. The first represents the predicted classes and the second represents the true classes. The function is then supposed to return a named vector holding the accuracy, recall, precision and f1 score of the binary classification of y_true using y_pred. Test your function for the case where the predictions are 1,1,1,0,0,0 and the truth is 1,1,0,1,1,0. Hint: first compute the confusion matrix by using the function table and then compute the four metrics from that table.\n\n\nbinary_class_metrics &lt;- function(y_pred, y_true){\n  # Check that both variables are factors of the same length\n  n &lt;- length(y_pred)\n  stopifnot(is.factor(y_pred),\n            is.factor(y_true),\n            length(y_true) == n)\n  # Check that levels are identical and that there are only 2 levels\n  stopifnot(all(levels(y_pred) == levels(y_true)),\n            length(levels(y_pred)) == 2)\n\n  confusion_matrix &lt;- table(y_pred, y_true)\n  accuracy &lt;- sum(diag(confusion_matrix))/n\n  recall &lt;- confusion_matrix[2,2] / sum(confusion_matrix[, 2])\n  precision &lt;- confusion_matrix[2,2] / sum(confusion_matrix[2, ])\n  f1 &lt;- 2*recall*precision/(recall + precision)\n  return(c(accuracy = accuracy, recall = recall, precision = precision, f1 = f1))\n}\n\nbinary_class_metrics(factor(c(1,1,1,0,0,0)), factor(c(1,1,0,1,1,0)))\n\n accuracy    recall precision        f1 \n0.5000000 0.5000000 0.6666667 0.5714286 \n\n\n\nOn the entire training data, estimate a logistic regression model for the default variable, using the other three variables as predictors. Inspect the model and interpret the coefficients. Which of them are significantly different from zero at the 5% significance level?\n\n\nlog_reg &lt;- glm(default ~ student + balance + income, data = train, family = binomial())\nsummary(log_reg)\n\n\nCall:\nglm(formula = default ~ student + balance + income, family = binomial(), \n    data = train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.083e+01  5.216e-01 -20.772  &lt; 2e-16 ***\nstudentYes  -7.168e-01  2.522e-01  -2.842  0.00448 ** \nbalance      5.740e-03  2.472e-04  23.220  &lt; 2e-16 ***\nincome       1.883e-06  8.679e-06   0.217  0.82824    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2576.5  on 8999  degrees of freedom\nResidual deviance: 1391.5  on 8996  degrees of freedom\nAIC: 1399.5\n\nNumber of Fisher Scoring iterations: 8\n\n# Interpretation:\n# Ceteris paribus, the log odds for default are around 0.7168 lower on average\n# for a student than a non-student.\n# Ceteris paribus, for every additional one hundred USD in average outstanding\n# balance, the log odds for default increases by around 0.5740 on average.\n# Ceteris paribus, for every additional one thousand USD in income, the log\n# odds for default increases by around 0.001883 on average.\n\n# Significance:\n# At the 5% level, only the student dummy and the balance variable are significant.\n\n\nEmploying your function binary_class_metrics from earlier, investigate the training set performance of using the logistic regression model from d. as a classifier. Interpret the values of accuracy, recall and precision. Hint: with the fitted function, you can get the estimated default probabilities from your glm object. Be careful to set the factor levels to No and Yes to comply with the default factor in the training data.\n\n\nlog_reg_preds_train &lt;- factor(ifelse(fitted(log_reg) &gt; 0.5, \"Yes\", \"No\"))\nlog_reg_perf_train &lt;- binary_class_metrics(log_reg_preds_train, train$default)\nlog_reg_perf_train\n\n accuracy    recall precision        f1 \n0.9743333 0.3219178 0.7401575 0.4486874 \n\n# Interpretation accuracy: 97.4% of the training observations were correctly classified.\n# Interpretation recall: 32.2% of the true default cases were correctly identified in\n# the training data.\n# Interpretation precision: When the logistic regression predicted default in the\n# training data, it was correct 74.0% of the time.\n\n\nIn the Default data set, we have one categorical predictor, namely whether the customer is a student or not. While logistic regression was easily able to handle that, this is more difficult for KNN classification, as one must decide how to measure distance in a feature space that combines categorical and numerical features. As we have a lot of data, we will follow a simple approach: fitting a separate KNN classifier for students and non-students, both with the same value for \\(K\\). Use 10-fold cross validation on the training data to find the value for \\(K\\) that maximizes the F1 score. Hint: in every iteration over the 10 folds, fit and predict the students and non-students separately using the two numeric features as predictors. Remember to scale the numeric features in every iteration.\n\n\nK &lt;- 10\ntrain$fold &lt;- sample(rep(1:K, length.out = nrow(train)))\n\nn_neighbours &lt;- 2:20\nfor(nn in n_neighbours){\n  knn_colname &lt;- sprintf(\"class_preds_KNN%d\", nn)\n  train[[knn_colname]] &lt;- factor(NA, levels = c(\"No\", \"Yes\"))\n  for(k in 1:K){\n    X_train_students &lt;- subset(train, fold != k & student == \"Yes\", 3:4)\n    X_train_students_scaled &lt;- scale(X_train_students)\n    y_train_students &lt;- train$default[train$fold != k & train$student == \"Yes\"]\n    X_test_students &lt;- subset(train, fold == k & student == \"Yes\", 3:4)\n    X_test_students_scaled &lt;- scale(X_test_students)\n\n    X_train_nonstudents &lt;- subset(train, fold != k & student == \"No\", 3:4)\n    X_train_nonstudents_scaled &lt;- scale(X_train_nonstudents)\n    y_train_nonstudents &lt;- train$default[train$fold != k & train$student == \"No\"]\n    X_test_nonstudents &lt;- subset(train, fold == k & student == \"No\", 3:4)\n    X_test_nonstudents_scaled &lt;- scale(X_test_nonstudents)\n\n    preds_students &lt;- knn(train = X_train_students_scaled,\n                          test = X_test_students_scaled, cl = y_train_students, k = nn)\n\n    preds_nonstudents &lt;- knn(train = X_train_nonstudents_scaled,\n                             test = X_test_nonstudents_scaled,\n                             cl = y_train_nonstudents, k = nn)\n    train[train$fold == k & train$student == \"Yes\", knn_colname] &lt;- preds_students\n    train[train$fold == k & train$student == \"No\", knn_colname] &lt;- preds_nonstudents\n  }\n}\nknn_performance &lt;- sapply(6:24, function(i) binary_class_metrics(train[,i], train[,1]))\nK_opt &lt;- 9\n\n\nNext, fit a classification tree for default using the other three variables as predictors on the entire training data. Make sure to set the complexity parameter cp to zero in the fitting process. Visualize the resulting tree using corresponding functionality from the rpart.plot package.\n\n\nct &lt;- rpart(default ~ student + balance + income, data = train, control = list(cp = 0))\nrpart.plot(ct)\n\n\n\n\n\n\n\n\n\nUsing the fitted classification tree from g., determine the optimal complexity parameter cp and fit and visualize the optimally pruned classification tree.\n\n\n# Graphic analysis (run rsq.rpart(ct)) shows that a reasonable number of splits is 5.\n# The corresponding cp parameter is 0.0049505, so we use that\n# to prune the tree:\nct_pruned &lt;- prune(ct, cp = 0.0049505)\nrpart.plot(ct_pruned)\n\n\n\n\n\n\n\n\n\nFinally, use the logistic regression model from d., the combined KNN classifier with optimal \\(K\\) from f., the full classification tree from g. and the pruned classification tree from h. to predict the test observations in the initial hold-out sample from b. Feed the predictions of each into your function binary_class_metrics to evaluate their test set performances. Beyond just the metrics, consider the business implications of the different types of errors each model makes:\n\nFalse Positive (Predicting default when the customer will not): What are the potential costs to the credit card company?\nFalse Negative (Predicting no default when the customer will default): What are the potential costs to the credit card company?\nBased on these costs and the performance metrics, which model do you think the credit card company might prefer and why? Present the performance metrics in a table and follow it with your discussion.\n\n\nlog_reg_preds &lt;- factor(ifelse(predict(log_reg, newdata = test, type = \"response\") &gt; 0.5,\n                        \"Yes\", \"No\"))\n\nknn_preds &lt;- factor(rep(NA, nrow(test)), levels = c(\"No\", \"Yes\"))\nX_train_students_scaled &lt;- scale(train[train$student == \"Yes\", 3:4])\nX_train_nonstudents_scaled &lt;- scale(train[train$student == \"No\", 3:4])\ny_train_students &lt;- train$default[train$student == \"Yes\"]\ny_train_nonstudents &lt;- train$default[train$student == \"No\"]\nX_test_students_scaled &lt;- scale(test[test$student == \"Yes\", 3:4])\nX_test_nonstudents_scaled &lt;- scale(test[test$student == \"No\", 3:4])\nknn_preds[test$student == \"Yes\"] &lt;- knn(train = X_train_students_scaled,\n                                        test = X_test_students_scaled,\n                                        cl = y_train_students, k = K_opt)\nknn_preds[test$student == \"No\"] &lt;- knn(train = X_train_nonstudents_scaled,\n                                       test = X_test_nonstudents_scaled,\n                                       cl = y_train_nonstudents, k = K_opt)\nct_preds &lt;- predict(ct, newdata = test, type = \"class\")\nct_pruned_preds &lt;- predict(ct_pruned, newdata = test, type = \"class\")\n\nlog_reg_perf &lt;- binary_class_metrics(log_reg_preds, test$default)\nknn_perf &lt;- binary_class_metrics(knn_preds, test$default)\nct_perf &lt;- binary_class_metrics(ct_preds, test$default)\nct_pruned_perf &lt;- binary_class_metrics(ct_pruned_preds, test$default)\nknitr::kable(cbind(data.frame(Classifier = c(\"Logistic regression\",\n                                             \"KNN\", \"CT Full\", \"CT Pruned\")),\n                   rbind(log_reg_perf, knn_perf, ct_perf, ct_pruned_perf)),\n             row.names = FALSE)\n\n\n\nClassifier\naccuracy\nrecall\nprecision\nf1\n\n\n\n\nLogistic regression\n0.963\n0.2439024\n0.6250000\n0.3508772\n\n\nKNN\n0.961\n0.1951220\n0.5714286\n0.2909091\n\n\nCT Full\n0.962\n0.2682927\n0.5789474\n0.3666667\n\n\nCT Pruned\n0.964\n0.3170732\n0.6190476\n0.4193548\n\n\n\n# Discussion of Costs and Model Preference:\n# Costs of False Positives:\n# A false positive means the model predicts a customer will default, but they will not.\n# Potential costs for the credit card company include:\n# - Annoying customers and damaging customer relationships by taking unnecessary actions\n#   like lowering credit limits or refusing transactions.\n# - Potential loss of business if customers switch to competitors due to these actions.\n# - Resources spent on investigating accounts flagged as high risk that are actually low risk.\n# Costs of False Negatives:\n# A false negative means the model predicts a customer will not default, but they actually will.\n# Potential costs for the credit card company include:\n# - Direct financial losses from unpaid balances.\n# - Increased costs associated with debt collection and recovery processes.\n# - Potential impact on overall profitability and risk management.\n\n# Model Preference:\n# The preferred model depends on the relative costs of false positives and false negatives.\n# Typically, in credit card default prediction, the cost of a false negative (losing money on a\n# defaulting customer) is much higher than the cost of a false positive (inconveniencing a\n# customer). Therefore, a model with higher recall (ability to correctly identify actual defaults)\n# might be preferred, even if it comes at the cost of lower precision (more false positives).\n\n# Looking at the performance metrics, it seems that the pruned classification tree with its\n# high recall and precision, is the most appropriate choice. Even if its precision was\n# lower than that of other models, the credit card company might prioritize minimizing\n# false negatives to avoid significant financial losses.\nIn the course materials, you should find the advertising data set discussed extensively in the lecture. Using 5-fold cross validation, we compared the performance of a linear regression and a KNN regression in class. A linear regression using all three advertising budgets as predictors had an estimated test MSE of 3.06, while a KNN regression with the same predictors and \\(K=2\\), even had an estimated test MSE of 1.94. The goal is now to additionally examine the performance of a regression tree on that data set.\n\nWe will start with quite an advanced exercise that manually determines the first split in the tree. This is meant to build deeper understanding of what goes on behind the scenes of rpart, but is not strictly necessary, so you can skip ahead to c. if you wish to. The goal here is to write a function that takes two arguments: a vector of training observations of a quantitative feature and a vector of training observations of a quantitative response. The function should return the optimal split point \\(s\\) and the resulting \\(RSS\\) for that variable. Hint: first determine all possible split points of the predictor. Then cycle through the split points, at each point determining the left \\(RSS\\), the right \\(RSS\\) and then the sum. Save the split points and total \\(RSS\\) in a matrix and return the row of the matrix for which the total \\(RSS\\) is minimized. You may want to write a simple helper function to compute \\(RSS\\).\n\n\nrss &lt;- function(y) sum((y - mean(y))^2)\n\noptimal_split &lt;- function(x, y){\n  split_points &lt;- sort(unique(x))[-1]\n  rss &lt;- numeric(length(split_points))\n  i &lt;- 1L\n  for(s in split_points){\n    rss_left &lt;- rss(y[x &lt; s])\n    rss_right &lt;- rss(y[x &gt;= s])\n    rss[i] &lt;- rss_left + rss_right\n    i &lt;- i + 1L\n  }\n  out &lt;- cbind(split_points, rss)\n  return(out[which.min(rss), ])\n}\n\n\nNow, import the advertising data set and select only the columns 3 to 6 (we do not need the rest). Use your function from a. to determine the optimal split for social_media, for radio and for newspaper, when using sales as a response variable. Which of those three predictors and split points is able to generate the lowest \\(RSS\\)? This will be our first split.\n\n\nadvertising &lt;- read.csv(\"../data/advertising.csv\")\nadvertising &lt;- advertising[, 3:6]\noptimal_split(advertising$social_media, advertising$sales)\n\nsplit_points          rss \n     123.100     2858.538 \n\noptimal_split(advertising$radio, advertising$sales)\n\nsplit_points          rss \n      26.800     3923.347 \n\noptimal_split(advertising$newspaper, advertising$sales)\n\nsplit_points          rss \n      51.200     4881.959 \n\n# Lowest RSS is achieved for variable social_media at split point 123.1.\n# This gives an RSS of 2858.538.\n\n\nIf you have not done it so far, import the advertising data set and select only the columns 3 to 6 (we do not need the rest). Employ the rpart package to fit a regression tree for sales, using the other three variables as predictors and setting the complexity parameter cp to zero for the fitting process. Visualize the resulting tree. If you did exercises a. and b., compare the first split (at the very top) to the one you determined in exercise b. Did you come to the same result as the package?\n\n\nrt &lt;- rpart(sales ~ ., data = advertising, control = list(cp = 0))\nrpart.plot(rt)\n\n\n\n\n\n\n\n# According to rpart, the first split happens on social_media at split\n# point 122, which is slightly different to the 123.1 that we determined.\n# However, the splits made are actually equivalent as there are no observations\n# between 122 and 123.1, so it is just that rpart has a slightly different\n# method for determining possible split points. The following proves that\n# the first splits are equivalent:\ntable(advertising$social_media &lt; 123.1, advertising$social_media &lt; 122)\n\n\n        FALSE TRUE\n  FALSE   117    0\n  TRUE      0   83\n\n\n\nUsing the fitted regression tree from c., determine the optimal complexity parameter cp and fit and visualize the optimally pruned regression tree for this data set.\n\n\n# Graphic analysis (run rsq.rpart(rt)) shows that a reasonable number of splits is 6.\n# The corresponding cp parameter is 0.018284, so we use that\n# to prune the tree:\nrt_pruned &lt;- prune(rt, cp = 0.018284)\nrpart.plot(rt_pruned)\n\n\n\n\n\n\n\n\n\nFinally, determine the test MSE of the optimally pruned tree in 5-fold cross validation. To do this, simply set the complexity parameter cp to the optimal value you found in d. already during fitting in each loop cycle of the cross validation. How does the performance of the optimally pruned regression tree compare to the linear regression and KNN model from the lecture?\n\n\nK &lt;- 5\nadvertising$fold &lt;- sample(rep(1:K, length.out = nrow(advertising)))\nmses &lt;- numeric(K)\n\nfor(k in 1:K){\n  m &lt;- rpart(sales ~ social_media + radio + newspaper, data = subset(advertising, fold != k),\n             control = list(cp = 0.018284))\n  preds &lt;- predict(m, newdata = subset(advertising, fold == k))\n  mses[k] &lt;- mean((preds - advertising$sales[advertising$fold == k])^2)\n}\nmean(mses)\n\n[1] 3.813512\n\n# With an MSE of 3.77, the performance of the optimally pruned regression tree seems\n# to be slightly worse than the linear regression model that uses the three\n# predictors and considerably worse than the corresponding KNN regression model.\n\nWe already know the penguins data set of the palmerpenguins package from the visualization part of this course. We will now try to build classification models for determining the species of penguin based on certain physical characteristics.\n\nLoad the data set and create a relative frequency table of the different penguin species in the entire data set. What is a crucial difference of this classification task compared to the previous one on defaults of credit card customers in exercise 2.?\n\n\nlibrary(palmerpenguins)\nprop.table(table(penguins$species))\n\n\n   Adelie Chinstrap    Gentoo \n0.4418605 0.1976744 0.3604651 \n\n# There are three species of penguins and therefore three classes. Unlike the default\n# example, which was a binary classification problem, the task of classifying the\n# penguins is therefore a multi-class classification problem.\n\n\nIn the usual way, perform a random 80-20 train-test split. Make sure to remove any NA’s in the data set before you do so.\n\n\npenguins &lt;- as.data.frame(na.omit(penguins))\nn &lt;- nrow(penguins)\ntrain_ind &lt;- sample(n, floor(0.8*n))\n\ntrain &lt;- penguins[train_ind, ]\ntest &lt;- penguins[-train_ind, ]\n\n\nOut of the classification methods that we have learned, the ones that can easily be applied to multi-class classification problems are KNN classification and decision trees. We will start with the former: run a 5-fold cross-validation experiment to determine the accuracy-maximizing value of \\(K\\) for two different KNN classifiers: one that only uses predictors bill_length_mm and bill_depth_mm and one that additionally uses predictors flipper_length_mm and body_mass_g to predict species. Make sure you scale the predictors in every iteration over the 5 folds. Hint: it might be helpful to write a little helper function that – given a vector of class predictions and a vector of true classes – computes the accuracy, i.e. the proportion of cases, where predicted class is equal to true class.\n\n\nK &lt;- 5\ntrain$fold &lt;- sample(rep(1:K, length.out = nrow(train)))\nn_neighbours &lt;- 2:20\n\naccuracy &lt;- function(y_pred, y_true) mean(y_pred == y_true)\n\nfor(nn in n_neighbours){\n  knn_colname &lt;- sprintf(\"class_preds_KNN%d\", nn)\n  for(k in 1:K){\n    X_train_scaled &lt;- scale(subset(train, fold != k, 3:6))\n    y_train &lt;- train$species[train$fold != k]\n    X_test_scaled &lt;- scale(subset(train, fold == k, 3:6))\n\n    preds_knn_small &lt;- knn(train = X_train_scaled[, 1:2],\n                           test = X_test_scaled[, 1:2],\n                           cl = y_train, k = nn)\n    preds_knn_big &lt;- knn(train = X_train_scaled,\n                         test = X_test_scaled,\n                         cl = y_train, k = nn)\n\n    train[train$fold == k, paste0(knn_colname, \"SMALL\")] &lt;- preds_knn_small\n    train[train$fold == k, paste0(knn_colname, \"BIG\")] &lt;- preds_knn_big\n  }\n}\n\n# Compute predictive accuracies of the KNN models\nresults &lt;- data.frame(model = names(train)[10:47],\n                      accuracy = 0)\nfor(i in 1:nrow(results)){\n  results$accuracy[i] &lt;- accuracy(train[[results$model[i]]], train$species)\n}\nresults[which.max(results$accuracy), ]\n\n                 model  accuracy\n10 class_preds_KNN6BIG 0.9924812\n\n# The big KNN (using bill_length_mm, bill_depth_mm, flipper_length_mm and body_mass_g \n# as predictors) seems to consistently outperform the model using just the first two\n# predictors. The best big model has K = 6.\n\n\nNow, we will look at a classification tree for this data set. Fit a full classification tree for species based on predictors bill_length_mm, bill_depth_mm, flipper_length_mm and body_mass_g to the training data in the same way you would normally do for a binary classification problem. Determine the optimal complexity parameter cp and use it to also fit an optimally pruned tree. Visualize the outcome. What do you notice about the visualized tree compared to the binary classification case?\n\n\nct &lt;- rpart(species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g,\n            data = train, control = list(cp = 0))\n# Graphic analysis (run rsq.rpart(ct)) shows that a reasonable number of splits is 2.\n# The corresponding cp parameter is 0.045752, so we use that\n# to prune the tree:\nct_pruned &lt;- prune(ct, 0.045752)\nrpart.plot(ct_pruned)\n\n\n\n\n\n\n\n# Now, there are three colours in the plot, one for each class.\n\n\nUse the best-performing KNN model from c. as well as both the full and the optimally pruned tree from d. to predict the species of the penguins in the hold-out test data you created in b. Which of the three models gives the highest test accuracy in predicting the penguin species?\n\n\nX_train_scaled &lt;- scale(train[, 3:6])\ny_train &lt;- train$species\nX_test_scaled &lt;- scale(test[, 3:6])\n\npreds_knn &lt;- knn(train = X_train_scaled,\n                 test = X_test_scaled,\n                 cl = y_train, k = 6)\npreds_ct &lt;- predict(ct, newdata = test, type = \"class\")\npreds_ct_pruned &lt;- predict(ct_pruned, newdata = test, type = \"class\")\n\n# Features for all 3 methods:\n# bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g\n\n# Accuracy of KNN with K = 6:\naccuracy(preds_knn, test$species)\n\n[1] 0.9552239\n\n# Accuracy of full CT:\naccuracy(preds_ct, test$species)\n\n[1] 0.9104478\n\n# Accuracy of pruned CT:\naccuracy(preds_ct_pruned, test$species)\n\n[1] 0.9104478\n\n# With an accuracy of approx. 95.5%, the KNN model is clearly the best-performing.\n\nThe Seeds data set (obtainable from here) contains measurements of geometrical properties of kernels belonging to three different varieties of wheat: Kama, Rosa, and Canadian. Although the target class (variety) is known, we will approach this task as an unsupervised learning problem.\n\nDownload the data set from the given link to the UCI Machine Learning Repository. Note that the data set does not include any column names, so we have to set them manually. From left to right, the columns contain the following measurements: area \\(A\\), perimeter \\(P\\), compactness \\(C = 4\\pi \\cdot A/P^2\\), length of kernel, width of kernel, asymmetry coefficient, length of kernel groove and wheat variety (1 for Kama, 2 for Rosa or 3 for Canadian). Name the columns of the data.frame accordingly and transform the wheat variety column into a factor with the labels argument set appropriately.\n\n\nseeds &lt;- read.table(\"../data/seeds_dataset.txt\")\nnames(seeds) &lt;- c(\"area\", \"perimeter\", \"compactness\", \"kernel_length\",\n                  \"kernel_width\", \"asymmetry\", \"groove_length\", \"variety\")\nseeds$variety &lt;- factor(seeds$variety, labels = c(\"Kama\", \"Rosa\", \"Canadian\"))\n\n\nVisualize the following bivariate relationships in a scatter plot: perimeter vs. groove_length, kernel_width vs. groove_length and kernel_length vs. groove_length. Do you see any clearly identifiable groups emerging from these bivariate plots?\n\n\nggplot(seeds, aes(x = perimeter, y = groove_length)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(seeds, aes(x = kernel_width, y = groove_length)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(seeds, aes(x = kernel_length, y = groove_length)) +\n  geom_point()\n\n\n\n\n\n\n\n# Usually at least two distinct regions identifiable, however we will be clustering\n# on all variables, so even if no distinct groups are identifiable in two dimensions\n# there might be in higher dimensions.\n\n\nCreate a new data.frame called seeds_scaled, which contains all variables except for variety and all of them in their scaled version, i.e. with their mean removed and divided by their standard deviation.\n\n\nseeds_scaled &lt;- as.data.frame(scale(seeds[, 1:7]))\n\n\nOn the entire seeds_scaled data set, run K-means clustering with different values of \\(k\\) (e.g. from 1 to 10). In each run, extract the within-cluster sum of squares and store the results in a vector named withinss. Hint: the within-cluster sum of squares can be obtained under the name tot.withinss from the fitted kmeans object.\n\n\nk &lt;- 1:10\nwithinss &lt;- sapply(k, function(k) kmeans(seeds_scaled, centers = k)$tot.withinss)\n\n\nPlot the number of clusters \\(k\\) (on the x-axis) against the values of withinss (on the y-axis). Where is the “elbow” in that plot and which value of \\(k\\) do you deem most appropriate as a consequence?\n\n\ndf_withinss &lt;- data.frame(k = k, withinss = withinss)\nggplot(df_withinss, aes(x = k, y = withinss)) +\n  geom_line() + \n  geom_point() +\n  scale_x_continuous(breaks = k)\n\n\n\n\n\n\n\n# Now, there is obviously some subjective judgement in this, but there is a good argument\n# to be made that the elbow in that plot occurs for k = 3, which is therefore the value\n# we deem most appropriate for the number of clusters. Of course, this is cheating slightly\n# as we know that there are three wheat variants in the data. This type of class information\n# is not always available when performing clustering.\n\n\nRun K-means clustering again, this time with the optimal \\(k\\) you chose in e. Add the cluster assignment vector (as a factor) to your original data.frame and highlight the identified clusters by colour in the bivariate scatter plots from b. Do they match with your expectations regarding the groups you visually identified in b.?\n\n\nkm_seeds &lt;- kmeans(seeds_scaled, centers = 3)\nseeds$clust &lt;- factor(km_seeds$cluster)\n\nggplot(seeds, aes(x = perimeter, y = groove_length, color = clust)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(seeds, aes(x = kernel_width, y = groove_length, color = clust)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(seeds, aes(x = kernel_length, y = groove_length, color = clust)) +\n  geom_point()\n\n\n\n\n\n\n\n# Clustering appears generally sensible based on the visual impression from b.\n# The separation of obvious, visually identifiable clusters was successful and\n# even less obvious cluster assignments appear reasonable.\n\n\nFinally, for each cluster, determine which wheat variety appears in it most frequently and label the cluster according to that variety. Then create a cross-tabulation of that label with the actual wheat varieties. How well was the K-means algorithm able to identify wheat varieties without having any information about these classes?\n\n\nbest_labels &lt;- sapply(levels(seeds$clust),\n                      function(x) names(which.max(table(seeds$variety[seeds$clust == x]))))\n# Note: the subsequent ordering of the levels is not strictly necessary,\n# it is only done here to have the table appear in the nice confusion matrix type of way.\nseeds$clust &lt;- factor(seeds$clust,\n                      levels = match(levels(seeds$variety), best_labels),\n                      labels = levels(seeds$variety))\ntable(seeds$clust, seeds$variety)\n\n\n           Kama Rosa Canadian\n  Kama       62    5        4\n  Rosa        2   65        0\n  Canadian    6    0       66\n\n# Clearly, the clusters are remarkably congruent with the wheat varieties even though\n# the fitting of the clusters happened in a fully unsupervised fashion, i.e. there\n# was no information on wheat variety used whatsoever.",
    "crumbs": [
      "Exercise Solutions",
      "Solutions IV"
    ]
  },
  {
    "objectID": "exercises/exercise-solutions-iii.html",
    "href": "exercises/exercise-solutions-iii.html",
    "title": "Exercises III",
    "section": "",
    "text": "The Data Science Workflow – Visualize\n\nInvestigating the relationship between age and number of children based on the gss_sm data set:\n\nThe socviz package contains a data set called gss_sm, which is a small subset of the questions from the 2016 General Social Survey (GSS). The GSS is a long-running survey of American adults that asks about a range of topics of interest to social scientists. To access this data set, start by installing and loading the socviz package.\nStart by creating a simple scatter plot of age (on the x axis) against childs (on the y axis), i.e. illustrate how the number of children varies with age.\nThe plot from task b. suffers from overplotting, i.e. many data points are in the same position and overlap. To avoid this, see what happens when you set the position argument of the point geom to \"jitter\".\nNow, add a smooth line describing the relationship between age and number of children into the plot.\nFacet the plot by sex (rows) and race (columns).\nChange the theme of the plot to theme_minimal. Also, add axis tick marks on the x axis every 10 years. Finally, choose your favourite colour from here and use its name to colour the points in the plot. Also set their size to 0.25 to make them smaller and further reduce the overplotting issue. Finally, label the axes with Age and Number of Children, respectively.\n\n\nlibrary(ggplot2)\n\n# a.\n# Install only if it has not been installed already:\nif(!require(\"socviz\")) install.packages(\"socviz\") \nlibrary(socviz)\n\n# b.\nggplot(gss_sm, aes(x = age, y = childs)) +\n  geom_point()\n\n\n\n\n\n\n\n# c.\nggplot(gss_sm, aes(x = age, y = childs)) +\n  geom_point(position = \"jitter\")\n\n\n\n\n\n\n\n# d.\nggplot(gss_sm, aes(x = age, y = childs)) +\n  geom_point(position = \"jitter\") + \n  geom_smooth()\n\n\n\n\n\n\n\n# e.\nggplot(gss_sm, aes(x = age, y = childs)) +\n  geom_point(position = \"jitter\") + \n  geom_smooth() + \n  facet_grid(sex ~ race)\n\n\n\n\n\n\n\n# f.\nggplot(gss_sm, aes(x = age, y = childs)) +\n  geom_point(color = \"deeppink4\", position = \"jitter\", size = 0.25) + \n  geom_smooth() + \n  facet_grid(sex ~ race) + \n  scale_x_continuous(breaks = seq(20, 80, by = 10)) + \n  labs(x = \"Age\", y = \"Number of Children\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nAnalyzing the relationship between education and political views based on the gss_sm data set:\n\nCreate a frequency table of the degree variable and save it as a variable called degree_freqs. Turn it into a data.frame with column names degree and freq.\nUse the degree_freqs object to create a bar plot indicating the absolute frequencies of the different levels of education in the data set.\nReorder the degrees in the bar plot to reflect the order of their absolute frequencies from highest to lowest. Also, relabel the axes to Education Level and Frequency, respectively.\nNow, create a contingency table of the variables degree and polviews and save it as a variable called degrees_polview. Turn it into a data.frame with column names degree, polviews and freq.\nUsing degrees_polview, create a stacked bar plot of degree, where the fill aesthetic is used to indicate polviews. Afterwards, also create a bar plot of polviews, where the fill aesthetic is used to indicate degree.\nTurn both bar plots from task e. into ones that show conditional relative frequencies instead of absolute frequencies to show the relationship between education and political views.\nFor the first of the two bar plots, add the title “Education and political views”, a subtitle “Data from the 2016 General Social Survey”, a caption “Source: socviz package”, appropriate axes labels and a header for the legend saying “Political views”. Moreover, choose a suitable color palette from the colorbrewer set and apply it to the graph. Finally, use theme_economist_white from the ggthemes package.\nFinally, use the code you have already written to also have a look at the relationship between religion and polviews. For the most part, this should just involve some copying and pasting as well as adapting certain labels accordingly. Note that you have to create a new contingency table, of course.\n\n\n# a.\ndegree_freqs &lt;- table(gss_sm$degree)\ndegree_freqs &lt;- as.data.frame(degree_freqs)\nnames(degree_freqs) &lt;- c(\"degree\", \"freq\")\n\n# b.\nggplot(degree_freqs, aes(x = degree, y = freq)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n# c.\nggplot(degree_freqs,\n       aes(x = reorder(degree, freq, decreasing = TRUE),\n           y = freq)) + \n  labs(x = \"Education Level\", y = \"Frequency\") + \n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n# d.\ndegrees_polview &lt;- table(gss_sm$degree, gss_sm$polviews)\ndegrees_polview &lt;- as.data.frame(degrees_polview)\nnames(degrees_polview) &lt;- c(\"degree\", \"polviews\", \"freq\")\n\n# e.\nggplot(degrees_polview, aes(x = degree, y = freq, fill = polviews)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\nggplot(degrees_polview, aes(x = polviews, y = freq, fill = degree)) + \n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n# f.\nggplot(degrees_polview, aes(x = degree, y = freq, fill = polviews)) + \n  geom_bar(stat = \"identity\", position = \"fill\")\n\n\n\n\n\n\n\nggplot(degrees_polview, aes(x = polviews, y = freq, fill = degree)) + \n  geom_bar(stat = \"identity\", position = \"fill\")\n\n\n\n\n\n\n\n# g.\nlibrary(ggthemes)\nggplot(degrees_polview, aes(x = degree, y = freq, fill = polviews)) + \n  geom_bar(stat = \"identity\", position = \"fill\") + \n  labs(x = \"Degree\", y = \"Relative frequency\",\n       title = \"Education and political views\",\n       subtitle = \"Data from the 2016 General Social Survey\",\n       caption = \"Source: socviz package\",\n       fill = \"Political Views\") + \n  scale_fill_brewer(palette = \"RdBu\", direction = -1) + \n  theme_economist_white()\n\n\n\n\n\n\n\n# h.\nreligion_polview &lt;- table(gss_sm$religion, gss_sm$polviews)\nreligion_polview &lt;- as.data.frame(religion_polview)\nnames(religion_polview) &lt;- c(\"religion\", \"polviews\", \"freq\")\n\nggplot(religion_polview, aes(x = religion, y = freq, fill = polviews)) + \n  geom_bar(stat = \"identity\", position = \"fill\") + \n  labs(x = \"Religion\", y = \"Relative frequency\",\n       title = \"Religion and political views\",\n       subtitle = \"Data from the 2016 General Social Survey\",\n       caption = \"Source: socviz package\",\n       fill = \"Political Views\") + \n  scale_fill_brewer(palette = \"RdBu\", direction = -1) + \n  theme_economist_white()\n\n\n\n\n\n\n\n\nAnalyzing the relationship between economic well-being and life expectancy for countries in the world in 2020 based on the gapminder data set:\n\nIn the course materials, you should find the data sets called gm_gdp.csv, gm_lex.csv, gm_pop.csv and gm_geo.csv. These data sets all stem from the Gapminder Project, a Swedish non-profit venture that promotes understanding of information about social, economic, and environmental development through the use of data visualization and statistics. Start by loading all these four data sets into R under the names of gdp, lex, pop and geo, respectively. Inspect these four data sets and familiarize yourself with their structure. Refer to gm_description.csv for additional information if you need it. Rename the following columns:\n\nthe fourth column of gdp to gdp_percap.\nthe third column of lex to time and the fourth column to life_expectancy.\nthe fourth column of pop to population.\n\nPerform the following joins in the given order:\n\nan inner join between columns 1, 3 and 4 of both gdp and lex on variables geo and time. Call the resulting object gm_df.\nan inner join between gm_df and columns 1, 3 and 4 of pop on variables geo and time. Redefine gm_df as the resulting object.\nan inner join between gm_df and columns 1, 2 and 3 of geo on variable geo. Redefine gm_df as the resulting object.\n\nVerify that gm_df now has columns geo, time, gdp_percap, life_expectancy, population, name and four_regions.\nWe will start by illustrating the relationship between GDP per capita and life expectancy at birth in a single year. For this, create a data.frame called gm_df2020 that only includes data points from the year 2020. Using gm_df2020, then create a simple scatter plot of gdp_percap on the x axis and life_expectancy on the y axis.\nIn the plot, you can see a point at life_expectancy 0. This is because in the year 2020, we do not have life expectancy information for Hong Kong and Liechtenstein. The corresponding values are set to 0, which is not sensible. To avoid this being visible, we can simply restrict the plot range of the y axis. Set it, using scale_y_continuous, to the range from 40 to 95.\nAnother problem of this plot is that the GDP per capita spans a very wide range of values. To mitigate this for visualization, apply log2-scaling to the x axis. Hint: check the documentation for possible values to pass to the transform argument of the scale_x_continuous function.\nTo match the log2-scaling of the x axis, set the tick marks at powers of 2 starting from 256 and ending at 131,072.\nNow, we want the size of the points to represent the sizes of the country populations. Add the corresponding aesthetic for population to your plot. Control the minimal and maximal sizes of the points via the range argument of scale_size_continuous, the minimum should be 0.5 and the maximum should be 20.\nThe big size of some points now means that many points are not visible due to overlap. To mitigate this, increase the transparency of the points by setting alpha to 0.4.\nNext, it would be nice to represent the four world regions (Africa, Americas, Asia and Europe) by colour. Add the corresponding aesthetic for the variable four_regions and apply a suitable colorbrewer palette of your choosing.\nAdd the label GDP per capita (log scale) to the x axis and the label Life expectancy at birth to the y axis. Furthermore, add a title saying “Economic well-being and life expectancy”, a subtitle saying “Data from 2020” and a caption saying “Source: gapminder” to the plot. Rename the header of the legend for the world regions to World Region.\nFinally, remove the guide for the size aesthetic and change the theme to theme_test.\n\n\n# a.\ngdp &lt;- read.csv(\"../data/gm_gdp.csv\")\nlex &lt;- read.csv(\"../data/gm_lex.csv\")\npop &lt;- read.csv(\"../data/gm_pop.csv\")\ngeo &lt;- read.csv(\"../data/gm_geo.csv\")\n\nnames(gdp)[4] &lt;- \"gdp_percap\"\nnames(lex)[3:4] &lt;- c(\"time\", \"life_expectancy\")\nnames(pop)[4] &lt;- \"population\"\n\n# b.\ngm_df &lt;- merge(gdp[, c(1, 3, 4)],\n            lex[, c(1, 3, 4)],\n            by = c(\"geo\", \"time\"))\ngm_df &lt;- merge(gm_df,\n               pop[, c(1, 3, 4)],\n               by = c(\"geo\", \"time\"))\ngm_df &lt;- merge(gm_df,\n               geo[, 1:3],\n               by = \"geo\")\n\n# c.\nall(names(gm_df) == c(\"geo\", \"time\", \"gdp_percap\", \"life_expectancy\",\n                      \"population\", \"name\", \"four_regions\"))\n\n[1] TRUE\n\n# d.\ngm_df2020 &lt;- gm_df[gm_df$time == 2020, ]\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy)) +\n  geom_point()\n\n\n\n\n\n\n\n# e.\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy)) +\n  geom_point() + \n  scale_y_continuous(limits = c(40, 95))\n\n\n\n\n\n\n\n# f.\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy)) +\n  geom_point() + \n  scale_x_continuous(transform = \"log2\") + \n  scale_y_continuous(limits = c(40, 95))\n\n\n\n\n\n\n\n# g.\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy)) +\n  geom_point() + \n  scale_x_continuous(breaks = 2^(8:17), transform = \"log2\") + \n  scale_y_continuous(limits = c(40, 95))\n\n\n\n\n\n\n\n# h.\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy, size = population)) +\n  geom_point() + \n  scale_x_continuous(breaks = 2^(8:17), transform = \"log2\") + \n  scale_y_continuous(limits = c(40, 95)) + \n  scale_size_continuous(range = c(0.5, 20))\n\n\n\n\n\n\n\n# i.\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy, size = population)) +\n  geom_point(alpha = 0.4) + \n  scale_x_continuous(breaks = 2^(8:17), transform = \"log2\") + \n  scale_y_continuous(limits = c(40, 95)) + \n  scale_size_continuous(range = c(0.5, 20))\n\n\n\n\n\n\n\n# j.\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy,\n                      size = population, color = four_regions)) +\n  geom_point(alpha = 0.4) + \n  scale_x_continuous(breaks = 2^(8:17), transform = \"log2\") + \n  scale_y_continuous(limits = c(40, 95)) + \n  scale_size_continuous(range = c(0.5, 20)) + \n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n# k.\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy,\n                      size = population, color = four_regions)) +\n  geom_point(alpha = 0.4) + \n  scale_x_continuous(breaks = 2^(8:17), transform = \"log2\") + \n  scale_y_continuous(limits = c(40, 95)) + \n  scale_size_continuous(range = c(0.5, 20)) + \n  scale_color_brewer(palette = \"Dark2\") + \n  labs(x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy at birth\",\n       title = \"Economic well-being and life expectancy\",\n       subtitle = \"Data from 2020\",\n       caption = \"Source: gapminder\",\n       color = \"World Region\")\n\n\n\n\n\n\n\n# l.\nggplot(gm_df2020, aes(x = gdp_percap, y = life_expectancy,\n                      size = population, color = four_regions)) +\n  geom_point(alpha = 0.4) + \n  scale_x_continuous(breaks = 2^(8:17), transform = \"log2\") + \n  scale_y_continuous(limits = c(40, 95)) + \n  scale_size_continuous(range = c(0.5, 20)) + \n  scale_color_brewer(palette = \"Dark2\") + \n  labs(x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy at birth\",\n       title = \"Economic well-being and life expectancy\",\n       subtitle = \"Data from 2020\",\n       caption = \"Source: gapminder\",\n       color = \"World Region\") + \n  guides(size = \"none\") + \n  theme_test()\n\n\n\n\n\n\n\n\nAnalyzing the relationship between child mortality and total fertility rate over time based on the gapminder data set:\n\nWe will now merge additional data into the data.frame called gm_df created in exercise 3. First load the data sets gm_tfr.csv and gm_cm.csv into R under the names of tfr and cm. The first contains data on the total fertility rate (i.e. number of babies per woman) and the second contains data on child mortality (i.e. number of 0-5 year-olds dying per 1000 born children). After import, rename their fourth columns to tfr and cm, respectively.\nPerform the following joins in the given order:\n\nan inner join between gm_df and columns 1, 3 and 4 of tfr on variables geo and time. Redefine gm_df as the resulting object.\nan inner join between gm_df and columns 1, 3 and 4 of cm on variables geo and time. Redefine gm_df as the resulting object.\n\nVerify that the variables tfr and cm are now available in the data.frame called gm_df.\nWe want to look at the relationship between child mortality and total fertility rate in six years, namely 1800, 1900, 1930, 1950, 1980 and 2010. Restrict gm_df to only include data from these six years.\nCreate a scatter plot with cm on the x axis and tfr on the y axis. Facet the plot by time into a 2 x 3 grid using facet_wrap, such that each subplot represents one of the six years we have pre-selected.\nNow, map four_regions and population to the color and size aesthetics, respectively, exactly in the same way as in exercise 3. To prevent too large points dominating the subplots, the only difference should be setting the maximum point size to 10.\nThis time, we want to be able to read off at least some of the individual countries from the plot. To do this, load the ggrepel package, map the name variable to the label aesthetic and use geom_text_repel with arguments size = 2 and max_overlaps = 15 to add the labels.\nLabel the plot appropriately and adapt the overall plot design to conform with the appearance of the final plot from exercise 3.\n\n\n# a.\ntfr &lt;- read.csv(\"../data/gm_tfr.csv\")\ncm &lt;- read.csv(\"../data/gm_cm.csv\")\nnames(tfr)[4] &lt;- \"tfr\"\nnames(cm)[4] &lt;- \"cm\"\n\n# b.\ngm_df &lt;- merge(gm_df,\n               tfr[, c(1, 3, 4)],\n               by = c(\"geo\", \"time\"))\ngm_df &lt;- merge(gm_df,\n               cm[, c(1, 3, 4)],\n               by = c(\"geo\", \"time\"))\n\n# c.\nall(names(gm_df) == c(\"geo\", \"time\", \"gdp_percap\", \"life_expectancy\",\n                      \"population\", \"name\", \"four_regions\", \"tfr\", \"cm\"))\n\n[1] TRUE\n\n# d.\ngm_df &lt;- subset(gm_df, time %in% c(1800, 1900, 1930, 1950, 1980, 2010))\n\n# e.\nggplot(gm_df, aes(x = cm, y = tfr)) +\n  geom_point() + \n  facet_wrap(~ time)\n\n\n\n\n\n\n\n# f.\nggplot(gm_df, aes(x = cm, y = tfr,\n                  size = population, color = four_regions)) +\n  geom_point(alpha = 0.4) + \n  scale_size_continuous(range = c(0.5, 10)) + \n  scale_color_brewer(palette = \"Dark2\") + \n  facet_wrap(~ time)\n\n\n\n\n\n\n\n# g.\nlibrary(ggrepel)\nggplot(gm_df, aes(x = cm, y = tfr, label = name,\n                  size = population, color = four_regions)) +\n  geom_point(alpha = 0.4) + \n  geom_text_repel(size = 2, max.overlaps = 15) + \n  scale_size_continuous(range = c(0.5, 10)) + \n  scale_color_brewer(palette = \"Dark2\") + \n  facet_wrap(~ time)\n\n\n\n\n\n\n\n# h.\nggplot(gm_df, aes(x = cm, y = tfr, label = name,\n                  size = population, color = four_regions)) +\n  geom_point(alpha = 0.4) + \n  geom_text_repel(size = 2, max.overlaps = 15) + \n  scale_size_continuous(range = c(0.5, 10)) + \n  scale_color_brewer(palette = \"Dark2\") + \n  facet_wrap(~ time) + \n  labs(x = \"Child mortatility in deaths of 0-5 year-olds per 1000 births\",\n       y = \"Total fertility rate (# babies / woman)\",\n       title = \"Child mortality and total fertility rate over time\",\n       subtitle = \"Data in the years 1800, 1900, 1930, 1950, 1980 and 2010\",\n       caption = \"Source: gapminder\") + \n  guides(size=\"none\") +\n  theme_test()\n\n\n\n\n\n\n\n\nAnalyzing the relationship between chemical properties of wine and its perceived quality:\n\nIn exercise set II, you downloaded and imported the Wine Quality data sets from the UCI Machine Learning Repository. Import both of these data sets under the names white_wines and red_wines again. To both data.frames, add a column called type, which is white for all wines in the first one and red for all wines in the second one. Then, using the function rbind, combine the two data frames into one called wines. Finally, turn the variables quality and type into a factor.\nFirst, we want to investigate the relationship between alcohol percentage of the wines and their sensory quality. Create parallel boxplots of the alcohol variable for all observed levels of the quality variable. Use the fill aesthetic to represent the two different types of wine.\nUse coord_flip to have quality on the y axis and alcohol on the x axis.\nLet’s represent the wines in appropriate colours. Use scale_fill_manual to manually adapt the colours of the fill aesthetic. One way to specify colours in R is with the help of so-called hex codes, use “#7B0323” for red wines and “#F1EAA3” for white wines. To make these colours visible, choose a theme with a white background, such as theme_test. Finally, label your plot appropriately.\nCreate further similar plots relating the wine quality to different chemical properties. In particular:\n\nvolatile.acidity. Try using a jitter geom instead of the boxplot geom. Experiment with the point size to reduce the overplotting issue.\nsulphates. Try using a violin geom this time.\n\n\n\n# a.\nwhite_wine_path &lt;- file.path(\"../data\", \"winequality-white.csv\")\nred_wine_path &lt;- file.path(\"../data\", \"winequality-red.csv\")\nwhite_wines &lt;- read.csv2(white_wine_path, dec = \".\")\nred_wines &lt;- read.csv2(red_wine_path, dec = \".\")\nwhite_wines$type &lt;- \"white\"\nred_wines$type &lt;- \"red\"\nwines &lt;- rbind(white_wines, red_wines)\nwines$quality &lt;- factor(wines$quality)\nwines$type &lt;- factor(wines$type)\n\n# b.\nggplot(wines, aes(x = quality, y = alcohol, fill = type)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# c.\nggplot(wines, aes(x = quality, y = alcohol, fill = type)) +\n  geom_boxplot() + \n  coord_flip()\n\n\n\n\n\n\n\n# d.\nwine_colors &lt;- c(\"#7B0323\", \"#F1EAA3\")\nggplot(wines, aes(x = quality, y = alcohol, fill = type)) +\n  geom_boxplot() + \n  scale_fill_manual(values = wine_colors) + \n  labs(x = \"Quality score\", y = \"Alcohol (% vol.)\",\n       title = \"Relationship between alcohol percentage and wine quality\",\n       subtitle = \"Red and white vinho verde wine samples from the north of Portugal\",\n       caption = \"Source: Wine Quality dataset UCI ML Repo\",\n       fill = \"Type\") + \n  coord_flip() + \n  theme_test()\n\n\n\n\n\n\n\n# e.\nggplot(wines, aes(x = quality, y = volatile.acidity, color = type)) +\n  geom_jitter(size = 0.25) + \n  scale_color_manual(values = wine_colors) + \n  labs(x = \"Quality score\", y = \"Volatile acidity\",\n       title = \"Relationship between volatile acidity and wine quality\",\n       subtitle = \"Red and white vinho verde wine samples from the north of Portugal\",\n       caption = \"Source: Wine Quality dataset UCI ML Repo\",\n       color = \"Type\") + \n  coord_flip() + \n  theme_test()\n\n\n\n\n\n\n\nggplot(wines, aes(x = quality, y = sulphates, fill = type)) +\n  geom_violin() + \n  scale_fill_manual(values = wine_colors) + \n  labs(x = \"Quality score\", y = \"Sulphates\",\n       title = \"Relationship between sulphates and wine quality\",\n       subtitle = \"Red and white vinho verde wine samples from the north of Portugal\",\n       caption = \"Source: Wine Quality dataset UCI ML Repo\",\n       fill = \"Type\") + \n  coord_flip() + \n  theme_test()\n\n\n\n\n\n\n\n\nAnalyzing flipped counties by demographics in the 2016 US presidential election:\n\nIn the socviz package, there is a data set called county_data, which contains county-level data from the 2016 US presidential election. In this data set, the variable flipped indicates, whether a given county flipped to Republican or Democrat compared to the 2012 election. Start by creating a simple scatter plot of county population (pop) on the x axis and the percentage of black population (black/100) on the y axis, but only for the counties that did not flip in the 2016 election. Use a log10 scale on the x axis, color the points in grey and increase their transparency by setting alpha = 0.25.\nNow, add points for the counties that did flip in the 2016 election. You do this simply by adding another point geom, but with its own data set and aesthetic mapping. Map the variable partywinner16 (indicating the party to which the county flipped in the election) to the color aesthetic and use the typical Republican and Democrat party colours (which is “#CB454A” for Republican and “#2E74C0” for Democrat) as the colours.\nFor nicely labeled tick marks, we can use the scales package. Having installed the package, load it and pass the function comma as the labels argument to scale_x_log10 and the function percent as the labels argument to scale_y_continuous. Notice how that changes the labels for the tick marks in the plot.\nAdd the following labels your plot:\n\nx: County population (log scale)\ny: Percent black population\ntitle: Flipped counties, 2016\nsubtitle: Counties in grey did not flip.\ncaption: Source: socviz package\ncolor: County flipped to…\n\nUse a text_repel geom to label all counties that flipped and have a black population of more than 25% with a label indicating the state they belong to. Set the size of the text to 2. Finally, set the theme to theme_economist from the ggthemes package.\n\n\n# a.\nggplot(subset(county_data, flipped == \"No\"), aes(x = pop, y = black/100)) + \n  geom_point(alpha = 0.25, color = \"grey\") + \n  scale_x_log10()\n\n\n\n\n\n\n\n# b.\nparty_colors &lt;- c(\"#2E74C0\", \"#CB454A\")\nggplot(subset(county_data, flipped == \"No\"), aes(x = pop, y = black/100)) + \n  geom_point(alpha = 0.25, color = \"grey\") + \n  geom_point(data = subset(county_data, flipped == \"Yes\"),\n             mapping = aes(x = pop, y = black/100, color = partywinner16)) + \n  scale_color_manual(values = party_colors) + \n  scale_x_log10()\n\n\n\n\n\n\n\n# c.\nlibrary(scales)\nggplot(subset(county_data, flipped == \"No\"), aes(x = pop, y = black/100)) + \n  geom_point(alpha = 0.25, color = \"grey\") + \n  geom_point(data = subset(county_data, flipped == \"Yes\"),\n             mapping = aes(x = pop, y = black/100, color = partywinner16)) + \n  scale_color_manual(values = party_colors) + \n  scale_x_log10(labels = comma) + \n  scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\n# d.\nggplot(subset(county_data, flipped == \"No\"), aes(x = pop, y = black/100)) + \n  geom_point(alpha = 0.25, color = \"grey\") + \n  geom_point(data = subset(county_data, flipped == \"Yes\"),\n             mapping = aes(x = pop, y = black/100, color = partywinner16)) + \n  labs(x = \"County Population (log scale)\",\n       y = \"Percent Black Population\",\n       title = \"Flipped counties, 2016\",\n       subtitle = \"Counties in grey did not flip.\",\n       caption = \"Source: socviz package\",\n       color = \"County flipped to ... \") + \n  scale_color_manual(values = party_colors) + \n  scale_x_log10(labels = comma) + \n  scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\n# e.\nggplot(subset(county_data, flipped == \"No\"), aes(x = pop, y = black/100)) + \n  geom_point(alpha = 0.25, color = \"grey\") + \n  geom_point(data = subset(county_data, flipped == \"Yes\"),\n             mapping = aes(x = pop, y = black/100, color = partywinner16)) + \n  geom_text_repel(data = subset(county_data, flipped == \"Yes\" & black &gt; 25),\n                  mapping = aes(x = pop, y = black/100, label = state),\n                  size = 2) + \n  labs(x = \"County Population (log scale)\",\n       y = \"Percent Black Population\",\n       title = \"Flipped counties, 2016\",\n       subtitle = \"Counties in grey did not flip.\",\n       caption = \"Source: socviz package\",\n       color = \"County flipped to... \") + \n  scale_color_manual(values = party_colors) + \n  scale_x_log10(labels = comma) + \n  scale_y_continuous(labels = percent) + \n  theme_economist()",
    "crumbs": [
      "Exercise Solutions",
      "Solutions III"
    ]
  }
]