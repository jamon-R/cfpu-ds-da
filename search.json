[
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-a-sexy-profession-in-2012",
    "href": "slides/01_Introduction_to_DS.html#data-science-a-sexy-profession-in-2012",
    "title": "Data Science and Data Analytics",
    "section": "Data Science – A sexy profession in 2012?",
    "text": "Data Science – A sexy profession in 2012?\n\n\nSource: Harvard Business Review in October 2012"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-a-sexy-profession-today",
    "href": "slides/01_Introduction_to_DS.html#data-science-a-sexy-profession-today",
    "title": "Data Science and Data Analytics",
    "section": "Data Science – A sexy profession today?",
    "text": "Data Science – A sexy profession today?\n\n\nSource: Harvard Business Review in July 2022"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#what-is-data-science-1",
    "href": "slides/01_Introduction_to_DS.html#what-is-data-science-1",
    "title": "Data Science and Data Analytics",
    "section": "What is Data Science?",
    "text": "What is Data Science?"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#what-is-data-science-2",
    "href": "slides/01_Introduction_to_DS.html#what-is-data-science-2",
    "title": "Data Science and Data Analytics",
    "section": "What is Data Science?",
    "text": "What is Data Science?\nData science is an interdisciplinary academic field that combines statistics, mathematics and computing with specific subject matter expertise to uncover actionable insights hidden in an organization’s data."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#a-brief-history-of-data-science",
    "href": "slides/01_Introduction_to_DS.html#a-brief-history-of-data-science",
    "title": "Data Science and Data Analytics",
    "section": "A brief history of Data Science",
    "text": "A brief history of Data Science\n\n\n1962: Statistician John Tukey describes a field he calls data analysis, similar to what we now understand to be data science.\n1974: Computer scientist Peter Naur proposes the term data science, but in a different sense to today, namely as an alternative name for computer science.\n1992: Conference participants at the University of Montpellier II acknowledge the emergence of a new, inherently interdisciplinary field called data science focused on gaining insight from very diverse types of data.\n2001: William S. Cleveland introduces data science as its own discipline in his article Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics.\n2003: Columbia University starts publishing The Journal of Data Science.\n2012: With the HBR article on data scientist as the “sexiest job of the 21st century”, the term finds its way from the scientific realm into the mainstream."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#a-brief-history-of-data-science-1",
    "href": "slides/01_Introduction_to_DS.html#a-brief-history-of-data-science-1",
    "title": "Data Science and Data Analytics",
    "section": "A brief history of Data Science",
    "text": "A brief history of Data Science"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-and-artificial-intelligence-ai",
    "href": "slides/01_Introduction_to_DS.html#data-science-and-artificial-intelligence-ai",
    "title": "Data Science and Data Analytics",
    "section": "Data Science and Artificial Intelligence (AI)",
    "text": "Data Science and Artificial Intelligence (AI)\nSince the arrival of ChatGPT, everyone is talking about AI…\n\nIs Data Science = AI? How are the two related?"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-and-artificial-intelligence-ai-1",
    "href": "slides/01_Introduction_to_DS.html#data-science-and-artificial-intelligence-ai-1",
    "title": "Data Science and Data Analytics",
    "section": "Data Science and Artificial Intelligence (AI)",
    "text": "Data Science and Artificial Intelligence (AI)\nTo answer these questions, we first need some further definitions:\n\n\n\nArtificial Intelligence (AI) is the broad field of developing machines that can replicate human behaviour, including tasks related to perception, reasoning, learning and problem-solving.\nOne way of achieving this is through Machine Learning (ML) algorithms that detect patterns in large data sets and learn to make predictions from them.\nDeep Learning (DL) is an advanced branch of ML based on so-called neural networks. Innovations in this field are responsible for recent AI breakthroughs, such as ChatGPT."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#data-science-at-the-heart-of-ai",
    "href": "slides/01_Introduction_to_DS.html#data-science-at-the-heart-of-ai",
    "title": "Data Science and Data Analytics",
    "section": "Data Science at the heart of AI",
    "text": "Data Science at the heart of AI\n\n\n\nWith all of these fields, data science has significant overlap, but nonetheless, constitutes its own discipline.\nAnalogously, there are branches of AI (e.g. robotics or symbolic reasoning) that do not revolve around learning patterns from various data sources.\nMany of the advances in AI require precisely the interdisciplinary combination of computer science, maths, statistics and domain expertise that data science provides.\nAnd yet, a data scientist does much more than develop and apply algorithms for AI…"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#so-what-does-a-data-scientist-do",
    "href": "slides/01_Introduction_to_DS.html#so-what-does-a-data-scientist-do",
    "title": "Data Science and Data Analytics",
    "section": "So what does a Data Scientist do?",
    "text": "So what does a Data Scientist do?\n\n\n\nNow that we know roughly, what data science is, what does this discipline actually entail more concretely?\nIn other words, what is it that a data scientist does every day?\nThere are many ways to structure the data science, let’s first look at the seven stages of a data science project."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem",
    "href": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem",
    "title": "Data Science and Data Analytics",
    "section": "Step 1 – Identify the problem",
    "text": "Step 1 – Identify the problem"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-key-aspects",
    "href": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-key-aspects",
    "title": "Data Science and Data Analytics",
    "section": "Step 1 – Identify the problem – Key aspects",
    "text": "Step 1 – Identify the problem – Key aspects\n\n\nUnderstand the business context:\n\nFamiliarize yourself with the industry, market trends, and internal operations.\nExample: As a data scientist in a struggling e-commerce company, study industry benchmarks and analyze internal sales, user behavior, and engagement data.\n\nDefine the problem statement:\n\nTogether with the business, clearly articulate what issue needs solving.\nExample: “Our customer churn rate has increased by 20% over the last six months, impacting revenue.”\n\nIdentify stakeholders:\n\nDetermine who is impacted and who can provide insights.\nExample: marketing to explain and refine their existing customer engagement strategies or customer service to point out existing issues in post-purchase support."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-key-aspects-1",
    "href": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-key-aspects-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 1 – Identify the problem – Key aspects",
    "text": "Step 1 – Identify the problem – Key aspects\n\n\nOutline goals & objectives:\n\nIn collaboration with business, specify what success looks like and define measurable KPIs.\nExample: “Reduce the churn rate by 10% within the next six months.”\n\nMake the goals actionable:\n\nIdentify potential strategies to achieve the goals as well as the data required to implement them.\nExample: train a machine learning model to predict customer churn and use this model to tailor new customer engagement strategies. Requires data on past customer behaviour on the website and whether or not they churned."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-ds-contributions",
    "href": "slides/01_Introduction_to_DS.html#step-1-identify-the-problem-ds-contributions",
    "title": "Data Science and Data Analytics",
    "section": "Step 1 – Identify the problem – DS contributions",
    "text": "Step 1 – Identify the problem – DS contributions"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-2-identify-available-data-sources",
    "href": "slides/01_Introduction_to_DS.html#step-2-identify-available-data-sources",
    "title": "Data Science and Data Analytics",
    "section": "Step 2 – Identify available data sources",
    "text": "Step 2 – Identify available data sources"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-2-identify-available-data-sources-1",
    "href": "slides/01_Introduction_to_DS.html#step-2-identify-available-data-sources-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 2 – Identify available data sources",
    "text": "Step 2 – Identify available data sources\n\nDefinition: A data source refers to the physical or digital system where data is generated, stored and managed as a data table, data object or any other storage format. It is also where, stakeholders of this data (e.g. a data scientist) can access data for further use.\nWhy identify data sources early?\n\nFoundation for analysis: Data sources determine the quality, scope, and reliability of your insights.\nStrategic planning: Knowing where data comes from helps define project goals and design the analytics pipeline.\nIntegration & innovation: Combining diverse sources (internal and external) can reveal hidden trends and create competitive advantages."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#classifying-data-sources",
    "href": "slides/01_Introduction_to_DS.html#classifying-data-sources",
    "title": "Data Science and Data Analytics",
    "section": "Classifying data sources",
    "text": "Classifying data sources\n\n\nInternal sources\n\nDefinition: Data generated, stored, and maintained within the organization.\nExamples:\n\nEnterprise systems (CRM, ERP)\nIn-house databases\nSpreadsheets, documents, …\n\nKey point: Proprietary data that reflects your organization’s operations.\n\n\nExternal sources\n\nDefinition: Data sourced from outside the organization.\nExamples:\n\nPublic datasets (government statistics, market research)\nAPIs (social media, financial data feeds)\n\nKey point: Supplement internal data and provide broader market or industry insights."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-crm-system",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-crm-system",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – CRM system",
    "text": "Internal data sources – CRM system\nA Customer Relationship Management (CRM) system helps manage customer data. It helps businesses keep customer contact details up to date, track every customer interaction, and manage customer accounts."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-erp-system",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-erp-system",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – ERP system",
    "text": "Internal data sources – ERP system\nThe CRM typically forms part of the Enterprise Resource Planning (ERP) system. This type of software integrates data about all main business processes in a single system."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-databases",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-databases",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Databases",
    "text": "Internal data sources – Databases\n\n\n\nA database is an organized collection of data stored and accessed electronically.\nDatabases are managed using specialized software called a Database Management System (DBMS), which allows users to store, retrieve, and manipulate data efficiently.\nDatabases are the backbone of modern applications, supporting businesses, organizations, and systems across industries.\nDifferent types of databases can be classified in terms of their structure, usage, or storage methods. Two main types are:\n\nRelational / SQL\nNoSQL"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\nA relational database organizes data into tables with certain pre-defined relationships to each other. These relationships are logical connections, established on the basis of interaction among these tables."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-1",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-1",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\nThe relational model underlying such databases uses special lingo to refer to the entities that make up its structure:\n\n\n\n\n\nFamiliar term\nRelational DB term\n\n\n\n\nTable\nRelation\n\n\nRow\nTuple\n\n\nColumn\nAttribute\n\n\n\n\n\n\n\n\n\n\n\n\nUnlike tables, relations are unordered, i.e. you can shuffle the order of rows or columns and still get the same relation.\nAttributes (columns) specify a data type (e.g. integer or text), and each tuple (or row) contains the value of that specific data type."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-2",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-2",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\n\nAll tables in a relational database have an attribute known as the primary key, which is a unique identifier of a row.\nEach row can be used to create a relationship between different tables using a foreign key, i.e. a reference to a primary key of another existing table.\n\n\nLet’s see how this looks in practice: Suppose we have the following table:\n\n\n\n\n\n\n\n\n\n\n\n\n\nbook_id\nTitle\nAuthor\nFormat\nPublisher\nCountry\nPrice\n\n\n\n\n1\nHarry Potter\nJ.K. Rowling\nPaperback\nBanana Press\nUK\n$20\n\n\n2\nHarry Potter\nJ.K. Rowling\nE-book\nBanana Press\nUK\n$10\n\n\n3\nSherlock Holmes\nConan Doyle\nPaperback\nGuava Press\nUS\n$30\n\n\n4\nThe Hobbit\nJ.R.R. Tolkien\nPaperback\nBanana Press\nUK\n$30\n\n\n5\nSherlock Holmes\nConan Doyle\nPaperback\nGuava Press\nUS\n$15\n\n\n\n\n\n\nClearly, the primary key in this table is book_id."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-3",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-3",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\n\n\nWhat if “Banana Press” changed its name to “Pineapple Press”?\nWe would have to change every single instance of “Banana Press” in the Publisher attribute. Very cumbersome…\nWhat if we instead organized our database like this?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbook_id\nTitle\nAuthor\nFormat\nPublisher_ID\nPrice\n\n\n\n\n1\nHarry Potter\nJ.K. Rowling\nPaperback\npub_1\n$20\n\n\n2\nHarry Potter\nJ.K. Rowling\nE-book\npub_1\n$10\n\n\n3\nSherlock Holmes\nConan Doyle\nPaperback\npub_2\n$30\n\n\n4\nThe Hobbit\nJ.R.R. Tolkien\nPaperback\npub_1\n$30\n\n\n5\nSherlock Holmes\nConan Doyle\nPaperback\npub_2\n$15\n\n\n\n\n\n\nPublisher_ID\nPublisher\nCountry\n\n\n\n\npub_1\nBanana Press\nUK\n\n\npub_2\nGuava Press\nUS"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-4",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-4",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\n\nWe introduced the foreign key “Publisher_ID” into book relation.\nNow, we would just have to change the name of the publisher once, namely in the publisher relation.\nThis is an example of a process called database normalization, which helps reduce redundancy and improve data integrity.\nTo retrieve data from a relational database, one uses a specialized computer language called Structured Query Language (SQL), which gives relational databases an alternative name.\nTo reconstruct our original book relation, the SQL statement would be:\n\n\nSELECT b.book_id, b.Title, b.Author, b.Format,\n       p.Publisher, p.Country, b.Price\nFROM Books AS b\nLEFT JOIN Publishers AS p ON b.Publisher_ID = p.Publisher_ID"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-5",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-relational-databases-5",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Relational databases",
    "text": "Internal data sources – Relational databases\nSQL will not be part of this course, but data scientists working in industry tend to write a lot of SQL…"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-types-of-data",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-types-of-data",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Types of data",
    "text": "Internal data sources – Types of data\n\nRelational databases are the de-facto standard for managing structured data, i.e. data that is highly organized in a table-like format.\nData can also take other forms though:\n\nUnstructured data is not organized in a specific way and does therefore not lend itself to the relational model.\nExamples: text documents, images and video.\nSemi-structured data is a hybrid of the two preceding forms, containing some organizational elements (like tags or markers), but allowing for more flexibility than data stored in a relational model.\nExamples: Data stored in JSON, XML or HTML files."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-types-of-data-1",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-types-of-data-1",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Types of data",
    "text": "Internal data sources – Types of data\nFor unstructured and semi-structured data, the relational model is typically not suitable, or at least suboptimal. For this purpose, non-relational models are preferred."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – NoSQL databases",
    "text": "Internal data sources – NoSQL databases\n\nDespite some fuzzy terminology, non-relational databases are commonly referred to as NoSQL.\nTypes of NoSQL databases include:\n\nKey-value store: simplest form of a NoSQL database, typically of little use in data science.\nWide column store: uses tables, rows and columns similar to relational DBs, but names and format of columns can vary from row to row.\nDocument store: perfect for semi-structured data, can handle complex data structures.\nGraph database prioritize relationships between data objects. They use nodes (data entities) and edges (relationships) to model data.\n\nAs an illustration of the kind of benefits achievable through NoSQL databases, we consider an example for a document store."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-1",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-1",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – NoSQL databases",
    "text": "Internal data sources – NoSQL databases"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-2",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-2",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – NoSQL databases",
    "text": "Internal data sources – NoSQL databases\n\nAs the name suggests, a document store is centred around the concept of “document”, like a JSON or XML file.\nAll documents are assumed to be encoded in the same format.\nEach document has a unique key that can be used to retrieve it.\nA collection of documents could be considered analogous to a table in a relational database, where each row is a document. However, a collection of documents is much more flexible:\n\nIn a table, all rows must have the same sequence of columns.\nIn a collection of documents however, each document can have completely different fields."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-3",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-nosql-databases-3",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – NoSQL databases",
    "text": "Internal data sources – NoSQL databases\n\n\n\n\n\n\n\n\n\nDocument 1 – Harry Potter\n\n\n\n\n\n\n\nDocument 2 – Sherlock Holmes\n\n\n\n\n\n\n\nDocument 3 – The Hobbit\n\n\n\n\n\nAdvantages of such a NoSQL database:\n\nNo need to normalize\nEasier extensibility: changes in one document do not affect others.\nBetter locality (all information on a document in one place instead of spread across several tables)."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#internal-data-sources-others",
    "href": "slides/01_Introduction_to_DS.html#internal-data-sources-others",
    "title": "Data Science and Data Analytics",
    "section": "Internal data sources – Others",
    "text": "Internal data sources – Others\n\nUnfortunately, a lot of data in corporations is not in a nicely structured form in a database… Instead, big amounts of data are spread around in messy spread sheets, e-mails, presentations, csv-files, etc.\nOrganizing and cleaning such data to gain meaningful insight is a key part of being a data scientist."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-public-data-sets",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-public-data-sets",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – Public data sets",
    "text": "External data sources – Public data sets\n\nExternal data sources are often crucial to understand the general economic environment, market or industry trends, and much more.\nThe first approach towards external data sources is usually to verify whether there are any publicly available data sets from reliable sources, such as:"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs\n\nOften, rather than downloading a data set in form of a csv or xls file, data providers offer Application Programming Interfaces (APIs) to access their data directly.\nAPIs serve as a bridge between different software applications, enabling them to communicate and share data in (near) real-time.\nAs a data source, APIs can be used, for instance, to access data from…\n\n… social media to detect and leverage current viral trends.\n… AI-providers like OpenAI to incorporate information produced by ChatGPT.\n… tech companies like Google to identify highly-rated competitors in the area based on Google Maps.\n…"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-1",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-1",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs\n\nLet’s say you wanted to find out, how many views all of your company’s tweets on X generated last week. If your company tweets a lot, that’s a lot of manual labour… Instead, we could write a program (in R) that uses the X API to gather this information."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-2",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-2",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs\n\n\nCommunication with APIs works through a request and response cycle: the request is sent to the API, which retrieves the data and returns it to the user in form of the response.\nAn API request will typically include the following components:\n\nEndpoint: a dedicated URL declared by the API provider to access the desired resource, e.g. https://api.x.com/2/insights/historical (see here).\nMethod: the type of operation the client wants to perform. Data retrieval operations typically use the GET method (see here for more information on HTTP methods).\nParameters: the variables indicating the specific instructions for the API to process, e.g. the time period for which you want to analyze the views of all tweets.\nRequest headers: meta information about the request, e.g. authentication credentials, such as an API key.\nRequest body: contains further information to be passed to the API, e.g. parameters that are not passed as part of the endpoint."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-3",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-3",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-4",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-4",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs\n\nThe response returned by the API will then typically include the following components:\n\nStatus code: HTTP status codes are three-digit codes that indicate the outcome of an API request. Some common ones are:\n\n200: OK (server successfully returned the requested data)\n400: Bad request (e.g. malformatted request syntax)\n404: Not found (server cannot find requested resource)\n\nResponse headers: very similar to request headers, but provide additional information about response.\nResponse body: includes actual requested data, e.g. the number of views for each of last week’s tweets."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#external-data-sources-apis-5",
    "href": "slides/01_Introduction_to_DS.html#external-data-sources-apis-5",
    "title": "Data Science and Data Analytics",
    "section": "External data sources – APIs",
    "text": "External data sources – APIs"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-3-identify-if-additional-sources-are-needed",
    "href": "slides/01_Introduction_to_DS.html#step-3-identify-if-additional-sources-are-needed",
    "title": "Data Science and Data Analytics",
    "section": "Step 3 – Identify if additional sources are needed",
    "text": "Step 3 – Identify if additional sources are needed"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-4-statistical-analysis",
    "href": "slides/01_Introduction_to_DS.html#step-4-statistical-analysis",
    "title": "Data Science and Data Analytics",
    "section": "Step 4 – Statistical analysis",
    "text": "Step 4 – Statistical analysis"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-4-statistical-analysis-1",
    "href": "slides/01_Introduction_to_DS.html#step-4-statistical-analysis-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 4 – Statistical analysis",
    "text": "Step 4 – Statistical analysis\n\nAfter data has been obtained, the data scientist needs to clean, process and explore it. This can involve:\n\nHandling missing values, outliers, and noise.\nExploratory data analysis (EDA): Visualizations, correlation analysis, and summary statistics.\n\nBased on clean data, statistical tests can be employed to validate existing business hypotheses. Here, all the tests you have (hopefully) learned in your statistics classes come in, e.g. T-tests, ANOVA, \\(\\chi^2\\) tests, etc.\nThrough this process, the data scientist can start to develop an understanding of the data."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development",
    "href": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development",
    "title": "Data Science and Data Analytics",
    "section": "Step 5 – Implementation and development",
    "text": "Step 5 – Implementation and development"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development-1",
    "href": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 5 – Implementation and development",
    "text": "Step 5 – Implementation and development\n\nAfter familiarizing themselves with the data, data scientists then have to translate insights into action. This involves offering some kind of product back to the business.\nThis could take the form of:\n\nTraining and deploying a machine learning model, e.g. offering the business a model that tells them the probability that a given customer will churn.\nBuilding dashboards to make the data-driven insights accessible to non-technical people in business.\nSupport strategic decision making by providing appropriate statistics, figures, graphs and other data."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development-2",
    "href": "slides/01_Introduction_to_DS.html#step-5-implementation-and-development-2",
    "title": "Data Science and Data Analytics",
    "section": "Step 5 – Implementation and development",
    "text": "Step 5 – Implementation and development"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-6-communicate-results",
    "href": "slides/01_Introduction_to_DS.html#step-6-communicate-results",
    "title": "Data Science and Data Analytics",
    "section": "Step 6 – Communicate results",
    "text": "Step 6 – Communicate results"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-6-communicate-results-1",
    "href": "slides/01_Introduction_to_DS.html#step-6-communicate-results-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 6 – Communicate results",
    "text": "Step 6 – Communicate results\n\nCommunication is an often underappreciated aspect of data science in business: any data insight can only be as good as it can be made understood by the people that (should) act on it.\nKey communication goals:\n\nSimplify complex technical details without losing accuracy.\nTailor the message to your audience (executives, technical teams, clients).\n\nCommunication strategies:\n\nVisual storytelling: Use charts, graphs, and dashboards to highlight trends and key metrics.\nNarrative & context: Frame results within a business story: What problem was solved? How do the results impact the business?\nActionable insights: Include clear recommendations and next steps."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-7-maintenance",
    "href": "slides/01_Introduction_to_DS.html#step-7-maintenance",
    "title": "Data Science and Data Analytics",
    "section": "Step 7 – Maintenance",
    "text": "Step 7 – Maintenance"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#step-7-maintenance-1",
    "href": "slides/01_Introduction_to_DS.html#step-7-maintenance-1",
    "title": "Data Science and Data Analytics",
    "section": "Step 7 – Maintenance",
    "text": "Step 7 – Maintenance\n\nOnce a data science product like a machine learning model or a dashboard is “in production”, i.e. is used by business, it will not age like fine wine…\nInstead, changes in data over time and general software rot will quickly cause the product to be useless if left unchanged.\nHence, data scientists need to subject their products to continuous maintenance:\n\nMonitor performance over time: track metrics to detect data drift or deterioration in model performance.\nUpdate models and systems: retrain models with new data when necessary.\nRegularly engage stakeholders for feedback\nIdentify new problems and areas for improvement and restart the cycle."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-essence-of-data-science",
    "href": "slides/01_Introduction_to_DS.html#the-essence-of-data-science",
    "title": "Data Science and Data Analytics",
    "section": "The essence of Data Science",
    "text": "The essence of Data Science\n\n\n\nData science is a demanding field that requires skills in maths, statistics and computer science, but also high levels of domain expertise and excellent communication.\nThus, it is impossible to cover all areas of DS in a single course…\nThis course focuses on what could be called the essence of data science:\n\nStatistical analysis\nImplementation and development\nCommunication of results\n\nWe will further break these steps down in the data science workflow."
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-1",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-1",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow",
    "text": "The Data Science workflow\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-i",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-i",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part I",
    "text": "The Data Science workflow – Part I\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-i-1",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-i-1",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part I",
    "text": "The Data Science workflow – Part I\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-ii",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-ii",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part II",
    "text": "The Data Science workflow – Part II\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-iii",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-iii",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part III",
    "text": "The Data Science workflow – Part III\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-understanding-data",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-understanding-data",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Understanding data",
    "text": "The Data Science workflow – Understanding data\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-iv",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-part-iv",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Part IV",
    "text": "The Data Science workflow – Part IV\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#the-data-science-workflow-how-program",
    "href": "slides/01_Introduction_to_DS.html#the-data-science-workflow-how-program",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – How? Program!",
    "text": "The Data Science workflow – How? Program!\n\n\nSource: Wickham et al. (2023)"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#how-about-data-analytics",
    "href": "slides/01_Introduction_to_DS.html#how-about-data-analytics",
    "title": "Data Science and Data Analytics",
    "section": "How about Data Analytics?",
    "text": "How about Data Analytics?\n\nThe name of the course is Data Science and Data Analytics, but only data science has been mentioned so far. How about data analytics?\nEssentially, data analytics is data science without the strong computer science and machine learning focus. Therefore, the steps involved in data analytics are typically a subset of the steps in a data science project:"
  },
  {
    "objectID": "slides/01_Introduction_to_DS.html#how-about-data-analytics-1",
    "href": "slides/01_Introduction_to_DS.html#how-about-data-analytics-1",
    "title": "Data Science and Data Analytics",
    "section": "How about Data Analytics?",
    "text": "How about Data Analytics?\nThe two fields thus have a large intersection, which is also reflected in the tasks that data scientists and data analysts share:\n\nAs we are focused on tasks in the intersection, we will mostly be referring to data science as the overarching topic in the remainder of this course."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#programming-languages-for-data-science",
    "href": "slides/02_The_Essentials_of_R_Programming.html#programming-languages-for-data-science",
    "title": "Data Science and Data Analytics",
    "section": "Programming languages for Data Science",
    "text": "Programming languages for Data Science"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#what-is-r",
    "href": "slides/02_The_Essentials_of_R_Programming.html#what-is-r",
    "title": "Data Science and Data Analytics",
    "section": "What is R?",
    "text": "What is R?\n\n\nR is an open-source statistical programming language.\nR is not a programming language like C or Java. It was not created by software engineers for software development. Instead, it was developed by statisticians as an environment for statistical computing and graphics.\nSome of its attractive features are:\n\nR is free and open source.\nR runs on all major platforms: Windows, macOS, UNIX/Linux.\nThere is a large, growing, and active community of R users and, as a result, there are numerous resources for learning and asking questions.\nIt is easily extensible through packages. In this way, it is easy to share software implementations of new data science methodologies."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#a-very-brief-history-of-r",
    "href": "slides/02_The_Essentials_of_R_Programming.html#a-very-brief-history-of-r",
    "title": "Data Science and Data Analytics",
    "section": "A (very) brief history of R",
    "text": "A (very) brief history of R\n\n\nR was first developed by Robert Gentleman and Ross Ihaka (University of Auckland, New Zealand) in the 1990s.\nVery early in the development of R, two statisticians at TU Vienna (Kurt Hornik and Fritz Leisch) got word about the initiative in New Zealand and started contributing to R.\nSoon, the growing interest in this project from researchers all over the world led to the establishment of the R Core team, which maintains R to this day.\nThe strong involvement of the two Viennese researchers also led to the establishment of the R Foundation for Statistical Computing in Vienna (now at WU). Its main mission is the support of the continued development of R.\nToday, R is the lingua franca of statistics and an essential tool for data science."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#importance-of-r-in-data-science",
    "href": "slides/02_The_Essentials_of_R_Programming.html#importance-of-r-in-data-science",
    "title": "Data Science and Data Analytics",
    "section": "Importance of R in Data Science",
    "text": "Importance of R in Data Science"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#what-is-rstudio",
    "href": "slides/02_The_Essentials_of_R_Programming.html#what-is-rstudio",
    "title": "Data Science and Data Analytics",
    "section": "What is RStudio?",
    "text": "What is RStudio?\n\n\nRStudio is a convenient interface for R called an integrated development environment (IDE). An IDE is the program on a computer that a developer uses to write code.\nRStudio is an open-source IDE by Posit specifically designed to support developers in writing R code. It is not a requirement for programming with R, but it is very commonly used by R programmers and data scientists."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#what-is-rstudio-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#what-is-rstudio-1",
    "title": "Data Science and Data Analytics",
    "section": "What is RStudio?",
    "text": "What is RStudio?"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#a-tour-of-rstudio",
    "href": "slides/02_The_Essentials_of_R_Programming.html#a-tour-of-rstudio",
    "title": "Data Science and Data Analytics",
    "section": "A Tour of RStudio",
    "text": "A Tour of RStudio"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#a-tour-of-rstudio-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#a-tour-of-rstudio-1",
    "title": "Data Science and Data Analytics",
    "section": "A Tour of RStudio",
    "text": "A Tour of RStudio\n\nWhile we can write and execute code directly in the R console in the bottom left pane in RStudio, it is better to write all code in an R script.\nAn R script is simply a text file that contains R code.\nIn RStudio, R scripts can be opened, viewed and edited in the top left pane. To open a new R script, go to File &gt; New File &gt; R script.\nA single line of code in an R script can be sent to the console for execution via Ctrl + Enter (or Cmd + Enter on Mac).\nTo write a comment in an R script, simply put a hashtag (#) in front of the comment:\n\n\n# This is our first R script\n2 + 2 # We calculate 2 + 2\n\n[1] 4"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-mathematical-operators",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-mathematical-operators",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Mathematical operators",
    "text": "First steps in R – Mathematical operators\nFirst, let’s use R as a calculator. We can write a calculation into our R script and R will give us the result, when the code is executed:\n\n2 + 3 # Addition\n\n[1] 5\n\n2 - 4 # Subtraction\n\n[1] -2\n\n3 * 4 # Multiplication\n\n[1] 12\n\n4 / 2 # Division\n\n[1] 2\n\n3^3 # Exponentiation\n\n[1] 27\n\n(2 + 3)*(3 + 4) # Bracketing\n\n[1] 35\n\n2^(3 + 4/2)*4 # All together\n\n[1] 128"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-relational-and-logical-operators",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-relational-and-logical-operators",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Relational and logical operators",
    "text": "First steps in R – Relational and logical operators\nWe can also perform comparisons using R:\n\n3 &lt; 5\n\n[1] TRUE\n\n3 &gt; 5\n\n[1] FALSE\n\n3 &lt;= 3\n\n[1] TRUE\n\n3 &gt;= 5\n\n[1] FALSE\n\n3 == 5 # Attention for the double equal sign!\n\n[1] FALSE\n\n3 != 5\n\n[1] TRUE\n\n\n\nWith the help of & and |, we can also carry out AND and OR operations:\n\n(3 == 5) | (5 == 5) # OR\n\n[1] TRUE\n\n(3 == 5) & (5 == 5) # AND\n\n[1] FALSE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions\nR comes with many functions that you can use to perform tasks from simple to sophisticated. Functions have inputs (aka arguments) that you pass into them and outputs (aka return values) that they give back. Functions are fundamental to how R works.\n\nLet’s see an example. Say, we want to round the number 5.293586 to two decimal digits. Fortunately, there is a function in R called round. But how do we use it?\nTo find out, we can bring up the R help, which provides documentation for every function in R, by typing ? and the name of the function into the console:\n\n?round"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-1",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-2",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions\n\nround(5.293586, 2)\n\n[1] 5.29\n\n\n\nGenerally, we can use the args function to see the arguments that a function takes. For example, let’s say, we wanted to simulate someone throwing a regular die, i.e. randomly sample numbers from 1 to 6. Luckily, there is an R function called sample. Let’s inspect it.\n\nargs(sample)\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\n\n\n\nTo find out more information on what these arguments actually mean, let’s consult the documentation again:\n\n?sample"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-3",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-4",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-4",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions\nSo, to simulate 20 throws of a regular die, we have to use the function like this:\n\nsample(6, 20, replace = TRUE)\n\n [1] 1 5 1 1 2 4 2 2 1 4 1 5 6 4 2 2 3 1 1 3\n\n\n\nNote that we can give the arguments to the function\n\n\nin the order that they are given in the documentation (then they do not need to be named).\nby explicitly naming them when calling the function (then the order does not matter).\nby mixing unnamed and named arguments (like we have done here).\n\n\n\n\nLet’s see another example of this."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-5",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-functions-5",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Functions",
    "text": "First steps in R – Functions\nLet’s say, we wanted to compute the base 2 logarithm of 10. For this, we need the log function in R:\n\nargs(log)\n\nfunction (x, base = exp(1)) \nNULL\n\n\n\nSo, based on the three options given on the previous slide, all of the following three ways of calling log lead to the correct result:\n\nlog(10, 2)\n\n[1] 3.321928\n\nlog(base = 2, x = 10)\n\n[1] 3.321928\n\nlog(10, base = 2)\n\n[1] 3.321928\n\n\nHowever, options 1 and 3 are preferable, as switching the order of arguments tends to make code less easily readable for others (which could be yourself in the future…)"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Variables and assignment",
    "text": "First steps in R – Variables and assignment\nLet’s say, you were given the task to solve the quadratic equation\n\\[3x^2 - 5x - 1 = 0\\]\nYou will remember from back in your high school days that we can use the “midnight formula” for this:\n\\[x_{1/2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\]\nWith R, we can easily compute this of course:\n\n(-(-5) + sqrt((-5)^2 - 4*3*(-1)))/(2*3)\n\n[1] 1.847127\n\n(-(-5) - sqrt((-5)^2 - 4*3*(-1)))/(2*3)\n\n[1] -0.1804604"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-1",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Variables and assignment",
    "text": "First steps in R – Variables and assignment\nNext, we are given another quadratic equation\n\\[4x^2 - 8x + 2 = 0\\]\nTo compute the solution with R, we would have to replace every occurrence of \\(a\\), \\(b\\) and \\(c\\), so in total, we would have to make 10 replacements to get both solutions. That’s too cumbersome and error-prone…\n\nInstead, we can define variables \\(a\\), \\(b\\) and \\(c\\) and simply assign different values to them every time we have to solve a quadratic equation. Such assignments happen in R with the help of the assignment operator &lt;- (read it as “gets”):\n\na &lt;- 3\nb &lt;- -5\nc &lt;- -1\n# Note that nothing is printed to the console when assigning variables.\n\nNow, we can write expressions using these variables like we would in maths."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-2",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Variables and assignment",
    "text": "First steps in R – Variables and assignment\n\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n\n[1] 1.847127\n\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n\n[1] -0.1804604\n\n\nNow, to compute the solution to the second quadratic equation, we simply re-define the variables a, b and c and then evaluate the same expression again:\n\na &lt;- 4\nb &lt;- -8\nc &lt;- 2\n\n(-b + sqrt(b^2 - 4*a*c))/(2*a)\n\n[1] 1.707107\n\n(-b - sqrt(b^2 - 4*a*c))/(2*a)\n\n[1] 0.2928932\n\n\n\nNote that R is case-sensitive, so it now knows variables with names a, b and c, but it does not know A, B or C:\n\nA\n\nError: Objekt 'A' nicht gefunden"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-variables-and-assignment-3",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – Variables and assignment",
    "text": "First steps in R – Variables and assignment\nWith assignments, we are creating objects in R that are saved and can be referenced by the names that we give them (e.g. a, b and c). Creating objects like this will make them appear in the work space in pane 4 of the RStudio window:\n\n\n\n\n\nWe can also see all variables currently defined in the work space by typing ls():\n\nls()\n\n[1] \"a\" \"b\" \"c\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-user-defined-functions",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-user-defined-functions",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – User-defined functions",
    "text": "First steps in R – User-defined functions\nWith the assignment operator, we can also define our own functions of course! To define a function, we need a function name, arguments, a function body and a return value:"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-user-defined-functions-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#first-steps-in-r-user-defined-functions-1",
    "title": "Data Science and Data Analytics",
    "section": "First steps in R – User-defined functions",
    "text": "First steps in R – User-defined functions\nConsider the following example: say we want to write a function called quadratic_solver (function name) that gives us the solution to any quadratic equation. It needs input arguments a, b and c and should return the two solutions. In the function body, the two solutions should be computed for the three inputs. So, we could create the function as follows:\n\nquadratic_solver &lt;- function(a, b, c){\n  sol_1 &lt;- (-b + sqrt(b^2 - 4*a*c))/(2*a)\n  sol_2 &lt;- (-b - sqrt(b^2 - 4*a*c))/(2*a)\n  return(c(sol_1, sol_2))\n}\n\n\nNow, we can use this function like any other:\n\nquadratic_solver(3, -5, -1)\n\n[1]  1.8471271 -0.1804604\n\nquadratic_solver(4, -8, 2)\n\n[1] 1.7071068 0.2928932"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#data-types-in-r",
    "href": "slides/02_The_Essentials_of_R_Programming.html#data-types-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Data types in R",
    "text": "Data types in R\nIn R, the following six data types are available:\n\ndouble (i.e., double precision floating-point number – R lingo for real number),\ninteger,\ncharacter (sometimes referred to as string),\nlogical/boolean (can only take values TRUE or FALSE),\ncomplex (as in complex numbers, not relevant for us),\nraw (not relevant for us),\n\nWe can find the type of an object using the function typeof. We can verify, whether an object is of a certain &lt;type&gt; by using the function is.&lt;type&gt;. Let’s see some examples."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#double",
    "href": "slides/02_The_Essentials_of_R_Programming.html#double",
    "title": "Data Science and Data Analytics",
    "section": "Double",
    "text": "Double\nBy default, R will save any number that you type in as a double.\n\na &lt;- 2\nb &lt;- 3.141593\n\ntypeof(a)\n\n[1] \"double\"\n\ntypeof(b)\n\n[1] \"double\"\n\nis.double(a)\n\n[1] TRUE\n\nis.double(b)\n\n[1] TRUE\n\n\nTogether with integer, the data type double is one of the two numeric types, i.e. representing numbers:\n\nis.numeric(a)\n\n[1] TRUE\n\nis.numeric(b)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#integer",
    "href": "slides/02_The_Essentials_of_R_Programming.html#integer",
    "title": "Data Science and Data Analytics",
    "section": "Integer",
    "text": "Integer\nIntegers (whole numbers) are (positive or negative) numbers that can be written without a decimal component. This data type is more important for developers as it saves memory (compared to doubles). To specify an integer over a double, the number has to be followed by an uppercase L:\n\ni &lt;- 10L\n\ntypeof(i)\n\n[1] \"integer\"\n\nis.integer(i)\n\n[1] TRUE\n\nis.double(i)\n\n[1] FALSE\n\n\n(Pure) integers are also numeric of course:\n\nis.numeric(i)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#character",
    "href": "slides/02_The_Essentials_of_R_Programming.html#character",
    "title": "Data Science and Data Analytics",
    "section": "Character",
    "text": "Character\nText data is represented in R with the help of the character data type. To demarcate a string of characters, you can use double or single quotes (\"\" or '').\n\nletter &lt;- \"a\"\nw &lt;- 'Hello, world.'\n\ntypeof(letter)\n\n[1] \"character\"\n\ntypeof(w)\n\n[1] \"character\"\n\nis.character(letter)\n\n[1] TRUE\n\nis.character(w)\n\n[1] TRUE\n\n\nOf course, character objects are not numeric.\n\nis.numeric(letter)\n\n[1] FALSE\n\nis.numeric(w)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#logical",
    "href": "slides/02_The_Essentials_of_R_Programming.html#logical",
    "title": "Data Science and Data Analytics",
    "section": "Logical",
    "text": "Logical\nWhen working with relational operators, we already saw a couple of instances of the logical data type. It can only take the values TRUE and FALSE. Negation of a logical object (i.e. saying NOT) can be achieved with the help of the ! operator:\n\nbigger &lt;- 5 &gt; 3\n\ntypeof(bigger)\n\n[1] \"logical\"\n\nis.logical(bigger)\n\n[1] TRUE\n\n!bigger\n\n[1] FALSE\n\n\nLogical objects are also not numeric.\n\nis.numeric(bigger)\n\n[1] FALSE\n\nis.numeric(!bigger)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#type-coercion",
    "href": "slides/02_The_Essentials_of_R_Programming.html#type-coercion",
    "title": "Data Science and Data Analytics",
    "section": "Type coercion",
    "text": "Type coercion\nWe can coerce an object to be of a certain &lt;type&gt; by using the function as.&lt;type&gt;. This process is called type coercion.\nIn some cases, this is very intuitive…\n\nas.double(\"2.71828\")\n\n[1] 2.71828\n\nas.integer(\"10\")\n\n[1] 10\n\nas.numeric(\"10\") # creates a double\n\n[1] 10\n\nas.character(42)\n\n[1] \"42\"\n\n\n\n… in others, maybe not so much…\n\nas.integer(2.9) # returns the closest smaller integer\n\n[1] 2\n\nas.logical(2.9) # returns TRUE for any number other than 0\n\n[1] TRUE\n\nas.logical(0)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#special-values",
    "href": "slides/02_The_Essentials_of_R_Programming.html#special-values",
    "title": "Data Science and Data Analytics",
    "section": "Special values",
    "text": "Special values\nNon-sensical attempts at coercion are translated as NA (not available).\n\nas.numeric(\"Hello, world.\")\n\nWarning: NAs durch Umwandlung erzeugt\n\n\n[1] NA\n\nas.logical(\"5\")\n\n[1] NA\n\n\n\nNA is one of four special values. While NA can occur for any data type, the other three can only occur for numeric data types. These are:\n\nInf: positive infinity\n-Inf: negative infinity\nNaN: not a number\n\nWhile they are not technically numbers, these special values still follow logical rules when applying mathematical operations on them."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#special-values-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#special-values-1",
    "title": "Data Science and Data Analytics",
    "section": "Special values",
    "text": "Special values\n\n\n\n2 / 0\n\n[1] Inf\n\n2 / 0 + 2 / 0\n\n[1] Inf\n\n-2 / 0\n\n[1] -Inf\n\n-2 / 0 - 2 / 0\n\n[1] -Inf\n\n\n\n\n0 / 0\n\n[1] NaN\n\n2 / 0 - 2 / 0\n\n[1] NaN\n\n- 1 / 0 + 2 / 0\n\n[1] NaN\n\n\n\n\nUnlike NaN, NAs are genuinely unknown values. Nevertheless, they also have their logical rules. Consider the following example. Let’s think about why each of the following four results makes sense:\n\n\n\nTRUE | NA\n\n[1] TRUE\n\nTRUE & NA\n\n[1] NA\n\n\n\n\nFALSE | NA\n\n[1] NA\n\nFALSE & NA\n\n[1] FALSE\n\n\n\n\n\nNote: all special values have their own is.&lt;special&gt; function to check for them, i.e. is.na, is.nan and is.infinite."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#data-structures",
    "href": "slides/02_The_Essentials_of_R_Programming.html#data-structures",
    "title": "Data Science and Data Analytics",
    "section": "Data structures",
    "text": "Data structures\n\nOn top of these six basic data types, R builds several kinds of more complex data structures in different ways to suit different applications that are regularly encountered in the statistics / data science context.\nWe will use the following regularly and hence discuss them in more detail:\n\n(Atomic) vector\nlist\nmatrix\ndata.frame\nfactor\nDate"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#atomic-vector",
    "href": "slides/02_The_Essentials_of_R_Programming.html#atomic-vector",
    "title": "Data Science and Data Analytics",
    "section": "Atomic vector",
    "text": "Atomic vector\nAn atomic vector is a simple vector of values of one data type. Values can be concatenated together into a vector using the c function. For example:\n\nx &lt;- c(3, 4, 5)\nx\n\n[1] 3 4 5\n\n\n\nSince we have filled the vector x with values of type double, it will itself also be of type double. It has several attributes / characteristics such as length:\n\ntypeof(x)\n\n[1] \"double\"\n\nis.double(x)\n\n[1] TRUE\n\nlength(x)\n\n[1] 3\n\n\n\n\nWe can also assign names to the vector elements:\n\nnames(x) &lt;- c(\"a\", \"b\", \"c\")\nx\n\na b c \n3 4 5"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#atomic-vector-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#atomic-vector-1",
    "title": "Data Science and Data Analytics",
    "section": "Atomic vector",
    "text": "Atomic vector\nWhat happens if we try to create a vector with data of different types?\n\ny &lt;- c(1, \"a\", TRUE)\ny\n\n[1] \"1\"    \"a\"    \"TRUE\"\n\n\n\nWhen we attempt to combine different types, R will coerce the data in a fixed order, namely character \\(\\to\\) double \\(\\to\\) integer \\(\\to\\) logical, i.e. if any data of a higher-order type appears in the vector creation, the vector will be of that type:\n\ntypeof(y)\n\n[1] \"character\"\n\n\n\n\nSo, what will be the type of the following vector?\n\nz &lt;- c(0L, 1L, 2, TRUE)\n\n\n\nAs the 2 is of type double, this is the highest-order type in the vector, so:\n\ntypeof(z)\n\n[1] \"double\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-vectorization",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-vectorization",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Vectorization",
    "text": "Working with vectors – Vectorization\nMost operations in R are vectorized, which means they are (automatically) performed element by element:\n\nx\n\na b c \n3 4 5 \n\nx^2\n\n a  b  c \n 9 16 25 \n\nlog(x)\n\n       a        b        c \n1.098612 1.386294 1.609438 \n\n\nThis also applies when we want to add / subtract / multiply / divide two vectors element-wise:\n\ny &lt;- c(5, 12, 13)\nx + y\n\n a  b  c \n 8 16 18 \n\nx * y\n\n a  b  c \n15 48 65"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-vectorization-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-vectorization-1",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Vectorization",
    "text": "Working with vectors – Vectorization\nIf two vectors are of different lengths, then R recycles the smaller one to allow operations like the following:\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(10, 20, 30, 40, 50, 60)\nx + 2\n\n[1] 3 4 5\n\nx + y\n\n[1] 11 22 33 41 52 63\n\n\nHowever, beware of recycling:\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(10, 20, 30, 40, 50, 60, 70)\nx + y\n\nWarning in x + y: Länge des längeren Objektes\n     ist kein Vielfaches der Länge des kürzeren Objektes\n\n\n[1] 11 22 33 41 52 63 71"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-sequences",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-sequences",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Sequences",
    "text": "Working with vectors – Sequences\nTo create vectors that are sequences, there is a very useful R function called seq:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(2, 20, by = 2)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\nseq(1.2, 0.2, by = -0.1)\n\n [1] 1.2 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2\n\n\n\nSequences of consecutive integers – like the first one we saw – are particularly frequently needed in (R) programming. For this reason, there is a short-hand notation (“syntactic sugar”) to create sequences of that kind, which is start_value:end_value:\n\ns &lt;- 1:10\ntypeof(s)\n\n[1] \"integer\"\n\ns\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-subsetting",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-subsetting",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Subsetting",
    "text": "Working with vectors – Subsetting\nIf we want to extract or replace elements of a vector, you can use square brackets [] with logical or numeric input:\n\nalphabet &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n              \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\n# Numeric subsetting\nalphabet[1:3] # first three\n\n[1] \"a\" \"b\" \"c\"\n\nalphabet[3:1] # first three in descending order\n\n[1] \"c\" \"b\" \"a\"\n\nalphabet[c(2, 4)] # second and fourth\n\n[1] \"b\" \"d\"\n\nalphabet[-(1:10)] # all except the first 10\n\n [1] \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\nalphabet[5] &lt;- \"E\" # replace the fifth element by a capital E\n\n# Logical subsetting\nalphabet == \"E\"\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE FALSE\n\nalphabet[alphabet == \"E\"]\n\n[1] \"E\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-subsetting-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-subsetting-1",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Subsetting",
    "text": "Working with vectors – Subsetting\nWe can also use the which function to turn a logical vector into an integer vector that gives the indices of all elements in the vector that are TRUE:\n\nwhich(alphabet == \"E\")\n\n[1] 5\n\nalphabet[which(alphabet == \"E\")]\n\n[1] \"E\"\n\n\n\nWith the %in% operator, we can check whether elements of the vector are contained in another vector:\n\nalphabet %in% c(\"x\", \"y\", \"z\")\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[25]  TRUE  TRUE\n\nwhich(alphabet %in% c(\"x\", \"y\", \"z\"))\n\n[1] 24 25 26\n\nalphabet[alphabet %in% c(\"x\", \"y\", \"z\")]\n\n[1] \"x\" \"y\" \"z\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-sorting",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-vectors-sorting",
    "title": "Data Science and Data Analytics",
    "section": "Working with vectors – Sorting",
    "text": "Working with vectors – Sorting\nFor sorting vectors, we can use the sort function. This works with character vectors:\n\n# Sorting a character vector\nunsorted_letters &lt;- sample(alphabet)\nunsorted_letters\n\n [1] \"d\" \"E\" \"m\" \"y\" \"t\" \"b\" \"h\" \"c\" \"a\" \"j\" \"v\" \"n\" \"k\" \"f\" \"q\" \"p\" \"z\" \"l\" \"x\"\n[20] \"u\" \"w\" \"i\" \"r\" \"g\" \"s\" \"o\"\n\nsorted_letters &lt;- sort(unsorted_letters)\nsorted_letters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"E\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\n… as well as with numeric vectors:\n\n# Sorting a numeric vector in decreasing order:\nunsorted_numbers &lt;- sample(1:20)\nunsorted_numbers\n\n [1]  3 17 18  6 19  2  4 20 16  5 15  7 12 10  9 11  1 14 13  8\n\nsorted_numbers &lt;- sort(unsorted_numbers, decreasing = TRUE)\nsorted_numbers\n\n [1] 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3  2  1"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#list",
    "href": "slides/02_The_Essentials_of_R_Programming.html#list",
    "title": "Data Science and Data Analytics",
    "section": "List",
    "text": "List\nLists are a step up in complexity from atomic vectors: each element can be any type, not just vectors. We construct lists with the function list:\n\nl1 &lt;- list(1:3, \"R\", c(TRUE, FALSE, TRUE), c(2.3, 5.9))\n\nl1\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] \"R\"\n\n[[3]]\n[1]  TRUE FALSE  TRUE\n\n[[4]]\n[1] 2.3 5.9\n\ntypeof(l1)\n\n[1] \"list\"\n\n\nLists are sometimes called recursive vectors because a list can contain other lists. This makes them fundamentally different from atomic vectors."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#list-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#list-1",
    "title": "Data Science and Data Analytics",
    "section": "List",
    "text": "List\nThe elements of a list can also have names. These can be accessed with the help of the $ operator. Alternatively, to access a single element of a list, we can also use double square brackets [[]]:\n\nnames(l1) &lt;- c(\"A\", \"B\", \"C\", \"D\")\nl1\n\n$A\n[1] 1 2 3\n\n$B\n[1] \"R\"\n\n$C\n[1]  TRUE FALSE  TRUE\n\n$D\n[1] 2.3 5.9\n\nl1$A\n\n[1] 1 2 3\n\nl1[[3]]\n\n[1]  TRUE FALSE  TRUE\n\nl1[[\"D\"]]\n\n[1] 2.3 5.9"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-lists-subsetting",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-lists-subsetting",
    "title": "Data Science and Data Analytics",
    "section": "Working with lists – Subsetting",
    "text": "Working with lists – Subsetting\nFor subsetting lists, essentially the same rules apply as for atomic vectors, i.e. we can subset using numeric or logical arguments and single square brackets []:\n\n# Numeric subsetting\nl1[1:2] # First two elements\n\n$A\n[1] 1 2 3\n\n$B\n[1] \"R\"\n\nl1[-(1:2)] # All elements except the first\n\n$C\n[1]  TRUE FALSE  TRUE\n\n$D\n[1] 2.3 5.9\n\n# Logical subsetting\nl1[c(FALSE, FALSE, FALSE, TRUE)]\n\n$D\n[1] 2.3 5.9"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#matrix",
    "href": "slides/02_The_Essentials_of_R_Programming.html#matrix",
    "title": "Data Science and Data Analytics",
    "section": "Matrix",
    "text": "Matrix\nOne way to construct more complex data structures on top of elementary building blocks like vectors or lists is to assign a class to them. A class is metadata about the object that can determine how common functions operate on that object.\n\nProbably the easiest example of this is a matrix. A matrix is just a two-dimensional array of numbers:\n\nm &lt;- matrix(1:12, nrow = 3, ncol = 4)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe function matrix will fill up the matrix column by column by default. There is an option to fill them by row instead, see ?matrix."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#matrix-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#matrix-1",
    "title": "Data Science and Data Analytics",
    "section": "Matrix",
    "text": "Matrix\nTechnically speaking, in R, a matrix is actually just a vector with an attribute that specifies the dimensions (i.e. number of rows and columns) of the matrix. We can access the dimensions of a matrix with the help of the functions dim, nrow and ncol:\n\ntypeof(m) # just an integer vector behind the scenes\n\n[1] \"integer\"\n\ndim(m) # but dimensions are 3 x 4 (3 rows, 4 columns)\n\n[1] 3 4\n\nnrow(m) # number of rows\n\n[1] 3\n\nncol(m) # number of columns\n\n[1] 4\n\nclass(m) # It is of class matrix and array\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#matrix-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#matrix-2",
    "title": "Data Science and Data Analytics",
    "section": "Matrix",
    "text": "Matrix\nWe can add additional rows or columns with the help of rbind or cbind:\n\nm &lt;- cbind(m, c(13, 14, 15))\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    4    7   10   13\n[2,]    2    5    8   11   14\n[3,]    3    6    9   12   15\n\nm &lt;- rbind(rep(1, 5), m)\nm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    1    1    1\n[2,]    1    4    7   10   13\n[3,]    2    5    8   11   14\n[4,]    3    6    9   12   15\n\n\n\nFor a matrix, we can set both row names and column names:\n\nrownames(m) &lt;- c(\"row_1\", \"row_2\", \"row_3\", \"row_4\")\ncolnames(m) &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\")\nm\n\n      A B C  D  E\nrow_1 1 1 1  1  1\nrow_2 1 4 7 10 13\nrow_3 2 5 8 11 14\nrow_4 3 6 9 12 15"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-matrices-subsetting",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-matrices-subsetting",
    "title": "Data Science and Data Analytics",
    "section": "Working with matrices – Subsetting",
    "text": "Working with matrices – Subsetting\nAs a matrix is a two-dimensional object, we need two indices separated by a comma for subsetting. If we do not specify one dimension, all elements across that dimension are selected:\n\nm[2, ] # 2nd row\n\n A  B  C  D  E \n 1  4  7 10 13 \n\nm[, 3] # 3rd column\n\nrow_1 row_2 row_3 row_4 \n    1     7     8     9 \n\nm[3, 4] # 3rd element of the 4th column\n\n[1] 11\n\nm[c(FALSE, FALSE, FALSE, TRUE), ] # 4th row\n\n A  B  C  D  E \n 3  6  9 12 15 \n\n\n\nWe can also subset by name if column and / or row names are provided:\n\nm[\"row_4\", \"E\"]\n\n[1] 15"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#data-frame",
    "href": "slides/02_The_Essentials_of_R_Programming.html#data-frame",
    "title": "Data Science and Data Analytics",
    "section": "Data Frame",
    "text": "Data Frame\nAs a matrix is basically just a vector underneath, all values still need to be of the same data type. However, real tabular data often includes data of different types (e.g. name, height, education). To represent this, we need a data structure where the columns can be of different types.\n\nIn R, such a data structure is provided by the data.frame. Underlying it is a named list, whose elements represent columns. Therefore, all elements need to have the same length. Consider the following example:\n\nheights &lt;- c(176, 178, 156)\nnames &lt;- c(\"Anna\", \"Jakob\", \"Lisa\")\neduc &lt;- c(\"BSc\", \"MA\", \"PhD\")\ndf &lt;- data.frame(heights = heights, names = names, educ = educ)\ndf\n\n  heights names educ\n1     176  Anna  BSc\n2     178 Jakob   MA\n3     156  Lisa  PhD"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#data-frame-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#data-frame-1",
    "title": "Data Science and Data Analytics",
    "section": "Data Frame",
    "text": "Data Frame\n\nLet’s inspect the “list nature” of the data.frame object df we just created:\n\ntypeof(df)\n\n[1] \"list\"\n\nclass(df)\n\n[1] \"data.frame\"\n\n\n\n\n\nEven though df is fundamentally a list, it behaves differently than a standard list:\n\ndf\n\n  heights names educ\n1     176  Anna  BSc\n2     178 Jakob   MA\n3     156  Lisa  PhD\n\nas.list(df)\n\n$heights\n[1] 176 178 156\n\n$names\n[1] \"Anna\"  \"Jakob\" \"Lisa\" \n\n$educ\n[1] \"BSc\" \"MA\"  \"PhD\"\n\n\nThis is because, on top of being a list, the object df is of class data.frame, which changes how common functions operate on it."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-data-frames",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-data-frames",
    "title": "Data Science and Data Analytics",
    "section": "Working with Data Frames",
    "text": "Working with Data Frames\nSince a data.frame also represents two-dimensional data, many of the functions we used for matrices work on it as well:\n\ndim(df)\n\n[1] 3 3\n\nnrow(df)\n\n[1] 3\n\ncolnames(df)\n\n[1] \"heights\" \"names\"   \"educ\"   \n\n\n\nIn particular, all the subsetting methods we showed are also applicable:\n\ndf[1, ]\n\n  heights names educ\n1     176  Anna  BSc\n\ndf[, 2]\n\n[1] \"Anna\"  \"Jakob\" \"Lisa\" \n\ndf[c(TRUE, TRUE, FALSE), 3]\n\n[1] \"BSc\" \"MA\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-data-frames-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-data-frames-1",
    "title": "Data Science and Data Analytics",
    "section": "Working with Data Frames",
    "text": "Working with Data Frames\nHowever, as data.frames are lists underneath, we can also use the $ operator to access the elements of the list as before.\n\ndf$heights\n\n[1] 176 178 156\n\ndf$names\n\n[1] \"Anna\"  \"Jakob\" \"Lisa\" \n\ndf$educ\n\n[1] \"BSc\" \"MA\"  \"PhD\"\n\n\n\nThis is a particularly common action when working with real data, as it allows us to access the variables in the columns of the data.frame. For example, let’s say we made a mistake and found out that Jakob was really 187 cm tall. We could change the corresponding data point as follows:\n\ndf$heights[2] &lt;- 187\ndf\n\n  heights names educ\n1     176  Anna  BSc\n2     187 Jakob   MA\n3     156  Lisa  PhD"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#factor",
    "href": "slides/02_The_Essentials_of_R_Programming.html#factor",
    "title": "Data Science and Data Analytics",
    "section": "Factor",
    "text": "Factor\nA factor is R’s way to represent categorical data. Say, for example, we want to add the sex of the three people in our data set to the data.frame. For this, we create a factor and add it as an additional column to df:\n\nsex &lt;- factor(c(\"f\", \"m\", \"f\"))\ndf$sex &lt;- sex\ndf\n\n  heights names educ sex\n1     176  Anna  BSc   f\n2     187 Jakob   MA   m\n3     156  Lisa  PhD   f\n\n\n\nA factor has levels that represent the categories that this variable can take (here: “m” for male and “f” for female). In the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters.\n\ndf$sex\n\n[1] f m f\nLevels: f m"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#factor-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#factor-1",
    "title": "Data Science and Data Analytics",
    "section": "Factor",
    "text": "Factor\nA factor is another example of a data structure that is built on top of an atomic vector using a class attribute. The data is stored as an integer vector (data type), but because the object is of class factor, R uses different methods to act on the object (compared to a standard integer vector). This type of behaviour is one of the things that makes R very powerful for data science.\n\ntypeof(df$sex)\n\n[1] \"integer\"\n\nas.integer(df$sex)\n\n[1] 1 2 1\n\nclass(df$sex)\n\n[1] \"factor\"\n\ndf$sex\n\n[1] f m f\nLevels: f m\n\n\nAs we will see, properly encoding categorical data as factors is an essential step in data preparation."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#date",
    "href": "slides/02_The_Essentials_of_R_Programming.html#date",
    "title": "Data Science and Data Analytics",
    "section": "Date",
    "text": "Date\nFinally, a common type of data that needs to be represented in R are dates. Dates are typically represented in some sort of format like “DD-MM-YYYY” in the European context, for instance. Let’s say, we are given Anna’s, Jakob’s and Lisa’s birthday in different formats:\n\nbday_anna &lt;- \"1999-12-01\" # YYYY-MM-DD\nbday_jakob &lt;- \"16.4.98\" # MM.DD.YYYY\nbday_lisa &lt;- \"7/June/1995\" # DD/MM/YYYY\n\n\nR has a specific way of recognizing such date formats using so called format strings. For each way of representing date information, there is a corresponding format string. Some important ones are:\n\n\n\n\n\n\n\n\n\n\nFormat string\nDescription\nFormat string\nDescription\n\n\n\n\n%Y\nYear with century\n%y\nYear without century\n\n\n%m\nMonth of year (01-12)\n%j\nDay of the year (0-366)\n\n\n%d\nDay of month (01-31)\n%W\nCalendar week (0-53)\n\n\n%B\nFull month (e.g. June)\n%b\nAbbreviated month (e.g. Jun)"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates",
    "title": "Data Science and Data Analytics",
    "section": "Working with dates",
    "text": "Working with dates\nSo let’s try to use this to convert our dates (which are now just character objects) to actual dates that R understands using as.Date:\n\nbday_anna &lt;- as.Date(\"1999-12-01\", \"%Y-%m-%d\")\nbday_jakob &lt;- as.Date(\"16.4.98\", \"%d.%m.%y\")\nbday_lisa &lt;- as.Date(\"7/June/1995\", \"%d/%B/%Y\")\nbday_anna\n\n[1] \"1999-12-01\"\n\nbday_jakob\n\n[1] \"1998-04-16\"\n\nbday_lisa\n\n[1] \"1995-06-07\"\n\n\n\nNow, we can add these dates as a column to our data.frame\n\ndf$bday &lt;- c(bday_anna, bday_jakob, bday_lisa)\ndf\n\n  heights names educ sex       bday\n1     176  Anna  BSc   f 1999-12-01\n2     187 Jakob   MA   m 1998-04-16\n3     156  Lisa  PhD   f 1995-06-07"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-1",
    "title": "Data Science and Data Analytics",
    "section": "Working with dates",
    "text": "Working with dates\nDates are stored in R as the number of days since Unix time, which is the 1st January, 1970. Hence, date vectors are simply double vectors of class Date.\n\ntypeof(df$bday)\n\n[1] \"double\"\n\nas.double(df$bday)\n\n[1] 10926 10332  9288\n\nclass(df$bday)\n\n[1] \"Date\"\n\n\n\nWith this underlying representation, we can do maths on dates:\n\ndf$bday\n\n[1] \"1999-12-01\" \"1998-04-16\" \"1995-06-07\"\n\ndf$bday + 30\n\n[1] \"1999-12-31\" \"1998-05-16\" \"1995-07-07\"\n\ndf$bday[2] - df$bday[1]\n\nTime difference of -594 days"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-2",
    "title": "Data Science and Data Analytics",
    "section": "Working with dates",
    "text": "Working with dates\nNote that these operations cannot be performed on the original character strings representing the birthdays:\n\n\"1999-12-01\" + 10\n\nError in \"1999-12-01\" + 10: nicht-numerisches Argument für binären Operator\n\n\n\nTo extract individual parts of a date (like year, month, day, weekday, etc.), we can use the format function in conjunction with the format strings we saw before for date creation:\n\nformat(df$bday, \"%Y\") # extract birth year\n\n[1] \"1999\" \"1998\" \"1995\"\n\nformat(df$bday, \"%B\") # extract birth month\n\n[1] \"December\" \"April\"    \"June\"    \n\nformat(df$bday, \"%A\") # extract week day of birth date\n\n[1] \"Wednesday\" \"Thursday\"  \"Wednesday\""
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#working-with-dates-3",
    "title": "Data Science and Data Analytics",
    "section": "Working with dates",
    "text": "Working with dates"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#r-programming-for-data-science",
    "href": "slides/02_The_Essentials_of_R_Programming.html#r-programming-for-data-science",
    "title": "Data Science and Data Analytics",
    "section": "R programming for Data Science",
    "text": "R programming for Data Science\n\nBy coding in R, we can efficiently perform exploratory data analysis, build machine learning pipelines, and prepare beautiful plots to communicate results effectively using data visualization tools.\nHowever, R is not just a data analysis environment but a programming language.\nThe goal of this is course is not to teach advanced R programming, but in order to facilitate our lives as data scientists, we do need access to certain core programming principles like conditionals, loops and functionals.\nSo let’s dive right in!"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#conditionals",
    "href": "slides/02_The_Essentials_of_R_Programming.html#conditionals",
    "title": "Data Science and Data Analytics",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals are one of the basic features of programming. They are used for what is called control flow. The most common conditional expression is the if-else statement. It’s best illustrated with an example:\n\na &lt;- 0\nif (a != 0) {\n  print(1/a)\n} else{\n  print(\"Can't divide by zero!\")\n}\n\n[1] \"Can't divide by zero!\"\n\n\n\nThe condition a != 0 was evaluated to be FALSE (since a was assigned 0). So, the conditional went to the else statement and printed “Can’t divide by zero!” to the console as we told it to. By contrast:\n\na &lt;- 4\nif (a != 0) {\n  print(1/a)\n} else{\n  print(\"Can't divide by zero!\")\n}\n\n[1] 0.25"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#conditionals-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#conditionals-1",
    "title": "Data Science and Data Analytics",
    "section": "Conditionals",
    "text": "Conditionals\nLet’s incorporate conditionals into our quadratic_solver function from before. Remember that a quadratic equation has no real solutions if the discriminant \\(D\\) is negative (because of the root in the “midnight” formula):\n\\[D = b^2 - 4ac\\]\n\nSo far, when this is case, our function produces NaNs and a warning:\n\nquadratic_solver(2, 2, 2)\n\nWarning in sqrt(b^2 - 4 * a * c): NaNs wurden erzeugt\nWarning in sqrt(b^2 - 4 * a * c): NaNs wurden erzeugt\n\n\n[1] NaN NaN\n\n\n\n\nLet’s fix this by:\n\nReturning no solutions / NA if \\(D &lt; 0\\).\nReturning one solution if \\(D = 0\\).\nReturning two solutions (as before) if \\(D &gt; 0\\)."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#conditionals-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#conditionals-2",
    "title": "Data Science and Data Analytics",
    "section": "Conditionals",
    "text": "Conditionals\nOne solution could be the following:\n\nquadratic_solver_new &lt;- function(a, b, c){\n  D &lt;- b^2 - 4*a*c\n  if(D &lt; 0){\n    print(\"Discriminant is negative, no real solutions!\")\n    return(NA)\n  } else if(D &gt; 0) {\n    print(\"Discriminant is positive, two real solutions!\")\n    sol_1 &lt;- (-b + sqrt(D))/(2*a)\n    sol_2 &lt;- (-b - sqrt(D))/(2*a)\n    return(c(sol_1, sol_2))\n  } else {\n    print(\"Discriminant is zero, one real solution!\")\n    sol &lt;- -b/(2*a)\n    return(sol)\n  }\n}\n\n\n\n\n\nquadratic_solver_new(2, 2, 2)\n\n[1] \"Discriminant is negative, no real solutions!\"\n\n\n[1] NA\n\nquadratic_solver_new(3, -5, 1)\n\n[1] \"Discriminant is positive, two real solutions!\"\n\n\n[1] 1.4342585 0.2324081\n\n\n\n\nquadratic_solver_new(1, 2, 1)\n\n[1] \"Discriminant is zero, one real solution!\"\n\n\n[1] -1"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#conditionals-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#conditionals-3",
    "title": "Data Science and Data Analytics",
    "section": "Conditionals",
    "text": "Conditionals\nif-else statements like the ones we saw only work on a single logical. For a vectorized version, there is the very useful ifelse function:\n\na &lt;- c(0, 2, 4, 6, 0, 8)\nifelse(a != 0, 1/a, NA)\n\n[1]        NA 0.5000000 0.2500000 0.1666667        NA 0.1250000\n\n\n\nThis function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. When operating on vectors, ifelse takes the corresponding elements of the second or third argument.\nHere’ another example:\n\nx &lt;- 1:10\nifelse(x %% 2 == 0, \"even\", \"odd\")\n\n [1] \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\" \"odd\"  \"even\"\n\n\nNote: x %% 2 gives the remainder when dividing x by 2."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#for-loops",
    "href": "slides/02_The_Essentials_of_R_Programming.html#for-loops",
    "title": "Data Science and Data Analytics",
    "section": "For-loops",
    "text": "For-loops\nIn general, loops control flow structures that enable the repeated execution of a code block as long as a specified condition is met. This saves a lot of manual work and code duplication. We will discuss two types of loops: for loops and while loops.\n\nfor loops are used to iterate over items in a vector. The logic is “for every item in this vector, do the following”. This logic is implemented in the following basic form:\n\nfor(item in vector) {\n  perform_action\n}\n\n\n\nFollowing this notation, we can refer to the element of the vector in the current loop cycle with the name of item. In the first cycle, it will be the first element of the vector, in the second, it will be the second, and so on…"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#for-loops-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#for-loops-1",
    "title": "Data Science and Data Analytics",
    "section": "For-loops",
    "text": "For-loops\nLet’s see an example of a for loop that prints both solutions to the quadratic equation from before in a nice format:\n\nsolutions &lt;- quadratic_solver_new(3, -5, 1)\n\n[1] \"Discriminant is positive, two real solutions!\"\n\nn &lt;- length(solutions)\n\nfor(i in 1:n){\n  print(paste0(\"Solution \", i, \": \", round(solutions[i], 3)))\n}\n\n[1] \"Solution 1: 1.434\"\n[1] \"Solution 2: 0.232\"\n\n\n\nNote that we do not have to loop through an integer vector, we can loop through any atomic vector or list. For example, we could loop through all the elements of the list l1 from earlier and print it to the console only if it is a numeric vector:\n\nfor(el in l1){\n  if(is.numeric(el)){\n    print(el)\n  }\n}\n\n[1] 1 2 3\n[1] 2.3 5.9"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#while-loops",
    "href": "slides/02_The_Essentials_of_R_Programming.html#while-loops",
    "title": "Data Science and Data Analytics",
    "section": "While-loops",
    "text": "While-loops\nAnother type of loop is a while loop. It repeats a specified action as long as a certain condition is met. It has the following basic form:\n\nwhile(condition){\n  perform_action\n}\n\n\nNote that for and while loops are interchangeable in every circumstance, i.e. every for loop can be implemented as a while loop and vice versa. Usually, the choice between the two is a question of code readability and efficiency considerations. Compare the following two examples:\n\n\n\ni &lt;- 1L\nwhile(i &lt;= 3L){\n  print(i)\n  i &lt;- i + 1L\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\nNote that the last step in the loop is crucial. Otherwise we loop infinitely!\n\n\nfor(i in 1:3){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#functionals",
    "href": "slides/02_The_Essentials_of_R_Programming.html#functionals",
    "title": "Data Science and Data Analytics",
    "section": "Functionals",
    "text": "Functionals\nIn R, a very commonly used alternative to loops is the use of functionals. Functionals are functions that take another function as an input and returns a vector as output. Common functionals we will have a look at are lapply, and apply. Let’s start with lapply.\n\nlapply requires as arguments an atomic vector or a list and a function that it should apply to each element of that atomic vector or list. Let’s say we wanted to sort each vector in a list of vectors. We could do:\n\nl2 &lt;- list(sample(1:10), sample(1:20))\nl2\n\n[[1]]\n [1] 10  9  5  4  7  1  3  6  8  2\n\n[[2]]\n [1]  5 14 20  2 17  8  1 16 18 15  7 11 12  4 19 10  3 13  9  6\n\nlapply(l2, sort)\n\n[[1]]\n [1]  1  2  3  4  5  6  7  8  9 10\n\n[[2]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20"
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#functionals-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#functionals-1",
    "title": "Data Science and Data Analytics",
    "section": "Functionals",
    "text": "Functionals\nIn fact, we can pass named arguments that we would usually pass to the function to be applied directly to lapply instead:\n\nlapply(l2, sort, decreasing = TRUE)\n\n[[1]]\n [1] 10  9  8  7  6  5  4  3  2  1\n\n[[2]]\n [1] 20 19 18 17 16 15 14 13 12 11 10  9  8  7  6  5  4  3  2  1\n\n\n\nInstead of using lapply with a pre-existing (or user-defined) function, we can also create an inline function that exists only for the purpose of that lapply call. For example, we might want to square each vector in the list after sorting:\n\nlapply(l2, function(x) sort(x, decreasing = TRUE)^2)\n\n[[1]]\n [1] 100  81  64  49  36  25  16   9   4   1\n\n[[2]]\n [1] 400 361 324 289 256 225 196 169 144 121 100  81  64  49  36  25  16   9   4\n[20]   1\n\n\nSuch functions are called anonymous functions as they do not have a name."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#functionals-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#functionals-2",
    "title": "Data Science and Data Analytics",
    "section": "Functionals",
    "text": "Functionals\nThe matrix equivalent of lapply is called apply. It can apply a given function to every row and / or every column of a matrix. Besides requiring the matrix and the function to be applied, it requires an indication of whether application should happen over rows (1) or columns (2). Consider the following example to compute the row and column sums of a matrix:\n\nm &lt;- matrix(1:9, 3, 3)\nm\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\napply(m, 1, sum)\n\n[1] 12 15 18\n\napply(m, 2, sum)\n\n[1]  6 15 24\n\n\n\nThe use of functionals like lapply and apply is usually preferable over loops because it pre-specifies what should happen with the result (e.g. lapply will always hold the results in a list)."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#packages-1",
    "href": "slides/02_The_Essentials_of_R_Programming.html#packages-1",
    "title": "Data Science and Data Analytics",
    "section": "Packages",
    "text": "Packages\n\nWith the base installation, R comes with a lot of functionality already. However, statisticians and data scientists all over the world constantly come up with new methods and clever code to solve problems.\nTo share this code, they create R packages: these include fundamental units of reproducible R code, including reusable R functions, the documentation that describes how to use them, and sample data.\nTo make these packages available, the creators typically upload them to the Comprehensive R Archive Network (CRAN).\nFrom there, any R user can download and install these packages and use them for their own workflows."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#packages-2",
    "href": "slides/02_The_Essentials_of_R_Programming.html#packages-2",
    "title": "Data Science and Data Analytics",
    "section": "Packages",
    "text": "Packages\n\nAs of 22nd February 2025, there are 22,100 R packages available for download on CRAN. Each of them has a unique name that we can use to obtain it.\nWe will be working only with a select few of them.\nOne of them will be ggplot2, which is one of the most popular R packages used for data visualization.\nAs an R programmer, one typically finds packages that solve a given problem by googling the problem one is encountering. Chances are that someone else has had this problem before…\n\n\nTo install a package, we call the function install.packages on the name of the package:\n\ninstall.packages(\"ggplot2\")\n\nThis downloads and installs the package on your computer."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#packages-3",
    "href": "slides/02_The_Essentials_of_R_Programming.html#packages-3",
    "title": "Data Science and Data Analytics",
    "section": "Packages",
    "text": "Packages\nAs with any software, an R package only needs to be installed once.\n\nAfter installation, we need to tell R to make the functions provided by this package available to us in our current R session. For this we call the function library on the name of the package:\n\nlibrary(ggplot2)\n\n\nNow, all data and functions offered by this package can be used.\nA package comes with a reference manual and often additional documentation in the form of so called package vignettes.\nThis helps the user understand how to use the functions in the package.\nAdditionally, documentation on individual functions is always available through the help of the ? operator."
  },
  {
    "objectID": "slides/02_The_Essentials_of_R_Programming.html#packages-4",
    "href": "slides/02_The_Essentials_of_R_Programming.html#packages-4",
    "title": "Data Science and Data Analytics",
    "section": "Packages",
    "text": "Packages\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point() + \n  xlab(\"Engine Displacement\") + ylab(\"MPG\") + \n  theme(legend.text = element_text(size=14), axis.text = element_text(size=14), axis.title = element_text(size=14))"
  },
  {
    "objectID": "slides/06_penguins_presentation.html#meet-the-penguins",
    "href": "slides/06_penguins_presentation.html#meet-the-penguins",
    "title": "Palmer Penguins",
    "section": "Meet the penguins",
    "text": "Meet the penguins\n\nFor this analysis, we will use the penguins data set from the palmerpenguins package. The website belonging to this package can be found here.\nThe data set comprises measurements of three types of penguins, namely:\n\nAdelie 🐧\nChinstrap 🐧\nGentoo 🐧"
  },
  {
    "objectID": "slides/06_penguins_presentation.html#meet-the-penguins-1",
    "href": "slides/06_penguins_presentation.html#meet-the-penguins-1",
    "title": "Palmer Penguins",
    "section": "Meet the penguins",
    "text": "Meet the penguins\nFigure 1 shows a scatter plot of the flipper length against the body mass of all penguins in the data set:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n\nFigure 1: A scatter plot of flipper length against body mass"
  },
  {
    "objectID": "slides/00_Getting_started.html#welcome-to-data-science-and-data-analytics",
    "href": "slides/00_Getting_started.html#welcome-to-data-science-and-data-analytics",
    "title": "Data Science and Data Analytics",
    "section": "Welcome to Data Science and Data Analytics!",
    "text": "Welcome to Data Science and Data Analytics!\n\nThis is B-GV-12 Data Science and Data Analytics.\nThe goal of this course is to teach you…\n\nabout the nature and role of data science in a data-driven world.\nabout the data science workflow.\nto use open-source software to analyse, visualize and model data from various sources.\nabout different machine learning algorithms.\nto implement your own data science projects.\nto communicate the results of your analyses effectively.\nand much more…"
  },
  {
    "objectID": "slides/00_Getting_started.html#lectures",
    "href": "slides/00_Getting_started.html#lectures",
    "title": "Data Science and Data Analytics",
    "section": "Lectures",
    "text": "Lectures\n\nWith a few exceptions, lectures will be held on Fridays from 13:30 to 16:30.\nPlease regularly check your course schedule to not miss any lectures.\nLectures will consist of theory and practice discussed with the help of slides as well as live coding sessions.\nYou are highly encouraged to actively participate!\nExercises to practice what you have learned will be provided (but not graded)."
  },
  {
    "objectID": "slides/00_Getting_started.html#grading",
    "href": "slides/00_Getting_started.html#grading",
    "title": "Data Science and Data Analytics",
    "section": "Grading",
    "text": "Grading\n\nThere will be no final exam in this course!\nInstead, grading will be based on group projects:\n\nTeams of 4 - 5 students will initiate and design a small data science project autonomously.\nEach group will:\n\nidentify a business case mimicking a real-world research problem and an accompanying available data set.\nformulate research questions on the basis of the chosen data set.\nperform analyses using the concepts and methods learned throughout this course."
  },
  {
    "objectID": "slides/00_Getting_started.html#grading-1",
    "href": "slides/00_Getting_started.html#grading-1",
    "title": "Data Science and Data Analytics",
    "section": "Grading",
    "text": "Grading\n\nGrading will therefore be based on the following components:\n\nGroup project report (5-10 pages per group member): 65 %\nGroup presentation (\\(\\leq\\) 20 mins/group and \\(\\geq\\) 2 mins/group member): 25 %\nPeer review: 10 %\n\nIn line with the usual grading scheme, grades will be given as follows:\n\n\n\n\n\n\n\n\nPercentage\nGrade\n\n\n\n\n95 - 100 %\n1,0\n\n\n90 - 94 %\n1,3\n\n\n85 - 89 %\n1,7\n\n\n80 - 84 %\n2,0\n\n\n75 - 79 %\n2,3\n\n\n70 - 74 %\n2,7\n\n\n\n\n\n\n\nPercentage\nGrade\n\n\n\n\n65 - 69 %\n3,0\n\n\n60 - 64 %\n3,3\n\n\n55 - 59 %\n3,7\n\n\n50 - 54 %\n4,0\n\n\nbelow 50 %\n5,0"
  },
  {
    "objectID": "slides/00_Getting_started.html#grading-group-project",
    "href": "slides/00_Getting_started.html#grading-group-project",
    "title": "Data Science and Data Analytics",
    "section": "Grading: Group project",
    "text": "Grading: Group project\n\nThe structure of the group project (reflected in report and presentation) should be something like this:\n\nIntroduction / Motivation and research question\nData (sources, description, statistics, visualizations, …)\nModels and model evaluation\nResults\nDiscussion and comments\n\nAspects that will influence your grade will be: the originality of the question, understanding of the business case, data and methods, correctness of application, thoroughness of evaluation, creativity and quality of report and presentation (both verbal and visual)\nDeductions will be made for purely AI-generated contributions."
  },
  {
    "objectID": "slides/00_Getting_started.html#grading-group-project-1",
    "href": "slides/00_Getting_started.html#grading-group-project-1",
    "title": "Data Science and Data Analytics",
    "section": "Grading: Group project",
    "text": "Grading: Group project\n\n\nChoice of topic: while you are completely free in your choice of topic in the group, here are some areas of suggestion:\n\nFinance / Economics / Marketing\nText analysis\nEntertainment (in particular: movies and music)\nSocial network analysis\nSocial sciences\n\n\nSources for data sets: while you are again free also in your choice of data set, good places to get you started are:\n\nStatistik Austria\nKaggle\nUCI Machine learning repository\nWorld bank\nEU\n…\n\n\n\n\n\n\n\n\n\nCaution\n\n\nWhen selecting a data set, make sure, you are allowed to use this data for the purposes of your project! When selecting from the sources given, this should generally be ensured."
  },
  {
    "objectID": "slides/00_Getting_started.html#grading-peer-review",
    "href": "slides/00_Getting_started.html#grading-peer-review",
    "title": "Data Science and Data Analytics",
    "section": "Grading: Peer review",
    "text": "Grading: Peer review\n\n\nAfter final project presentations, each individual student will be asked to write a peer review of one of the other groups’ projects.\nEach student will be randomly assigned two projects, out of which one should be reviewed (based on their presentation only).\nEvaluation is based on the quality of review that you write, not on the feedback that your project receives.\nThe review should be max 250 words answering the following questions:\n\nBriefly describe the topic, research questions and the employed methods.\nState and briefly explain two positive comments about the work.\nState and briefly explain two improvement suggestions.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nIn your peer review, focus on content, not on the formatting or quality of the slides, for instance."
  },
  {
    "objectID": "slides/00_Getting_started.html#schedule",
    "href": "slides/00_Getting_started.html#schedule",
    "title": "Data Science and Data Analytics",
    "section": "Schedule",
    "text": "Schedule\n\n\nMarch\n14th: Getting started, Introduction to Data Science (DS)\n21st: The essentials of R programming\n31st: The DS workflow – Part I: Import, Tidy and Transform\n\nApril\n4th/11th: The DS workflow – Part II: Visualize\n30th: The DS workflow – Part III: Model\n\nMay\n6th/9th/16th/23rd: The DS workflow – Part III: Model\n28th: The DS workflow – Part IV: Communicate\n\nJune\n6th: Buffer session\n13th: Final presentations of the group projects\n\n\n\n\n\n\n\nDeadlines\n\n\n\n21st March: organize in teams, send team members via e-mail\n12th June: Send presentations via e-mail\n27th June: Hand in group project reports and peer reviews"
  },
  {
    "objectID": "slides/00_Getting_started.html#questions-and-contact",
    "href": "slides/00_Getting_started.html#questions-and-contact",
    "title": "Data Science and Data Analytics",
    "section": "Questions and contact",
    "text": "Questions and contact\n\nAny questions?\n\n\n\nContact:\n\nAnytime via e-mail: julianamonphd@gmail.com"
  },
  {
    "objectID": "slides/00_Getting_started.html#software-excel",
    "href": "slides/00_Getting_started.html#software-excel",
    "title": "Data Science and Data Analytics",
    "section": "Software – Excel? ❌",
    "text": "Software – Excel? ❌"
  },
  {
    "objectID": "slides/00_Getting_started.html#software-r",
    "href": "slides/00_Getting_started.html#software-r",
    "title": "Data Science and Data Analytics",
    "section": "Software – R ✅",
    "text": "Software – R ✅"
  },
  {
    "objectID": "slides/00_Getting_started.html#software-rstudio",
    "href": "slides/00_Getting_started.html#software-rstudio",
    "title": "Data Science and Data Analytics",
    "section": "Software – RStudio ✅",
    "text": "Software – RStudio ✅"
  },
  {
    "objectID": "slides/00_Getting_started.html#software-quarto",
    "href": "slides/00_Getting_started.html#software-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Software – Quarto ✅",
    "text": "Software – Quarto ✅"
  },
  {
    "objectID": "slides/00_Getting_started.html#software",
    "href": "slides/00_Getting_started.html#software",
    "title": "Data Science and Data Analytics",
    "section": "Software",
    "text": "Software\n\nModern data science is unthinkable without computer programming: typically, either Python or R is used.\nFor the purposes of this course, we will use:\n\nThe open-source statistical programming language R.\nA bespoke integrated development environment (IDE) for R called RStudio.\nAn authoring framework for creating beautiful reports, presentations, web sites, etc., combining text, code, results and visualizations, called Quarto.\n\nUntil next time, therefore please\n\neither install R, RStudio and Quarto on your laptop (recommended) or\nregister for a free account at Posit Cloud."
  },
  {
    "objectID": "slides/00_Getting_started.html#resources",
    "href": "slides/00_Getting_started.html#resources",
    "title": "Data Science and Data Analytics",
    "section": "Resources",
    "text": "Resources\n\nPrimarily: slides and exercises provided\nHowever, for a deeper dive and additional materials, I recommend:\n\n\n\nFor R programming\n  Excellent courses from Harvard Professor Rafael Irizarry, available for free here and here.\n\nFor machine learning\n Excellent book, available for free here."
  },
  {
    "objectID": "slides/00_Getting_started.html#resources-how-about-ai",
    "href": "slides/00_Getting_started.html#resources-how-about-ai",
    "title": "Data Science and Data Analytics",
    "section": "Resources – How about AI?",
    "text": "Resources – How about AI?\n\n\nWith the large-scale adoption of AI tools like ChatGPT, the way data scientists work is rapidly changing.\nThis course therefore actively encourages the use of AI tools for R programming. Here are some guidelines:\n\nUse ChatGPT for programming, not for writing the project report.\nDo not just copy-paste code generated by ChatGPT. Run it line-by-line, try to understand and edit as needed.\nEngineer your prompts until the response starts to look like code you are learning in this course.\nIf the response is not correct, ask for a correction.\n\nWith the arrival of AI, programming is becoming ever more accessible, but the need for people like you who actually understand the code they are running, is also increasing."
  },
  {
    "objectID": "slides/00_Getting_started.html#resources-how-about-ai-1",
    "href": "slides/00_Getting_started.html#resources-how-about-ai-1",
    "title": "Data Science and Data Analytics",
    "section": "Resources – How about AI?",
    "text": "Resources – How about AI?"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#the-data-science-workflow-import",
    "href": "slides/03_The_Data_Science_Workflow_I.html#the-data-science-workflow-import",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Import",
    "text": "The Data Science workflow – Import"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-into-r",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-into-r",
    "title": "Data Science and Data Analytics",
    "section": "Importing data into R",
    "text": "Importing data into R\n\nIn order to get our data science workflow started, we need to be able to import data from different sources into R.\nWhile there is a huge number of different data formats, we will focus on how to import data stored in two of the most common types of formats, namely:\n\nText files (like csv or tsv)\nSpreadsheets (Excel or Google Sheets)\n\nBefore we dive into how to get the data stored in such files into R, we need to be able to find these files on our computer in the first place…\nFor this, we first have to look into how R orients itself in the file system on our computer."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#the-file-system",
    "href": "slides/03_The_Data_Science_Workflow_I.html#the-file-system",
    "title": "Data Science and Data Analytics",
    "section": "The file system",
    "text": "The file system\n\nA computer’s file system consists of nested folders (directories). It can be visualized as tree structures, with directories branching out from the root.\nThe root directory contains all other directories.\nThe working directory is the current location in the filesystem."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#relative-and-full-paths",
    "href": "slides/03_The_Data_Science_Workflow_I.html#relative-and-full-paths",
    "title": "Data Science and Data Analytics",
    "section": "Relative and full paths",
    "text": "Relative and full paths\n\nA path lists directory names leading to a file. Think of it like instructions on what folders to click on, and in what order, to find the file. We distinguish:\n\nFull paths: Starts from the root directory, i.e. the very top of the file system hierarchy. An example would be:\n\n\nexample_path &lt;- system.file(package = \"dslabs\")\nexample_path\n\n[1] \"/home/julian/R/x86_64-pc-linux-gnu-library/4.5/dslabs\"\n\n\n\nRelative paths: Starts from the current working directory. Imagine the current working directory would be /home/julian, then the relative path to the folder above would simply be:\n\n\n\n[1] \"R/x86_64-pc-linux-gnu-library/4.5/dslabs\"\n\n\nIn R, we can use the list.files function to explore directories:\n\nlist.files(example_path)\n\n [1] \"data\"        \"DESCRIPTION\" \"extdata\"     \"help\"        \"html\"       \n [6] \"INDEX\"       \"Meta\"        \"NAMESPACE\"   \"R\"           \"script\""
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#relative-and-full-paths-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#relative-and-full-paths-1",
    "title": "Data Science and Data Analytics",
    "section": "Relative and full paths",
    "text": "Relative and full paths"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#the-working-directory",
    "href": "slides/03_The_Data_Science_Workflow_I.html#the-working-directory",
    "title": "Data Science and Data Analytics",
    "section": "The working directory",
    "text": "The working directory\n\nWhen referring to files in your R script, it is highly recommended that you use relative paths.\nReason: paths are unique to your computer, so if someone else runs your code on their computer, it will not find files whose location is described by absolute paths.\nTo determine the working directory of your current R session, you can type:\n\ngetwd()\n\n[1] \"/home/julian/Dokumente/Projekte/Lehre/CFPU Data Science und Data Analytics/cfpu-ds-da/slides\"\n\n\nTo change the working directory, use the function setwd:\n\nsetwd(\"/path/to/your/directory\")\n\nIn RStudio, you can alternatively also select the working directory via Session &gt; Set Working Directory."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#generating-path-names",
    "href": "slides/03_The_Data_Science_Workflow_I.html#generating-path-names",
    "title": "Data Science and Data Analytics",
    "section": "Generating path names",
    "text": "Generating path names\n\nDifferent operating systems have different conventions when specifying paths.\nFor example, Linux and Mac use forward slashes /, while Windows uses backslashes \\ to separate directories.\nThe R function file.path combines characters to form a complete path, automatically ensuring compatibility with the respective operating system.\nThis function is useful because we often want to define paths using a variable.\nConsider the following example:\n\ndir &lt;- system.file(package = \"dslabs\")\nfile.path(dir, \"extdata\", \"murders.csv\")\n\n[1] \"/home/julian/R/x86_64-pc-linux-gnu-library/4.5/dslabs/extdata/murders.csv\"\n\n\nHere the variable dir contains the full path for the dslabs package (needs to be installed!) and extdata/murders.csv is the relative path of a specific csv file in that folder."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files",
    "text": "Importing data from text files\n\nAll of us know text files. They are easy to open, can be easily read by humans and are easily transferable.\nWhen text files are used to store tabular data, line breaks are used to separate rows and a predefined character (the so-called delimiter) is used to separate columns within a row. Which one is used can depend on the file format:\n\n.csv (comma-separated values) typically uses comma (,) or semicolon (;).\n.tsv (tab-separated values) typically uses tab (which can be a preset number of spaces or \\t).\n.txt (“text”) can use any of the above or a simple space ( )\n\nHow we read text files into R depends on the delimiter used. Therefore, we need to have a look at the file to determine the delimiter."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-1",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files",
    "text": "Importing data from text files\nThis is the first couple of lines of the murders.csv text file from the dslabs package we saw referenced before. It contains the number of gun murders in each US state in the year 2010 as well as each state’s population. Clearly, it uses commas (,) as delimiter. Also note the use of a header in the first row."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.csv",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.csv",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .csv",
    "text": "Importing data from text files – .csv\n\nFor comma-delimited csv files, R offers the function read.csv to run on the (full or relative) path of the file. By default, it assumes that decimal points are used and that a header giving column names is present:\n\nargs(read.csv)\n\nfunction (file, header = TRUE, sep = \",\", quote = \"\\\"\", dec = \".\", \n    fill = TRUE, comment.char = \"\", ...) \nNULL\n\n\nFor semicolon-delimited csv files, R offers the function read.csv2 to run on the (full or relative) path of the file. By default, it assumes that decimal commas are used and that a header giving column names is present.\n\nargs(read.csv2)\n\nfunction (file, header = TRUE, sep = \";\", quote = \"\\\"\", dec = \",\", \n    fill = TRUE, comment.char = \"\", ...) \nNULL\n\n\nBoth of these functions return a data.frame containing the data from the file."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.csv-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.csv-1",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .csv",
    "text": "Importing data from text files – .csv\nAs murders.csv is comma-delimited, we use read.csv to read it into R:\n\ndir &lt;- system.file(package = \"dslabs\")\nmurders_df &lt;- read.csv(file.path(dir, \"extdata\", \"murders.csv\"))\nhead(murders_df, 6) # shows the first 6 rows of the data frame\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\n\nNote that the categorical variables (state, abb and region) are imported as character vectors:\n\nunlist(lapply(murders_df, typeof))\n\n      state         abb      region  population       total \n\"character\" \"character\" \"character\"   \"integer\"   \"integer\" \n\n\nFor data analysis purposes, we should probably turn these into factors. But for now, we are only interested in the successful import."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.tsv",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.tsv",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .tsv",
    "text": "Importing data from text files – .tsv\n\nFor tab-delimited tsv files, R offers the functions read.delim and read.delim2, again assuming the use of decimal points and decimal commas, respectively:\n\nargs(read.delim)\n\nfunction (file, header = TRUE, sep = \"\\t\", quote = \"\\\"\", dec = \".\", \n    fill = TRUE, comment.char = \"\", ...) \nNULL\n\nargs(read.delim2)\n\nfunction (file, header = TRUE, sep = \"\\t\", quote = \"\\\"\", dec = \",\", \n    fill = TRUE, comment.char = \"\", ...) \nNULL\n\n\nOtherwise, the use is identical to read.csv: the function requires the path to the file you want to import and returns a data.frame containing the (hopefully) correctly parsed data from the file."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.tsv-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.tsv-1",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .tsv",
    "text": "Importing data from text files – .tsv"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.txt",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-text-files-.txt",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from text files – .txt",
    "text": "Importing data from text files – .txt\n\nIn fact, all of the functions for importing data discussed so far are just interfaces to the R function read.table, which provides the most flexibility when importing data from text files:\n\nargs(read.table)\n\nfunction (file, header = FALSE, sep = \"\", quote = \"\\\"'\", dec = \".\", \n    numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"), row.names, \n    col.names, as.is = !stringsAsFactors, tryLogical = TRUE, \n    na.strings = \"NA\", colClasses = NA, nrows = -1, skip = 0, \n    check.names = TRUE, fill = !blank.lines.skip, strip.white = FALSE, \n    blank.lines.skip = TRUE, comment.char = \"#\", allowEscapes = FALSE, \n    flush = FALSE, stringsAsFactors = FALSE, fileEncoding = \"\", \n    encoding = \"unknown\", text, skipNul = FALSE) \nNULL\n\n\n(As always, to see the meaning of all of these arguments, see ?read.table)\nThis function is mostly used directly, when importing data from generic .txt files, where the format is often less strictly adhered to than in csv or tsv files."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#encoding",
    "href": "slides/03_The_Data_Science_Workflow_I.html#encoding",
    "title": "Data Science and Data Analytics",
    "section": "Encoding",
    "text": "Encoding\nNow, we know how to import data into R. So we download some data set, read it into R using the correct function, but then this happens…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#encoding-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#encoding-1",
    "title": "Data Science and Data Analytics",
    "section": "Encoding",
    "text": "Encoding\n\nSuch issues occur because of an incorrectly identified file encoding.\nEncoding refers to how the computer stores character strings as binary 0s and 1s. Examples of encoding systems are:\n\nASCII: uses 7 bits to represent symbols, enough for all English keyboard characters, but not much more…\nUnicode (especially UTF-8): the de-facto standard encoding of the internet, able to represent everything from the English alphabet to German Umlaute to Chinese characters and emojis.\n\nWhen reading a text file into R, its encoding needs to be known, otherwise the import either fails (see previous slide) or produces gibberish (e.g. German “Höhe” \\(\\to\\) “HÃ¶he”)."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#encoding-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#encoding-2",
    "title": "Data Science and Data Analytics",
    "section": "Encoding",
    "text": "Encoding\n\nRStudio typically uses UTF-8 as its default, which works in most cases. If it does not, you can use the guess_encoding function of the readr package to get insight into the encoding.\n\nlibrary(readr)\nweird_filepath &lt;- file.path(\"data\", \"calificaciones.csv\")\nas.data.frame(guess_encoding(weird_filepath))\n\n    encoding confidence\n1 ISO-8859-1       0.92\n2 ISO-8859-2       0.72\n3 ISO-8859-9       0.53\n\n\nThe function deems the ISO-8859-1 encoding to be the most likely encoding of the previous file. So we pass this value as the fileEncoding argument to read.csv2:\n\nhead(read.csv2(weird_filepath, sep = \",\", fileEncoding = \"ISO-8859-1\"), 3)\n\n    nombre                     f.n.             estampa puntuación\n1  Beyoncé 04 de septiembre de 1981 2023-09-22 02:11:02       87.5\n2 Blümchen      20 de abril de 1980 2023-09-22 03:23:05       99.0\n3     João      10 de junio de 1931 2023-09-21 22:43:28       98.9\n\n\nThis time, it worked! 😊"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\n\nAnother common way of sharing tabular data is through the use of spreadsheets, like Excel or Google Sheets. We will see how to import data in both of these types of documents.\nWith Excel, spreadsheets typically either have a .xls or .xlsx file suffix. Note that those are binary file formats, i.e. unlike text files, they are not human-readable when opened with a text editor.\nBase R does not have functionality to import data from Excel spreadsheets. However, the package readxl does. Its two main functions are:\n\nread_xls to read Excel spreadsheets with .xls ending and\nread_xlsx to read Excel spreadsheets with .xlsx ending.\n\nThese functions allow to select only certain areas of certain sheets, transform data types and much more…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-1",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\nConsider the following simple example. Suppose we have the following .xlsx-spreadsheet of famous people that died in 2016:"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-2",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\n\nNote how we only want to import the range A5:F15 in the sheet called other. We can pass these values as the corresponding arguments to the read_xlsx function:\n\nlibrary(readxl)\nargs(read_xlsx)\n\nfunction (path, sheet = NULL, range = NULL, col_names = TRUE, \n    col_types = NULL, na = \"\", trim_ws = TRUE, skip = 0, n_max = Inf, \n    guess_max = min(1000, n_max), progress = readxl_progress(), \n    .name_repair = \"unique\") \nNULL\n\n\nHence, to import this data, we should call:\n\ndir &lt;- system.file(package = \"readxl\")\nxlsx_filepath &lt;- file.path(dir, \"extdata\", \"deaths.xlsx\")\nhead(as.data.frame(read_xlsx(xlsx_filepath, sheet = \"other\", range = \"A5:F15\")), 5)\n\n            Name Profession Age Has kids Date of birth Date of death\n1     Vera Rubin  scientist  88     TRUE    1928-07-23    2016-12-25\n2    Mohamed Ali    athlete  74     TRUE    1942-01-17    2016-06-03\n3   Morley Safer journalist  84     TRUE    1931-11-08    2016-05-19\n4   Fidel Castro politician  90     TRUE    1926-08-13    2016-11-25\n5 Antonin Scalia     lawyer  79     TRUE    1936-03-11    2016-02-13"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-3",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-3",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\n\nGoogle Sheets is another widely used spreadsheet program, which is free and web-based. Just like with Excel, in Google Sheets data are organized in worksheets (also called sheets) inside of spreadsheet files.\nAgain, Base R does not have functionality to import data from Google Sheets spreadsheets. However, the package googlesheets4 does. Its main function is read_sheet, which reads a Google Sheet from a URL or a file id.\nGiven such a URL, its use is very similar to read_xlsx:\n\nlibrary(googlesheets4)\n\ngs4_deauth() # used to read publicly available Google Sheets\n# Obtaining data on global life expectancy since 1800 from the Gapminder project\n# For more information, see http://gapm.io/dlex\nsheet_id &lt;- \"1RehxZjXd7_rG8v2pJYV6aY0J3LAsgUPDQnbY4dRdiSs\"\n\ndf_gs &lt;- as.data.frame(read_sheet(sheet_id,\n                                  sheet = \"data-for-world-by-year\",\n                                  range = \"A1:D302\"))\n\n✔ Reading from \"_GM-Life Expectancy- Dataset - v14\".\n\n\n✔ Range ''data-for-world-by-year'!A1:D302'."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-4",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-from-spreadsheets-4",
    "title": "Data Science and Data Analytics",
    "section": "Importing data from spreadsheets",
    "text": "Importing data from spreadsheets\nNow, the data has successfully been imported:\n\nlibrary(knitr)\nkable(head(df_gs, 8)) # the function kable creates nice tables for presentations\n\n\n\n\ngeo\nname\ntime\nLife expectancy\n\n\n\n\nworld\nWorld\n1800\n30.64173\n\n\nworld\nWorld\n1801\n30.71239\n\n\nworld\nWorld\n1802\n30.60052\n\n\nworld\nWorld\n1803\n30.27759\n\n\nworld\nWorld\n1804\n30.19749\n\n\nworld\nWorld\n1805\n30.78082\n\n\nworld\nWorld\n1806\n30.79082\n\n\nworld\nWorld\n1807\n30.73985"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#importing-data-in-other-formats",
    "href": "slides/03_The_Data_Science_Workflow_I.html#importing-data-in-other-formats",
    "title": "Data Science and Data Analytics",
    "section": "Importing data in other formats",
    "text": "Importing data in other formats\n\nThere is a lot of other data formats, which can be read into R with the help of other packages. The following provides a brief and inevitably incomplete overview:\n\nPackage haven for data from SPSS, Stata or SAS.\nPackage DBI along with a DBMS-specific backend allows you to run SQL queries on a data base and obtain the result as a data.frame directly in R.\nPackage jsonline for importing JSON files.\nPackage xml2 for importing XML files.\n…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#the-data-science-workflow-tidy-and-transform",
    "href": "slides/03_The_Data_Science_Workflow_I.html#the-data-science-workflow-tidy-and-transform",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Tidy and Transform",
    "text": "The Data Science workflow – Tidy and Transform"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-and-transform-data-wrangling",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-and-transform-data-wrangling",
    "title": "Data Science and Data Analytics",
    "section": "Tidy and Transform – Data wrangling",
    "text": "Tidy and Transform – Data wrangling\n\nAfter we imported data into R, we want to make it easily processable for visualizations or model building. This process could involve:\n\nrenaming columns to avoid confusion and unnecessary typing.\nsubsetting the data to use only parts of it, i.e. filtering.\nhandling incorrect and/or missing values.\naggregating the data to compute summary statistics.\nreshaping the data to suit the needs of functions operating on them.\nadding or replacing columns.\njoining other data sets to enrich the information presented.\nand much more…\n\nJointly, we refer to these tidying and transformation tasks as data wrangling."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-and-transform-data-wrangling-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-and-transform-data-wrangling-1",
    "title": "Data Science and Data Analytics",
    "section": "Tidy and Transform – Data wrangling",
    "text": "Tidy and Transform – Data wrangling"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#example-data-set",
    "href": "slides/03_The_Data_Science_Workflow_I.html#example-data-set",
    "title": "Data Science and Data Analytics",
    "section": "Example data set",
    "text": "Example data set\nLet’s import some data that we will be using as an example:\n\nflights &lt;- read.csv(file.path(\"data\", \"nyc13_flights.csv\"))\nhead(flights)\n\n  year month day actual.time.of.departure scheduled.time.of.departure\n1 2013     1   1                      517                         515\n2 2013     1   1                      533                         529\n3 2013     1   1                      542                         540\n4 2013     1   1                      544                         545\n5 2013     1   1                      554                         600\n6 2013     1   1                      554                         558\n  depature.delay actual.time.of.arrival scheduled.time.of.arrival arrival.delay\n1              2                    830                       819            11\n2              4                    850                       830            20\n3              2                    923                       850            33\n4             -1                   1004                      1022           -18\n5             -6                    812                       837           -25\n6             -4                    740                       728            12\n  carrier flight plane.tail.number origin destination air.time distance\n1      UA   1545            N14228    EWR         IAH      227     1400\n2      UA   1714            N24211    LGA         IAH      227     1416\n3      AA   1141            N619AA    JFK         MIA      160     1089\n4      B6    725            N804JB    JFK         BQN      183     1576\n5      DL    461            N668DN    LGA         ATL      116      762\n6      UA   1696            N39463    EWR         ORD      150      719\n            time.hour\n1 2013-01-01 05:00:00\n2 2013-01-01 05:00:00\n3 2013-01-01 05:00:00\n4 2013-01-01 05:00:00\n5 2013-01-01 06:00:00\n6 2013-01-01 05:00:00\n\n\nThis data was adapted from the nycflights13 package. It contains information on all 166,158 domestic flights that departed from New York City (airports EWR, JFK and LGA) in the first six months of 2013. The data itself originates from the US Bureau of Transportation Statistics."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#column-naming",
    "href": "slides/03_The_Data_Science_Workflow_I.html#column-naming",
    "title": "Data Science and Data Analytics",
    "section": "Column naming",
    "text": "Column naming\nThe columns of this data set have quite long and descriptive names. Column names generally should:\n\nbe as short as possible.\nbe as long as necessary to be descriptive enough.\nnot use spaces or special characters.\nonly contain lowercase letters.\nuse snake_case for multiple words.\n\nThese rules are good to keep in mind also when naming any other R object. Let’s see what kind of information is contained in our example data set and how we could name the corresponding columns appropriately."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#column-naming-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#column-naming-1",
    "title": "Data Science and Data Analytics",
    "section": "Column naming",
    "text": "Column naming\n\nOur example data set contains the following columns:\n\n\n\nyear, month, day: date of departure\nactual.time.of.departure, scheduled.time.of.departure: actual and scheduled time of departure (in format HHMM or HMM). Too long, how about dep_time and sched_dep_time?\nactual.time.of.arrival, scheduled.time.of.arrival: actual and scheduled time of arrival (in format HHMM or HMM). Too long, how about arr_time and sched_arr_time?\ndeparture.delay, arrival.delay: departure and arrival delays, in minutes. To be consistent, how about dep_delay and arr_delay?\ncarrier, flight: airline and flight number.\nplane.tail.number: plane tail number. Too long, how about tailnum?\norigin, destination: origin and destination airport. origin fine, but maybe dest?\nair.time: amount of time spent in the air, in minutes. Change to air_time?\ndistance: distance between airports, in miles.\ntime.hour: scheduled date and hour of the flight. Change to time_hour?"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#column-naming-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#column-naming-2",
    "title": "Data Science and Data Analytics",
    "section": "Column naming",
    "text": "Column naming\nTo reset the column names of a data.frame, we can either use colnames or names:\n\nhead(names(flights))\n\n[1] \"year\"                        \"month\"                      \n[3] \"day\"                         \"actual.time.of.departure\"   \n[5] \"scheduled.time.of.departure\" \"depature.delay\"             \n\nnames(flights) &lt;- c(\"year\", \"month\", \"day\", \"dep_time\", \"sched_dep_time\", \"dep_delay\",\n                    \"arr_time\", \"sched_arr_time\", \"arr_delay\", \"carrier\", \"flight\",\n                    \"tail_num\", \"origin\", \"dest\", \"air_time\", \"distance\", \"time_hour\")\nhead(flights)\n\n  year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1 2013     1   1      517            515         2      830            819\n2 2013     1   1      533            529         4      850            830\n3 2013     1   1      542            540         2      923            850\n4 2013     1   1      544            545        -1     1004           1022\n5 2013     1   1      554            600        -6      812            837\n6 2013     1   1      554            558        -4      740            728\n  arr_delay carrier flight tail_num origin dest air_time distance\n1        11      UA   1545   N14228    EWR  IAH      227     1400\n2        20      UA   1714   N24211    LGA  IAH      227     1416\n3        33      AA   1141   N619AA    JFK  MIA      160     1089\n4       -18      B6    725   N804JB    JFK  BQN      183     1576\n5       -25      DL    461   N668DN    LGA  ATL      116      762\n6        12      UA   1696   N39463    EWR  ORD      150      719\n            time_hour\n1 2013-01-01 05:00:00\n2 2013-01-01 05:00:00\n3 2013-01-01 05:00:00\n4 2013-01-01 05:00:00\n5 2013-01-01 06:00:00\n6 2013-01-01 05:00:00"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#checking-data-types",
    "href": "slides/03_The_Data_Science_Workflow_I.html#checking-data-types",
    "title": "Data Science and Data Analytics",
    "section": "Checking data types",
    "text": "Checking data types\nNext, we should make sure that the data type and/or class used for each variable suits the data presented in this variable. In particular:\n\nEach numerical variable should of type integer or double.\nEach categorical variable should be a factor.\nEach non-categorical text-based variable should be of type character.\nEach date / date-time variable should be a Date or a POSIXct, respectively.\n\nLet’s have a look in our data set:\n\nsapply(flights, typeof)\n\n          year          month            day       dep_time sched_dep_time \n     \"integer\"      \"integer\"      \"integer\"      \"integer\"      \"integer\" \n     dep_delay       arr_time sched_arr_time      arr_delay        carrier \n     \"integer\"      \"integer\"      \"integer\"      \"integer\"    \"character\" \n        flight       tail_num         origin           dest       air_time \n     \"integer\"    \"character\"    \"character\"    \"character\"      \"integer\" \n      distance      time_hour \n     \"integer\"    \"character\""
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#adapting-data-types",
    "href": "slides/03_The_Data_Science_Workflow_I.html#adapting-data-types",
    "title": "Data Science and Data Analytics",
    "section": "Adapting data types",
    "text": "Adapting data types\nMost of the data types in our data set seem appropriate, however we should:\n\nredefine carrier, tail_num, origin and dest to be a factor.\nredefine time_hour to be a POSIXct date-time.\n\nUsing the function factor, the first part should be no problem:\n\nfactor_vars &lt;- c(\"carrier\", \"tail_num\", \"origin\", \"dest\")\nfor(var in factor_vars){\n  flights[[var]] &lt;- factor(flights[[var]])\n  print(head(flights[[var]]))\n}\n\n[1] UA UA AA B6 DL UA\nLevels: 9E AA AS B6 DL EV F9 FL HA MQ OO UA US VX WN YV\n[1] N14228 N24211 N619AA N804JB N668DN N39463\n3825 Levels: D942DN N0EGMQ N10156 N102UW N103US N104UW N10575 N105UW ... N9EAMQ\n[1] EWR LGA JFK JFK LGA EWR\nLevels: EWR JFK LGA\n[1] IAH IAH MIA BQN ATL ORD\n100 Levels: ABQ ACK ALB ATL AUS AVL BDL BGR BHM BNA BOS BQN BTV BUF BUR ... XNA"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#date-times",
    "href": "slides/03_The_Data_Science_Workflow_I.html#date-times",
    "title": "Data Science and Data Analytics",
    "section": "Date-times",
    "text": "Date-times\n\nIn the previous lecture, we have talked about dates in R, but we have not talked about how to represent time.\nAs simple as time seem to be, representing time-related information with a computer can be incredibly complex. Think about:\n\ntime zones (changing in geographical composition over time),\ndaylight saving time (DST) and its relevance in different countries,\ndifferent formats for writing dates and times (e.g. 16:00 vs. 4:00 pm),\n…\n\nIn this course, we will fortunately stick to relatively simple cases. However, data in the real world does not always behave that nicely…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#date-times-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#date-times-1",
    "title": "Data Science and Data Analytics",
    "section": "Date-times",
    "text": "Date-times\n\nThis map partitions the world into regions where local clocks all show the same time and have done so since 1970. Talk about complexity…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#date-time-in-r-posixct",
    "href": "slides/03_The_Data_Science_Workflow_I.html#date-time-in-r-posixct",
    "title": "Data Science and Data Analytics",
    "section": "Date-time in R – POSIXct",
    "text": "Date-time in R – POSIXct\nOne way of representing date-time information in R is with the POSIXct class. Just as with dates, we can use format strings also to identify hour, minute, second, time zone, etc.\nIn our flights data set, the variable time_hour has a relatively simple format:\n\nhead(flights$time_hour)\n\n[1] \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\"\n[4] \"2013-01-01 05:00:00\" \"2013-01-01 06:00:00\" \"2013-01-01 05:00:00\"\n\n\n\nAdditionally, we know that these are all times from the NYC time zone. In R, we can make use of a data base of time zones with the help of the function OlsonNames, named after the original creator Arthur David Olson. In there, there is a time zone called America/New_York:\n\nOlsonNames()[170]\n\n[1] \"America/Nassau\""
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#date-time-in-r-posixct-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#date-time-in-r-posixct-1",
    "title": "Data Science and Data Analytics",
    "section": "Date-time in R – POSIXct",
    "text": "Date-time in R – POSIXct\nNow, we can use the appropriate format string and time zone name to turn the time_hour variable into a POSIXct date-time. For this purpose, we require the function as.POSIXct:\n\nflights$time_hour &lt;- as.POSIXct(flights$time_hour,\n                                tz = \"America/New_York\",\n                                format = \"%Y-%m-%d %H:%M:%S\")\ntypeof(flights$time_hour)\n\n[1] \"double\"\n\nclass(flights$time_hour)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nhead(flights$time_hour, 2)\n\n[1] \"2013-01-01 05:00:00 EST\" \"2013-01-01 05:00:00 EST\"\n\ntail(flights$time_hour, 2)\n\n[1] \"2013-06-30 20:00:00 EDT\" \"2013-06-30 19:00:00 EDT\"\n\n\nNote how these POSIXct date-times now have EST (Eastern Standard Time) or EDT (Eastern Daylight Time) on them to indicate the time zone. as.POSIXct automatically applied the daylight saving time for the correct period."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#filtering",
    "href": "slides/03_The_Data_Science_Workflow_I.html#filtering",
    "title": "Data Science and Data Analytics",
    "section": "Filtering",
    "text": "Filtering\nNow that we have the correct data types, we might want to filter our data.frame. Filtering refers to the process of keeping rows based on certain conditions imposed on the values of the columns. For example, we might want to find all flights on 14th February that were more than one hour delayed upon arrival at their destination.\n\nWe can filter a data.frame by logical subsetting of the rows. For this, we have to combine the conditions we want to impose by using the logical operators, & (and), | (or) and ! (not). If we want to find the carriers and flight numbers of the aforementioned flights, we could do:\n\nhead(flights[flights$month == 2 & flights$day == 14 & flights$arr_delay &gt; 60,\n             c(\"year\", \"month\", \"day\", \"carrier\", \"flight\", \"arr_delay\")])\n\n      year month day carrier flight arr_delay\n38528 2013     2  14      9E   4023        92\n38551 2013     2  14      DL    807       143\n38604 2013     2  14      B6     56       118\n38613 2013     2  14      B6    600        65\n38623 2013     2  14      DL   1959       200\n38624 2013     2  14      UA    517        95"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#filtering-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#filtering-1",
    "title": "Data Science and Data Analytics",
    "section": "Filtering",
    "text": "Filtering\nLet’s see another example: suppose we want to find all flights that left from either Newark (EWR) or JFK to Los Angeles (LAX) on 14th February after 6:00 pm. We could do:\n\nflights[flights$month == 2 & \n          flights$day == 14 & \n          (flights$origin == \"EWR\" | flights$origin == \"JFK\") & \n          flights$dest == \"LAX\" & \n          flights$dep_time &gt; 1800,\n        c(\"year\", \"month\", \"day\", \"carrier\", \"flight\", \"origin\", \"dest\", \"dep_time\")]\n\n      year month day carrier flight origin dest dep_time\n39020 2013     2  14      AA    119    EWR  LAX     1812\n39076 2013     2  14      DL     87    JFK  LAX     1906\n39111 2013     2  14      AA     21    JFK  LAX     1943\n39146 2013     2  14      VX    415    JFK  LAX     2017\n39150 2013     2  14      UA    771    JFK  LAX     2027\n39184 2013     2  14      DL   2363    JFK  LAX     2118\n39185 2013     2  14      B6    677    JFK  LAX     2118\n39201 2013     2  14      AA    185    JFK  LAX     2147\n\n\n\nNote that – since there are only three possible origin airports in this data set – we could replace the third condition with\n\nflights$origin != \"LGA\""
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#filtering-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#filtering-2",
    "title": "Data Science and Data Analytics",
    "section": "Filtering",
    "text": "Filtering\nThat starts to be quite a lot of typing… To reduce the number of times that we have to type the name of the data.frame, we can use the subset function:\n\nsubset(flights,\n       month == 2 & day == 14 & origin != \"LGA\" & dest == \"LAX\" & dep_time &gt; 1800,\n       c(year, month, day, carrier, flight, origin, dest, dep_time))\n\n      year month day carrier flight origin dest dep_time\n39020 2013     2  14      AA    119    EWR  LAX     1812\n39076 2013     2  14      DL     87    JFK  LAX     1906\n39111 2013     2  14      AA     21    JFK  LAX     1943\n39146 2013     2  14      VX    415    JFK  LAX     2017\n39150 2013     2  14      UA    771    JFK  LAX     2027\n39184 2013     2  14      DL   2363    JFK  LAX     2118\n39185 2013     2  14      B6    677    JFK  LAX     2118\n39201 2013     2  14      AA    185    JFK  LAX     2147\n\n\nThis is much more clear and compact. Note that when using subset, the names of the columns we want to select do not have to be specified with quotation marks \"\"."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values",
    "href": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values",
    "title": "Data Science and Data Analytics",
    "section": "Handling missing values",
    "text": "Handling missing values\nLet’s see where in our data set we have missing values (indicated by NA):\n\nn_missing &lt;- sapply(flights, function(x) sum(is.na(x)))\nn_missing[n_missing &gt; 0]\n\n dep_time dep_delay  arr_time arr_delay  tail_num  air_time \n     4883      4883      5101      5480      1521      5480 \n\n\n\nOnly six variables seem to have missing values. Note how in this data set, the missing values can be interpreted:\n\ndep_time and dep_delay \\(\\to\\) flight was cancelled. In these cases, arr_time and arr_delay are also NA.\nAdditional missing values in arr_time \\(\\to\\) flight was diverted to another destination airport.\nAdditional missing values in arr_delay and air_time. Unknown reason for missingness, hard to construct without additional information.\nIn some cases, the tail_num of the plane seems to be simply unknown."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values-1",
    "title": "Data Science and Data Analytics",
    "section": "Handling missing values",
    "text": "Handling missing values\nLet’s start handling the cancelled flights first. Depending on the circumstances, we might want to add a variable to indicate a cancelled flight, like so…\n\nflights$cancelled &lt;- is.na(flights$dep_time)\nhead(flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"arr_time\",\n                 \"carrier\", \"flight\", \"cancelled\")], 5)\n\n  year month day dep_time arr_time carrier flight cancelled\n1 2013     1   1      517      830      UA   1545     FALSE\n2 2013     1   1      533      850      UA   1714     FALSE\n3 2013     1   1      542      923      AA   1141     FALSE\n4 2013     1   1      544     1004      B6    725     FALSE\n5 2013     1   1      554      812      DL    461     FALSE\n\n\n\n… or remove cancelled flights from the data set all together and put them into their own data.frame. Let’s go for that option:\n\ncancelled_flights &lt;- flights[is.na(flights$dep_time), ]\nflights &lt;- flights[!is.na(flights$dep_time), ]\nhead(cancelled_flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"arr_time\",\n                           \"carrier\", \"flight\", \"cancelled\")], 5)\n\n     year month day dep_time arr_time carrier flight cancelled\n839  2013     1   1       NA       NA      EV   4308      TRUE\n840  2013     1   1       NA       NA      AA    791      TRUE\n841  2013     1   1       NA       NA      AA   1925      TRUE\n842  2013     1   1       NA       NA      B6    125      TRUE\n1778 2013     1   2       NA       NA      EV   4352      TRUE"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#handling-missing-values-2",
    "title": "Data Science and Data Analytics",
    "section": "Handling missing values",
    "text": "Handling missing values\nNext, let’s do the same also with diverted flights, which will contain all remaining flights that have NAs in the arr_time variable:\n\ndiverted_flights &lt;- flights[is.na(flights$arr_time), ]\nflights &lt;- flights[!is.na(flights$arr_time), ]\nhead(diverted_flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"arr_time\",\n                          \"carrier\", \"flight\", \"origin\", \"dest\")], 5)\n\n     year month day dep_time arr_time carrier flight origin dest\n755  2013     1   1     2016       NA      EV   4204    EWR  OKC\n1715 2013     1   2     2041       NA      B6    147    JFK  RSW\n1757 2013     1   2     2145       NA      UA   1299    EWR  RSW\n7040 2013     1   9      615       NA      9E   3856    JFK  ATL\n7852 2013     1   9     2042       NA      B6    677    JFK  LAX\n\n\n\nLet’s look at how many missing values are remaining after separating out cancelled and diverted flights from our data set:\n\nn_missing &lt;- sapply(flights, function(x) sum(is.na(x)))\nn_missing[n_missing &gt; 0]\n\narr_delay  air_time \n      379       379 \n\n\nOnly a small number of NAs are remaining in the variables arr_delay and air_time. We will deal with them when we need to…"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-numeric-variables",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-numeric-variables",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Numeric variables",
    "text": "Descriptive statistics – Numeric variables\nNow, we are finally ready to compute some descriptive statistics. Say, we want to know the average departure and arrival delay of a (not cancelled or diverted) flight. We use the function mean:\n\nmean(flights$dep_delay)\n\n[1] 13.65542\n\nmean(flights$arr_delay)\n\n[1] NA\n\n\nThe average departure delay is around 13.6 minutes, but the average arrival delay is NA? Why is that?\n\nOf course, that’s exactly because of the remaining missing values in arr_delay. R cannot know the average of a vector of values where some values are unknown. However, most functions for descriptive statistics have an argument called na.rm:\n\nmean(flights$arr_delay, na.rm = TRUE)\n\n[1] 8.15129"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-numeric-variables-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-numeric-variables-1",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Numeric variables",
    "text": "Descriptive statistics – Numeric variables\nLet’s say we wanted to know what the maximum departure and arrival delays were. In that case, we would use the function max and also set its na.rm option to TRUE (strictly necessary only for arr_delay):\n\nmax(flights$dep_delay, na.rm = TRUE) / 60 # divide by 60 to get hours from minutes\n\n[1] 21.68333\n\nmax(flights$arr_delay, na.rm = TRUE) / 60 # divide by 60 to get hours from minutes\n\n[1] 21.2\n\n\nSo both maximum departure and arrival delays were over 21 hours!\n\nOther functions for important univariate descriptive statistics of numeric variables are:\n\nrange for both min and max in one go.\nmedian and quantile for quantiles.\nvar and sd for variance and standard deviation.\nfivenum and summary for five-point summaries."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-categorical-variables",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-categorical-variables",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Categorical variables",
    "text": "Descriptive statistics – Categorical variables\nFrom categorical variables, we very often want to compute a frequency table. This can be achieved with the R function table. Let’s say we want to know how many flights started from each of the three NYC airports:\n\n# Absolute frequencies:\ntable(flights$origin)\n\n\n  EWR   JFK   LGA \n58649 54097 48311 \n\n# Relative frequencies:\nprop.table(table(flights$origin))\n\n\n      EWR       JFK       LGA \n0.3641506 0.3358873 0.2999621 \n\n\n\nOr we want to know the distribution of carriers operating out of LaGuardia in %:\n\nround(prop.table(table(flights$carrier[flights$origin == \"LGA\"]))*100, 2)\n\n\n   9E    AA    AS    B6    DL    EV    F9    FL    HA    MQ    OO    UA    US \n 1.10 15.24  0.00  6.14 24.03  5.64  0.69  3.68  0.00 16.70  0.00  7.78 12.84 \n   VX    WN    YV \n 0.00  5.70  0.46"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping",
    "text": "Descriptive statistics – Grouping\nA very common action in data wrangling is computing statistics of a variable for each level of a categorical variable. In our flights data, we might be interested for example in:\n\nthe average departure delay for each carrier,\nthe median arrival delay in each month broken down by NYC airport,\nthe fastest air time for each route,\n…\n\nSuch actions require us to group the data by the factor levels of the categorical variable and then compute statistics on the variable of interest for each of the resulting groups."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-1",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping",
    "text": "Descriptive statistics – Grouping\nIn R, this kind of action can be achieved with the functions tapply and aggregate. Let’s first look at how tapply works for the three examples given on the previous slide:\n\n# Average departure delay by carrier:\ntapply(flights$dep_delay, flights$carrier, mean)\n\n       9E        AA        AS        B6        DL        EV        F9        FL \n18.238843  9.987883  8.058333 13.796756  9.530427 23.308381 24.197605 14.879078 \n       HA        MQ        OO        UA        US        VX        WN        YV \n11.845304 11.580444 63.000000 12.404769  3.938624 14.777251 17.169844 22.549550 \n\n# Median arrival delay for each month and airport\ntapply(flights$arr_delay, list(flights$origin, flights$month), median, na.rm = TRUE)\n\n     1  2  3  4  5  6\nEWR  0 -2 -4 -1 -6 -1\nJFK -7 -5 -7 -4 -9 -1\nLGA -4 -4 -7 -2 -9 -4\n\n# Fastest air time on each route\nhead(tapply(flights$air_time, paste0(flights$origin, \"_\", flights$dest), min, na.rm = TRUE))\n\nEWR_ALB EWR_ATL EWR_AUS EWR_AVL EWR_BDL EWR_BNA \n     24      88     181      76      20      70"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-2",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping",
    "text": "Descriptive statistics – Grouping\nSo tapply generally works like this:\n\ntapply(variable_of_interest, list_of_grouping_variables, aggregation_function, ...)\n\n\nBy contrast, the function aggregate works like this:\n\naggregate(df_of_interest, list_of_grouping_variables, aggregation_function, ...)\n\naggregate then applies the aggregation_function to each variable in the df_of_interest by group.\n\n\nSo, we can compute both average departure and arrival delays by carrier in one go, for example:\n\nhead(aggregate(flights[,c(\"dep_delay\", \"arr_delay\")],\n               list(flights$carrier),\n               mean, na.rm = TRUE))\n\n  Group.1 dep_delay arr_delay\n1      9E 18.238843  9.519527\n2      AA  9.987883  1.475949\n3      AS  8.058333 -3.036415\n4      B6 13.796756 10.410469\n5      DL  9.530427  1.745742\n6      EV 23.308381 20.328515"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-with-cut",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-with-cut",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping with cut",
    "text": "Descriptive statistics – Grouping with cut\n\nA frequently encountered goal is to group not based on an existing categorical variable, but on different intervals of a numeric variable.\nFor example, we might be interested in analyzing delay patterns based on air time: short-haul (\\(\\leq\\) 3 hours), medium-haul (3-6 hours) and long-haul (6-16 hours) (according to the IATA).\nTo group by these flight length categories, we have to cut our air_time variable at these cut points. For this, we can use the function cut.\nBesides specifying the cut points, cut offers us also to label the levels of the resulting factor."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-with-cut-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#descriptive-statistics-grouping-with-cut-1",
    "title": "Data Science and Data Analytics",
    "section": "Descriptive statistics – Grouping with cut",
    "text": "Descriptive statistics – Grouping with cut\nSo to add this variable to our data.frame, we might do:\n\nflights$length_cat &lt;- cut(flights$air_time, c(0, 180, 360, Inf),\n                          labels = c(\"short-haul\", \"medium-haul\", \"long-haul\"))\n\nhead(flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"arr_time\",\n                 \"origin\", \"dest\", \"air_time\", \"length_cat\")])\n\n  year month day dep_time arr_time origin dest air_time  length_cat\n1 2013     1   1      517      830    EWR  IAH      227 medium-haul\n2 2013     1   1      533      850    LGA  IAH      227 medium-haul\n3 2013     1   1      542      923    JFK  MIA      160  short-haul\n4 2013     1   1      544     1004    JFK  BQN      183 medium-haul\n5 2013     1   1      554      812    LGA  ATL      116  short-haul\n6 2013     1   1      554      740    EWR  ORD      150  short-haul\n\n\nNow, we can analyse average departure and arrival delays by these categories using aggregate, for instance:\n\naggregate(flights[,c(\"dep_delay\", \"arr_delay\")],\n          list(flights$length_cat), mean, na.rm = TRUE)\n\n      Group.1 dep_delay arr_delay\n1  short-haul  14.55357  10.06333\n2 medium-haul  11.22840   2.39164\n3   long-haul  10.17673  16.59834"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables",
    "text": "Joining tables\nOur flights data set only contains codes for carrier, air plane and airports. To properly understand this data, we need to be able to enrich this data set by more information, such as:\n\nFull name of the carrier\nFull name and location of the origin and destination airports\nType, model and size of the aircraft used\nWeather information at the origin airport at the time of departure\n\nIndeed, any moderately complex data science project will involve multiple tables that must be joined together in order to answer the questions that you are interested in."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-1",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables",
    "text": "Joining tables\nThe nycflights13 package contains four additional tables that can be joined into the flights table. The below graph illustrates their relation:"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-2",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables",
    "text": "Joining tables\nTherefore, in the flights data set,\n\nthe variables origin and dest are foreign keys that correspond to the primary key faa in airports.\nthe variable tail_num is a foreign key that corresponds to the primary key tail_num in planes.\nthe variable carrier is a foreign key that corresponds to the primary key carrier in airlines.\nthe variables origin and time_hour constitute a compound foreign key that corresponds to the compound primary key constituted by origin and time_hour in weather.\n\nTo illustrate how we can join information from these tables into flights using keys, we start with the easiest example of airlines."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-3",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-3",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables",
    "text": "Joining tables\nFor this, we have to read in the airlines table and inspect it:\n\nairlines &lt;- read.csv(file.path(\"data\", \"nyc13_airlines.csv\"))\nairlines\n\n   carrier                        name\n1       9E           Endeavor Air Inc.\n2       AA      American Airlines Inc.\n3       AS        Alaska Airlines Inc.\n4       B6             JetBlue Airways\n5       DL        Delta Air Lines Inc.\n6       EV    ExpressJet Airlines Inc.\n7       F9      Frontier Airlines Inc.\n8       FL AirTran Airways Corporation\n9       HA      Hawaiian Airlines Inc.\n10      MQ                   Envoy Air\n11      OO       SkyWest Airlines Inc.\n12      UA       United Air Lines Inc.\n13      US             US Airways Inc.\n14      VX              Virgin America\n15      WN      Southwest Airlines Co.\n16      YV          Mesa Airlines Inc.\n\n\nThis is a very simple and small data set with the carrier code and name of 16 American airline companies. Now, how do we join this information into the flights data set?"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-types-of-joins",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-types-of-joins",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Types of joins",
    "text": "Joining tables – Types of joins\n\nThere are many different types of joins that determine how two (or more) tables are brought together. We will illustrate only the most important ones with a toy example.\nSay, we have two tables, x and y, each with a key column and a column containing some values:\n\n\n\n\n\nThe colored key columns map background color to key value. The grey columns represent “value” columns."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-inner-join",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-inner-join",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Inner join",
    "text": "Joining tables – Inner join\n\nIn an inner join, we want to keep only the rows that have information in both tables.\nIn the example, this is only the case for data points with key 1 and 2. Therefore, the rows with key 3 and 4 do not make it to the joined table, if we join x and y on an inner join:"
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-left-outer-join",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-left-outer-join",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Left outer join",
    "text": "Joining tables – Left outer join\n\nIn a left outer join, we want to keep all rows in the left table (in this case x). Rows without a matching key in y receive NA for the value column of y:\n\n\n\n\n\nFor simplicity, such joins are usually simply called left joins."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-right-outer-join",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-right-outer-join",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Right outer join",
    "text": "Joining tables – Right outer join\n\nIn a right outer join, we want to keep all rows in the right table (in this case y). Rows without a matching key in x receive NA for the value column of x:\n\n\n\n\n\nFor simplicity, such joins are usually simply called right joins."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-full-outer-join",
    "href": "slides/03_The_Data_Science_Workflow_I.html#joining-tables-full-outer-join",
    "title": "Data Science and Data Analytics",
    "section": "Joining tables – Full outer join",
    "text": "Joining tables – Full outer join\n\nIn a full outer join, we want to keep all the rows across both tables and fill the missing values with NAs:\n\n\n\n\n\nFor simplicity, such joins are usually simply called full joins."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#performing-joins-with-merge",
    "href": "slides/03_The_Data_Science_Workflow_I.html#performing-joins-with-merge",
    "title": "Data Science and Data Analytics",
    "section": "Performing joins with merge",
    "text": "Performing joins with merge\nIn R, joining happens with the help of the function merge, whose arguments can be quite confusing at times… Here is the breakdown of the most important ones:\n\nx and y are the left and right data.frame to join.\nby is the name of the key(s) used for joining, if the column name of this key is the same in both x and y. If not, we specify the column names in x via the by.x argument and in y via the by.y argument.\nThe arguments all, all.x and all.y specify the type of join:\n\nSet all to FALSE for an inner join.\nSet all.x to TRUE for a left join.\nSet all.y to TRUE for a right join.\nSet all to TRUE for a full outer join."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#performing-joins-with-merge-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#performing-joins-with-merge-1",
    "title": "Data Science and Data Analytics",
    "section": "Performing joins with merge",
    "text": "Performing joins with merge\nSo let’s finally see joins in action! We left join the airlines data set into flights on the key of carrier. So, to do this in R, we run:\n\nflights &lt;- merge(flights, airlines, by = \"carrier\", all.x = TRUE)\nhead(flights[, c(\"year\", \"month\", \"day\", \"dep_time\", \"origin\", \"dest\", \"carrier\", \"name\")])\n\n  year month day dep_time origin dest carrier              name\n1 2013     5  19     2034    LGA  TYS      9E Endeavor Air Inc.\n2 2013     4  28     1621    JFK  MKE      9E Endeavor Air Inc.\n3 2013     4  23      749    EWR  CVG      9E Endeavor Air Inc.\n4 2013     6   5     2049    JFK  DCA      9E Endeavor Air Inc.\n5 2013     1  19     1944    JFK  IAD      9E Endeavor Air Inc.\n6 2013     6  27     1939    LGA  DAY      9E Endeavor Air Inc.\n\n\nNote two things about this successful join:\n\nFirst, in the airlines data set, we found a match for every carrier in flights, so the left join did not produce any NAs. Proof:\n\nsum(is.na(flights$name))\n\n[1] 0\n\n\nSecond, merge changed the order of the rows of the data.frame."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-data",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-data",
    "title": "Data Science and Data Analytics",
    "section": "Tidy data",
    "text": "Tidy data\n\nTo finish this chapter, let’s talk about tidy data.\nRather than just meaning “clean”, tidy data actually refers to a specific way of organizing your data that is beneficial for the types of actions we want to perform on them. There are three interrelated rules that make data tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nThese rules might seem pretty obvious, but actually, most real-world data sets do not meet these requirements as data is often organized to facilitate some goal other than analysis."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-1",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-1",
    "title": "Data Science and Data Analytics",
    "section": "Tidy data",
    "text": "Tidy data\n\nTo illustrate this point, we will have a look a new data set. It contains the fertility rates in Germany and South Korea in the years 1960 to 2015:\n\nfertility &lt;- read.csv(file.path(\"data\", \"fertility.csv\"))\nfertility[, 1:12]\n\n      country X1960 X1961 X1962 X1963 X1964 X1965 X1966 X1967 X1968 X1969 X1970\n1     Germany  2.41  2.44  2.47  2.49  2.49  2.48  2.44  2.37  2.28  2.17  2.04\n2 South Korea  6.16  5.99  5.79  5.57  5.36  5.16  4.99  4.85  4.73  4.62  4.53\n\ndim(fertility)\n\n[1]  2 57\n\n\nThis data is untidy. Why?\n\nIt contains a variable (namely the year) in column names, but according to the principles of tidy data, each variable should be its own column.\nThe observations are fertility rates in two countries, so these values should be organized in rows."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-2",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-2",
    "title": "Data Science and Data Analytics",
    "section": "Tidy data",
    "text": "Tidy data\nDue to its shape, such data is said to be in a wide format (few rows, lots of columns). We want to reshape it to long format (lots of rows, few columns). For this purpose, we use the R function reshape:\n\nfertility_long &lt;- reshape(fertility, direction = \"long\",\n                          varying = list(names(fertility)[-1]), v.names = \"fertility_rate\",\n                          idvar = \"country\",\n                          timevar = \"year\", times = 1960:2015)\nrownames(fertility_long) &lt;- NULL\nhead(fertility_long, 8)\n\n      country year fertility_rate\n1     Germany 1960           2.41\n2 South Korea 1960           6.16\n3     Germany 1961           2.44\n4 South Korea 1961           5.99\n5     Germany 1962           2.47\n6 South Korea 1962           5.79\n7     Germany 1963           2.49\n8 South Korea 1963           5.57\n\n\nNow, the data is tidy! Note that it is exactly the same underlying data, just represented in a slightly different way."
  },
  {
    "objectID": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-3",
    "href": "slides/03_The_Data_Science_Workflow_I.html#tidy-data-3",
    "title": "Data Science and Data Analytics",
    "section": "Tidy data",
    "text": "Tidy data\nThe function reshape takes some practice to get used to. But once we know how to get our data tidy, there are multiple advantages to using this consistent way of organizing your data:\n\nAs we will see, many functions for data analysis in R cannot be used, unless the data is tidy. This applies in particular to the visualizations we will create with the ggplot2 package.\nConsistent data structures are easier to work with. If every data set “looks and feels” the same, you can build routines that will make your analyses more efficient and effective.\nHaving variables consistently in columns is particularly sensible in R due to its vectorized nature. R performs at its best when it is able to run functions on vectors of values. With a tidy data format, we can fully leverage this potential."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Data Analytics",
    "section": "",
    "text": "General blurb",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "image_sources.html",
    "href": "image_sources.html",
    "title": "Image sources",
    "section": "",
    "text": "The following table displays the sources of all images used in the slides:\n\n\n\n\n\n\n\n\n\n\nfile\nSource\nNote\n\n\n\n\nai_ml_dl_ds_venn.png\nhttps://www.guvi.com/blog/must-know-neural-networks-for-data-science/\n\n\n\nai_venn.png\nhttps://link.springer.com/chapter/10.1007/978-981-19-5272-2_1\n\n\n\naimeme.png\nReddit\n\n\n\nanyideas.png\nOwn creation\n\n\n\napi.png\nhttps://www.postman.com/what-is-an-api/\n\n\n\narrow_csv.png\nOwn creation\n\n\n\narrow_delim.png\nOwn creation\n\n\n\nbias_var_tradeoff.png\nAn Introduction to Statistical Learning\n\n\n\nchatgpt_headlines.png\nhttps://www.searchengineacademy.com/blog/the-search-engine-sky-is-falling-does-chatgpt-mean-the-twilight-of-google/\n\n\n\ncitation.png\nOwn creation\n\n\n\ncitation_added.png\nOwn creation\n\n\n\ncitation_added2.png\nOwn creation\n\n\n\nclassification.png\nAn Introduction to Statistical Learning\n\n\n\nclusters.png\nTan, Steinbach, Karpatne, Kumar, 2018\n\n\n\nclusters_black.png\nTan, Steinbach, Karpatne, Kumar, 2018\n\n\n\ncrm.png\nhttps://localo.com/marketing-dictionary/what-is-crm\n\n\n\ncross_reference.png\nOwn creation\n\n\n\ncross_reference2.png\nOwn creation\n\n\n\ncsv_file.png\nhttps://rafalab.dfci.harvard.edu/dsbook-part-1/R/importing-data.html\n\n\n\ndashboard.png\nhttps://www.domo.com/learn/article/the-benefits-of-business-intelligence-dashboards\n\n\n\ndata-science-cycle.001.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.002.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.003.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.004.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.005.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.006.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.007.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.008.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata-science-cycle.009.png\nhttps://r4ds.hadley.nz/intro.html\nhttps://sta199-s24.github.io/\n\n\ndata_wrangling.png\nhttps://airbyte.com/data-engineering-resources/data-wrangling\n\n\n\ndate_format_meme.jpg\nhttps://www.reddit.com/r/trippinthroughtime/comments/7axug6/the_perfect_date/?rdt=64406\n\n\n\ndb.png\nhttps://mockey.ai/blog/google-gemini-vs-chatgpt/\n\n\n\ndecision_boundaries.png\nAn Introduction to Statistical Learning\n\n\n\ndestatis.svg\nDestatis (Statistisches Bundesamt)\n\n\n\ndirty_excel.png\nhttps://www.sophieheloisebennett.com/posts/excel-sheet-cleaning/\n\n\n\nds_circle.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_1.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_2.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_3.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_4.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_5.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_6.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_7.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_circle_8.png\nOwn creation\nhttps://www.researchgate.net/publication/336879052_Whitepaper_A_Data_Science_Approach_to_Solve_the_Returns_Problem\n\n\nds_importance_business.jpg\nhttps://www.jaroeducation.com/blog/data-science-and-analytics-importance/\n\n\n\nds_proglangs.jpg\nhttps://jelvix.com/blog/top-data-science-programming-languages\n\n\n\nds_vs_da.jpg\nhttps://www.qlik.com/us/data-analytics/data-science-vs-data-analytics\n\n\n\nds_vs_da2.png\nhttps://www.geeksforgeeks.org/difference-between-a-data-analyst-and-a-data-scientist/\n\n\n\ndsvenn.png\nhttps://www.cleanpng.com/png-data-science-venn-diagram-computer-science-machine-1831517/\n\n\n\nencoding_issue.png\nOwn creation\n\n\n\nencoding_issue2.png\nOwn creation\n\n\n\nerp.png\nhttps://koss.software/koss-lexikon/enterprise-resource-planning-system-erp\n\n\n\nerp2.png\nhttps://dmsiworks.com/faqs/is-business-central-a-good-erp-system\n\n\n\nexcel.png\nhttps://sta199-s24.github.io/\n\n\n\nfirst_quarto.png\nOwn creation\n\n\n\nfirst_quarto2.png\nOwn creation\n\n\n\nfirst_quarto3.png\nOwn creation\n\n\n\nfit_parametric.png\nAn Introduction to Statistical Learning\n\n\n\nfit_nonparametric.png\nAn Introduction to Statistical Learning\n\n\n\ngrammar_of_graphics.png\nhttps://r.qcbs.ca/workshop03/book-en/grammar-of-graphics-gg-basics.html\n\n\n\ngrammar_of_graphics_book.jpeg\nhttps://www.amazon.co.uk/Grammar-Graphics-Statistics-Computing/dp/0387987746\n\n\n\nHarvardX_Rbasics.png\nOwn creation\n\n\n\nHarvardX_Visualization.png\nOwn creation\n\n\n\nhbr_article1.png\nSource linked in slides\n\n\n\nhbr_article2.png\nSource linked in slides\n\n\n\nhp_json.png\nOwn creation\n\n\n\nhttp_get.jpg\nhttps://www.cloud4y.ru/en/blog/what-are-http-requests/\n\n\n\nhttp_response.jpg\nhttps://slideplayer.com/slide/5320369/\n\n\n\nISL_book.png\nhttps://www.statlearning.com/\n\n\n\njoins_full.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\njoins_inner.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\njoins_left.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\njoins_right.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\njoins_setup.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\nkfold_cv.png\nhttps://towardsdatascience.com/how-to-cross-validation-with-time-series-data-9802a06272c6/\n\n\n\nknn_reg.png\nAn Introduction to Statistical Learning\n\n\n\nknn_vs_lm.png\nAn Introduction to Statistical Learning\n\n\n\nknn_vs_lm2.png\nAn Introduction to Statistical Learning\n\n\n\nml_overview.png\nhttps://sanchittanwar75.medium.com/introduction-to-machine-learning-and-deep-learning-bd25b792e488\n\n\n\nnber.png\nhttps://www.nber.org/\n\n\n\nneighbor_unknown.png\nTan, Steinbach,Karpatne, Kumar, 2018\n\n\n\nnosql_types.png\nhttps://www.linkedin.com/pulse/types-nosql-databases-umang-agarwal\n\n\n\nnycflights13_data.png\nhttps://r4ds.hadley.nz/joins.html\n\n\n\noverfitting.png\nhttps://de.mathworks.com/discovery/overfitting.html\n\n\n\noverfitting.svg\nhttps://de.mathworks.com/discovery/overfitting.html\n\n\n\npalette_finder.png\nhttps://r-graph-gallery.com/color-palette-finder\n\n\n\npalmer_penguins.jpg\nhttps://education.rstudio.com/blog/2020/07/palmerpenguins-cran/\n\n\n\npaths_1.png\nhttps://dodona.be/en/activities/760650576/\n\n\n\npaths_2.png\nhttps://dodona.be/en/activities/760650576/\n\n\n\npenguins_beak.png\nhttps://education.rstudio.com/blog/2020/07/palmerpenguins-cran/\n\n\n\npenguins_cartoon.png\nhttps://allisonhorst.github.io/palmerpenguins/\n\n\n\npenguins_report.png\nOwn creation\n\n\n\npoints_symbols.png\nhttps://www.sthda.com/english/wiki/ggplot2-point-shapes\n\n\n\npresentation_quarto.png\nOwn creation\n\n\n\nquarto.png\nhttps://quarto.org/docs/get-started/hello/rstudio.html\n\n\n\nr.png\nhttps://sta199-s24.github.io/\n\n\n\nr_function_syntax.png\nhttps://www.learnbyexample.org/r-functions/\n\n\n\nr_help_round.png\nOwn creation\n\n\n\nr_help_sample.png\nOwn creation\n\n\n\nr_logo.png\nhttps://commons.wikimedia.org/wiki/File:R_logo.svg\n\n\n\nr_vs_rstudio.png\nhttps://moderndive.com/1-getting-started.html\n\n\n\nread_delim.png\nOwn creation\n\n\n\nreg_tree.png\nAn Introduction to Statistical Learning\n\n\n\nrel_db.jpg\nhttps://www.ddebose.com/projects1/movie-theater-database-psql\n\n\n\nrel_db2.svg\nhttps://en.wikipedia.org/wiki/Relational_database\n\n\n\nrender.png\nOwn creation\n\n\n\nrstudio.png\nhttps://sta199-s24.github.io/\n\n\n\nrstudio_logo.png\nhttps://techicons.dev/icons/rstudio\n\n\n\nrstudio_meme.jpg\nOwn creation\n\n\n\nrstudio_tour.png\nOwn creation\n\n\n\nrstudio_tour2.png\nhttps://sta199-s24.github.io/\n\n\n\nrvspython.jpg\nhttps://x.com/R_Amadio/status/1372624991485845510\n\n\n\nrvspython1.jpg\nhttps://x.com/R_Amadio/status/1372624991485845510\n\n\n\nrvspython2.jpg\nhttps://x.com/R_Amadio/status/1372624991485845510\n\n\n\nscale_funcs.png\nhttps://socviz.co/\n\n\n\nsh_json.png\nOwn creation\n\n\n\nspreadsheets_deaths.png\nhttps://r4ds.hadley.nz/spreadsheets.html\n\n\n\nsql_meme.jpg\nOwn creation\n\n\n\nstatista.png\nStatista\n\n\n\nstatistik_austria.png\nStatistik Austria\n\n\n\nsupervised_learning.png\nhttps://www.superannotate.com/blog/image-classification-basics\n\n\n\nth_json.png\nOwn creation\n\n\n\ntimezones.png\nhttps://en.wikipedia.org/wiki/Tz_database#/media/File:Timezone-boundary-builder_release_2023d.png\n\n\n\ntrain_test_mse.png\nAn Introduction to Statistical Learning\n\n\n\ntrain_test_mse2.png\nAn Introduction to Statistical Learning\n\n\n\ntrain_test_mse3.png\nAn Introduction to Statistical Learning\n\n\n\ntrain_test_split.jpg\nhttps://builtin.com/data-science/train-test-split\n\n\n\ntsv_file.png\nOwn creation\n\n\n\ntypes_of_data.png\nhttps://www.researchgate.net/publication/236860222_Developing_Dynamic_Packaging_Applications_Using_Semantic_Web-Based_Integration\n\n\n\ntype 1 and 2 errors.jpg\nhttps://learning.eupati.eu/mod/book/tool/print/index.php?id=362&chapterid=388\n\n\n\nUOS_Logo.jpg\nUniversity\n\n\n\nusdata.png\nhttps://data.gov/\n\n\n\nwb.png\nhttps://datacatalog.worldbank.org/home\n\n\n\nworkspace.png\nOwn creation\n\n\n\nX.jpg\nhttps://www.freepik.com/free-photos-vectors/twitter-x-logo",
    "crumbs": [
      "Image Sources"
    ]
  },
  {
    "objectID": "slides/06_penguins_quarto.html",
    "href": "slides/06_penguins_quarto.html",
    "title": "Penguins in Antarctica",
    "section": "",
    "text": "For this analysis, we will use the penguins data set from the palmerpenguins package. The website belonging to this package can be found here.\nThe data set contains size measurements, clutch observations, and blood isotope ratios for three penguins species observed on three islands in the Palmer Archipelago, Antarctica, over a study period of three years (Gorman, Williams, and Fraser 2014).\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n\n\n\nFigure 1: A scatter plot of flipper length against body mass\n\n\n\n\n\nFigure 1 shows a scatter plot of the flipper length against the body mass of all penguins in the data set.\nThe data set comprises measurements of three types of penguins, namely:\n\nAdelie 🐧\nChinstrap 🐧\nGentoo 🐧\n\n\n\n\n\n\n\nFigure 2\n\n\n\nFigure 2 shows a cartoon of the three penguin species in the data set."
  },
  {
    "objectID": "slides/06_penguins_quarto.html#data",
    "href": "slides/06_penguins_quarto.html#data",
    "title": "Penguins in Antarctica",
    "section": "",
    "text": "For this analysis, we will use the penguins data set from the palmerpenguins package. The website belonging to this package can be found here.\nThe data set contains size measurements, clutch observations, and blood isotope ratios for three penguins species observed on three islands in the Palmer Archipelago, Antarctica, over a study period of three years (Gorman, Williams, and Fraser 2014).\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n\n\n\nFigure 1: A scatter plot of flipper length against body mass\n\n\n\n\n\nFigure 1 shows a scatter plot of the flipper length against the body mass of all penguins in the data set.\nThe data set comprises measurements of three types of penguins, namely:\n\nAdelie 🐧\nChinstrap 🐧\nGentoo 🐧\n\n\n\n\n\n\n\nFigure 2\n\n\n\nFigure 2 shows a cartoon of the three penguin species in the data set."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#the-data-science-workflow-communicate",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#the-data-science-workflow-communicate",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Communicate",
    "text": "The Data Science workflow – Communicate"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#putting-it-all-together",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#putting-it-all-together",
    "title": "Data Science and Data Analytics",
    "section": "Putting it all together…",
    "text": "Putting it all together…\n\nThe final product of a data science project is often a report, which can take multiple different forms:\n\na scientific publication (typically PDF)\na technical report for your company (typically Word, Pages or PDF)\na presentation (typically PowerPoint, Keynote or PDF)\na blog post (typically HTML)\n…\n\nDepending on the audience, such reports may include\n\ntext to describe project, methods and findings\nfigures and tables to describe and visualize data and results\ncode you ran to get to these results"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#putting-it-all-together-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#putting-it-all-together-1",
    "title": "Data Science and Data Analytics",
    "section": "Putting it all together…",
    "text": "Putting it all together…\n\nNow, imagine…\n\nyou have to re-run your entire analysis with a new data set because the initial one was wrong in some way.\nyou realize that a mistake was made and the code needs to be fixed and re-run entirely.\na colleague from another department wants to reproduce the results from your report using your data and code.\n\nSituations like these are very commonplace for a data scientist. It is for this reason that we need a framework that integrates text, figures, tables, code and data to create reproducible reports.\nSuch a framework is given by Quarto."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#what-is-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#what-is-quarto",
    "title": "Data Science and Data Analytics",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nQuarto is a format for so-called literate programming documents. Literate programming weaves textual documentation and comments together with code, producing a document that is optimized for human understanding.\nThe basic workflow is very simple:\n\nCreate a Quarto file (ends in .qmd)\nIn this file, specify the output format (PDF, HTML, Word, …) as well as metadata about the document, such as a title, the name of the author, date, etc.\nAdd your code and text.\nCompile the document into the final report. In this process, Quarto runs all your code, creates all the plots and tables and integrates everything with your text into the specified output format.\n\nQuarto is very powerful. The rest of this course gives a basic introduction."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#our-first-quarto-document",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#our-first-quarto-document",
    "title": "Data Science and Data Analytics",
    "section": "Our first Quarto document",
    "text": "Our first Quarto document\nIn RStudio, we can create a new Quarto file by clicking on File &gt; New File &gt; Quarto Document. Give the document a fitting title and author and select PDF as the output format (for now). RStudio then creates a template .qmd file:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-yaml-header",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-yaml-header",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – YAML header",
    "text": "Elements of a Quarto document – YAML header\n\nAt the very top of the document – demarcated by three dashes (---) on either end – is the so-called YAML header:\ntitle: \"My first Quarto document\"\nauthor: \"Julian Amon\"\nformat: pdf\nYAML is a widely used language mainly used for providing configuration data.\nWith Quarto, it is used to define metadata about the document, such as (in this case) title, author and (output) format.\nWe can define many other things in the header than what is included in the template, e.g. theme, fontcolor or fig-width. A complete list of available YAML fields for PDF documents can be found here."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – Code chunks",
    "text": "Elements of a Quarto document – Code chunks\n\nTo run code inside a Quarto document, you need to put it inside a so-called code chunk. Such a code consists of four elements:\n\nThree back ticks and the programming language in curly braces to start the chunk (since we only use R, for us, this is always ```{r}).\nChunk options (optional): these are key-value pairs that can be used for customizing code chunks, each on their own line starting with #|.\nThe actual code.\nThree back ticks again to signify the end of the code chunk.\n\nThe template contains an example of a code chunks with options:\n```{r}\n#| echo: false\n\n2 * 2\n```"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks-1",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – Code chunks",
    "text": "Elements of a Quarto document – Code chunks\n\nThe full list of code options is available here. The most important ones are:\n\necho: false prevents code, but not the results from appearing in the finished file. Use this when writing reports aimed at people who do not want to see the underlying R code.\neval: false prevents code from being run. This is useful for displaying example code, or for disabling a large block of code without commenting each line.\ninclude: false runs the code, but doesn’t show the code or results in the final document. Use this for setup code (like loading libraries) that you do not want cluttering your report.\nmessage: false or warning: false prevents messages or warnings from appearing in the finished file.\n\nBy default, all of these code chunk options are true."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks-2",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-code-chunks-2",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – Code chunks",
    "text": "Elements of a Quarto document – Code chunks\n\nAs you work more with Quarto, you will discover that some of the default chunk options do not fit your needs and you want to change them.\nFor example, consider a situation where you are writing a report for a non-technical audience. To hide the underlying R code, we would have to set echo: false in each and every chunk.\nTo avoid this, we can set the preferred options globally in the YAML header under execute. So, to achieve global code suppression, we would set:\ntitle: \"My first Quarto document\"\nauthor: \"Julian Amon\"\nexecute:\n  echo: false\nformat: pdf\nThese global code chunk options can be overridden locally in any chunk."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-markdown-text",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#elements-of-a-quarto-document-markdown-text",
    "title": "Data Science and Data Analytics",
    "section": "Elements of a Quarto document – Markdown text",
    "text": "Elements of a Quarto document – Markdown text\n\nFinally, besides the YAML header and code chunks, the last (main) type of content we see in a Quarto file is text mixed with simple formatting instructions. This text is written in a so-called Markdown format.\nInstead of using buttons like in Word or Pages to format our text, Markdown lets us add headings, bold or italic text, lists, links and more, just by typing special characters. Consider the following examples:\n\n# Heading, ## Subheading and ### Subsubheading for section titles.\n*italic* for text in italics.\n**bold** for text in boldface.\n- Item 1\n- Item 2 for bullet point lists.\n[Link text](https://quarto.org) for clickable links."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#combining-all-three-elements-in-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#combining-all-three-elements-in-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Combining all three elements in Quarto",
    "text": "Combining all three elements in Quarto\n\nLet’s use our knowledge about the three elements of a Quarto document (YAML header, code chunks and Markdown text) to adapt the template that RStudio created for us in the following ways:\n\nAdd a date field to the header and set it (literally) to today.\nChange the title of the document to Penguins in Antarctica.\nChange the level 2 header ## Quarto to ## Data.\nImmediately after this header, add a code chunk that only loads the libraries palmerpenguins and ggplot2. For this chunk, set include: false.\nAfter that chunk, write a simple sentence in Markdown that points out that we use data from the palmerpenguins package. Make sure to include a link to the website of the package, which can be found here.\nFinally, in another chunk, create a scatter plot of flipper_length_mm length against body_mass_g. Remove all other content from the template."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#combining-all-three-elements-in-quarto-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#combining-all-three-elements-in-quarto-1",
    "title": "Data Science and Data Analytics",
    "section": "Combining all three elements in Quarto",
    "text": "Combining all three elements in Quarto\nAfter you did all that, your Quarto document should look something like this:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#compiling-our-first-quarto-report",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#compiling-our-first-quarto-report",
    "title": "Data Science and Data Analytics",
    "section": "Compiling our first Quarto report",
    "text": "Compiling our first Quarto report\n\nTo compile (or render) your final report from this Quarto file, first save the file into an appropriate location on your computer.\nThen, click on the  button at the top of the screen. This will run all the code, and combine text, code and results in the desired output format:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#the-visual-editor",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#the-visual-editor",
    "title": "Data Science and Data Analytics",
    "section": "The visual editor",
    "text": "The visual editor\n\nWe have successfully created our first Quarto report! 🎉\nWhile writing text in Markdown format is in principle easy to read and write, it still requires learning new syntax. For this reason, RStudio offers the visual editor to assist you in editing your Quarto document:\n\nThe idea of the visual editor is to provide a graphical user interface (GUI) that is more similar to working with, say, Microsoft Word.\nFor example, you can use the buttons on the menu bar for text formatting or to insert lists, images, tables, cross-references, etc. You can also paste an image from your clipboard directly into the visual editor and much more…\nWhile the visual editor displays your content with formatting, under the hood, it saves your content in plain Markdown. You can switch back and forth between visual and source editor to view and edit your content."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#the-visual-editor-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#the-visual-editor-1",
    "title": "Data Science and Data Analytics",
    "section": "The visual editor",
    "text": "The visual editor\n\nLet’s use the visual editor to make the following changes to our Quarto document on the penguins from the palmerpenguins package:\n\nWrite a sentence that explains that the data comprises three penguin species and list them with bullet points (Adelie, Chinstrap and Gentoo).\nAt the end of each bullet point, insert a penguin emoji (Insert &gt; Special Characters &gt; Insert Emoji...).\nBelow these bullet points, insert the cartoon displaying all three species from the package website. Make sure that it is aligned in the centre.\n\nAt the end, switch back to the source editor and see how your changes were implemented in Markdown. Finally, render your report again to see how the changes affect the final output."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#implementing-changes-in-the-visual-editor",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#implementing-changes-in-the-visual-editor",
    "title": "Data Science and Data Analytics",
    "section": "Implementing changes in the visual editor",
    "text": "Implementing changes in the visual editor\nWith all these changes, the visual editor should show something like this:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-and-citations-in-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-and-citations-in-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Cross-references and citations in Quarto",
    "text": "Cross-references and citations in Quarto\n\nTo write a scientific report or paper, two features are almost always required:\n\nCross references: referring to a figure or a table by a label, so that you can reference it in your text using its number (e.g., “see Figure 1”).\nCitations: citing academic literature using a .bib-file, so that references are properly formatted and listed automatically at the end of the document.\n\nFortunately, the visual editor makes both of these things very easy. To create a cross reference for a plot produced by a code chunk, we need to do two things:\n\nGive the chunk a label starting with fig-, e.g. fig-penguins1.\nProvide a caption for the plot in the fig-cap code chunk option, e.g. “A scatter plot of flipper length against body mass”.\n\nOnce, we have done this, we can then refer to this figure in the text with @fig-penguins1 or insert the cross reference via Insert &gt; Cross Reference."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-in-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-in-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Cross-references in Quarto",
    "text": "Cross-references in Quarto\n\n\nIn this way, it is ensured that wherever we move the chunk or its reference in the report, the reference will always go the correct plot.\nWe can create such references also for inserted images, of course:\n\nSimply click on the three dots next to the inserted image and go to the Attributes tab.\nGive the image a label starting with #fig-, e.g. #fig-cartoon\nYou can then refer to the image by @fig-cartoon (see next slide)."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-in-quarto-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#cross-references-in-quarto-1",
    "title": "Data Science and Data Analytics",
    "section": "Cross-references in Quarto",
    "text": "Cross-references in Quarto"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#citations-in-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#citations-in-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Citations in Quarto",
    "text": "Citations in Quarto\n\nLet’s say we wanted to cite the paper in which the penguin data was originally published. It can be found here.\nTo insert a citation in the visual editor, simply select Insert &gt; Citation at the place where you want the citation to be. RStudio then gives you several options. The easiest way is to paste the DOI of the paper:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#citations-in-quarto-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#citations-in-quarto-1",
    "title": "Data Science and Data Analytics",
    "section": "Citations in Quarto",
    "text": "Citations in Quarto\nAt the next rendering, Quarto will create a .bib file holding the added references, add the citation at the place where you have inserted it and append a list of references to the end of your document."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats",
    "text": "Quarto formats\n\nOne of the many great things about Quarto is that it offers many different output formats in which to render our report.\nSwitching between output formats is very easy: simply change the format in the YAML header.\nFor reports, the most common formats are:\n\npdf: what we were using for our penguin report so far.\nhtml: an HTML document with more flexibility than a PDF (e.g. animations).\ndocx: a Microsoft Word document.\n\nFor presentations, the most common formats are:\n\nrevealjs: an HTML-based presentation format allowing for interactivity, animations, dynamic content and much more.\npptx: a Microsoft PowerPoint presentation."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-switching-to-html",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-switching-to-html",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Switching to HTML",
    "text": "Quarto formats – Switching to HTML\n\nAs you may have noticed, the penguin emojis we included into our Quarto document were not rendered into the output PDF. This is because the PDF output format cannot handle emojis.\nTo remedy that, let’s switch to html in the YAML header:\ntitle: \"Penguins in Antarctica\"\nauthor: \"Julian Amon\"\ndate: today\nformat: html\nbibliography: references.bib\nWhen we now click on , the report will be generated as an HTML and not as a PDF. Note how it changes in appearance slightly and now correctly displays the emojis we added earlier."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-switching-to-html-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-switching-to-html-1",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Switching to HTML",
    "text": "Quarto formats – Switching to HTML"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Presentation with revealjs",
    "text": "Quarto formats – Presentation with revealjs\n\nAs indicated before, Quarto allows us not only to generate reports, but also presentations.\nWhile in principle, we could also simply switch the format to one of the presentation formats (e.g. revealjs or pptx) to generate a presentation, there are some special things to consider when doing that.\nPresentations work by dividing your content into slides. To do this with Quarto, the different level headings receive a special meaning when the output format is revealjs or pptx:\n\nSecond level headings (starting with ##) demarcate slides, i.e. a new second level heading will create a new slide.\nFirst level headings (starting with #) indicate the beginning of a new section with a section title slide."
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs-1",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Presentation with revealjs",
    "text": "Quarto formats – Presentation with revealjs\nA really basic presentation for the penguin example could look like this:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs-2",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#quarto-formats-presentation-with-revealjs-2",
    "title": "Data Science and Data Analytics",
    "section": "Quarto formats – Presentation with revealjs",
    "text": "Quarto formats – Presentation with revealjs\nWhen rendered, this looks like this:"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#presentations-with-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#presentations-with-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Presentations with Quarto",
    "text": "Presentations with Quarto\n\nQuarto has an incredible set of features for presentations. In fact, all of the slides for this course were created with Quarto!\nSome of these features include:\n\nCode highlighting and animations:\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\nSlide transitions\nTabsets (as in tabs splitting plot and code, for example)\nInteractive slides (see example on the next slide)\nand countless more…\n\nBasically, whatever you want to do, Quarto makes it possible! Thus, we cannot even remotely cover everything, features are best explored by yourself!"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#presentations-with-quarto-1",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#presentations-with-quarto-1",
    "title": "Data Science and Data Analytics",
    "section": "Presentations with Quarto",
    "text": "Presentations with Quarto"
  },
  {
    "objectID": "slides/06_The_Data_Science_Workflow_IV.html#further-references-for-quarto",
    "href": "slides/06_The_Data_Science_Workflow_IV.html#further-references-for-quarto",
    "title": "Data Science and Data Analytics",
    "section": "Further references for Quarto",
    "text": "Further references for Quarto\n\nThe main resource is the Quarto website. There you can find:\n\nBasic tutorials for getting started.\nSpecific guides on topics like presentations, computations, books, tools, authoring, websites, publishing, etc.\nA reference that explains all options for the possible output formats.\nA demo presentation made with revealjs that showcases some more of the cool features that Quarto offers for presentations.\n\nA Markdown tutorial for practice with elementary Markdown.\nA Markdown table generator that gives you the option to create Markdown tables in a very convenient way (including importing from Excel or similar).\nThe R for Data Science book, which is freely available online. In its last two chapters, it also contains a great introduction to Quarto."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#the-data-science-workflow-model",
    "href": "slides/05_The_Data_Science_Workflow_III.html#the-data-science-workflow-model",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Model",
    "text": "The Data Science workflow – Model"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#time-to-put-the-data-to-use",
    "href": "slides/05_The_Data_Science_Workflow_III.html#time-to-put-the-data-to-use",
    "title": "Data Science and Data Analytics",
    "section": "Time to put the data to use…",
    "text": "Time to put the data to use…\n\nHaving imported, tidied, transformed and visualized our data, now it is time to actually put this data to good use…\nThe goal of the modeling stage of the data science workflow is therefore to detect patterns and extract information from data.\nThe tools and processes we use for this purpose stem from a field called machine learning (or statistical learning).\nTo motivate our study of this field, let’s begin with a simple example:\n\nWe are statistical consultants hired to investigate the association between advertising and sales of a particular product.\nWe are given a data set consisting of the sales of that product in 200 different markets, along with advertising budgets for three different media: social_media, radio and newspaper."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example",
    "title": "Data Science and Data Analytics",
    "section": "Machine learning – A motivating example",
    "text": "Machine learning – A motivating example\n\nOur client cannot directly increase sales of course… But, they can control advertising expenses in each of the three media.\nOur goal is therefore to develop an accurate model that can be used to predict sales on the basis of the three media budgets."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example-1",
    "title": "Data Science and Data Analytics",
    "section": "Machine learning – A motivating example",
    "text": "Machine learning – A motivating example\n\nIn this setting, the advertising budgets are input variables:\n\nInput variables are also called predictors or features.\nWe denote them by \\(X\\) with a subscript to distinguish them.\nIn the example, \\(X_1\\) is the social_media budget, \\(X_2\\) the radio budget and \\(X_3\\) the newspaper budget.\n\nOn the other hand, sales is an output variable:\n\nThe output variable is also called the response (variable) or label.\nWe denote it by \\(Y\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-a-motivating-example-2",
    "title": "Data Science and Data Analytics",
    "section": "Machine learning – A motivating example",
    "text": "Machine learning – A motivating example\n\nUsing this notation more generally, suppose that we observe a quantitative response \\(Y\\) and \\(p\\) different predictors \\(X_1, X_2, \\ldots, X_p\\).\nWe assume that there is some relationship between \\(Y\\) and \\(X = (X_1, X_2, \\ldots, X_p)\\), which can be written in the very general form \\[Y = f(X) + \\epsilon,\\] where \\(f\\) is some fixed but unknown function of \\(X_1, \\ldots, X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero.\nWe can think of \\(f\\) as the systematic information that \\(X\\) provides about \\(Y\\).\nA large part of the machine learning literature revolves around different approaches for estimating \\(f\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#why-estimate-f-prediction",
    "href": "slides/05_The_Data_Science_Workflow_III.html#why-estimate-f-prediction",
    "title": "Data Science and Data Analytics",
    "section": "Why estimate \\(f\\)? – Prediction",
    "text": "Why estimate \\(f\\)? – Prediction\n\nEstimate \\(f\\) for prediction purposes: For inputs \\(X\\), we can predict \\(Y\\) using \\[\\hat{Y} = \\hat{f}(X),\\] where \\(\\hat{f}\\) represents our estimate for \\(f\\) and \\(\\hat{Y}\\) represents the resulting prediction for \\(Y\\).\nThe accuracy of \\(\\hat{Y}\\) as a prediction for \\(Y\\) depends on two quantities:\n\nReducible error: \\(\\hat{f}\\) will not be a perfect estimate for \\(f\\) and this inaccuracy will introduce some error. However, it is reducible by using more appropriate learning techniques or collecting more data.\nIrreducible error: Even if we knew \\(f\\), our prediction would have some error in it. This is because of the random error term \\(\\epsilon\\) affecting \\(Y\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#why-estimate-f-inference",
    "href": "slides/05_The_Data_Science_Workflow_III.html#why-estimate-f-inference",
    "title": "Data Science and Data Analytics",
    "section": "Why estimate \\(f\\)? – Inference",
    "text": "Why estimate \\(f\\)? – Inference\n\nWe might also estimate \\(f\\) for inference purposes, i.e. to understand the association between \\(Y\\) and \\(X_1, \\ldots, X_p\\).\nIn this context, one may be interested in answering the following questions:\n\nWhich predictors are associated with the response? Often only a small subset of the available predictors are important in explaining \\(Y\\).\nWhat is the relationship between the response and each predictor? Is it positive, negative, linear, non-linear, …?\n\nIn the advertising example, specific inference-related questions are:\n\nWhich media are associated with sales?\nWhich media generate the biggest boost in sales?\nHow large of an increase in sales is associated with a given increase in social media advertising?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#how-do-we-estimate-f",
    "href": "slides/05_The_Data_Science_Workflow_III.html#how-do-we-estimate-f",
    "title": "Data Science and Data Analytics",
    "section": "How do we estimate \\(f\\)?",
    "text": "How do we estimate \\(f\\)?\n\nWe assume that we have observed a set of \\(n\\) different data points. Let \\(x_{ij}\\) represent the value of the \\(j\\)th predictor for observation \\(i\\), where \\(i = 1, 2, \\ldots, n\\) and \\(j = 1, 2, \\ldots, p\\). Correspondingly, let \\(y_i\\) represent the response variable for observation \\(i\\).\nWith this, our data set consists of \\[\\{ (x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n) \\},\\] where \\(x_i = (x_{i1}, \\ldots, x_{ip})\\). These observations are called the training data because we will use them to train our method on how to estimate \\(f\\).\nOur goal with this is to estimate the unknown function \\(f\\), i.e. find a function \\(\\hat{f}\\) such that \\(Y \\approx \\hat{f}(X)\\) for any observation \\((X, Y)\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#parametric-methods-for-estimating-f",
    "href": "slides/05_The_Data_Science_Workflow_III.html#parametric-methods-for-estimating-f",
    "title": "Data Science and Data Analytics",
    "section": "Parametric methods for estimating \\(f\\)",
    "text": "Parametric methods for estimating \\(f\\)\n\nBroadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric.\nParametric methods involve a two-step model-based approach:\n\nMake an assumption about the function form of \\(f\\). For example, one very simple assumption is that \\(f\\) is linear in \\(X\\): \\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\\]\nFind and apply a procedure that uses training data to fit or train the model. In case of the linear model, that means finding values of parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) such that: \\[Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#non-parametric-methods-for-estimating-f",
    "href": "slides/05_The_Data_Science_Workflow_III.html#non-parametric-methods-for-estimating-f",
    "title": "Data Science and Data Analytics",
    "section": "Non-parametric methods for estimating \\(f\\)",
    "text": "Non-parametric methods for estimating \\(f\\)\n\nNon-parametric methods do not make explicit assumptions about the functional form of \\(f\\). Instead, they seek an estimate of \\(f\\) that gets as close to the training data points as possible without being too wiggly.\nAdvantage: can accommodate a much wider range of possible shapes for \\(f\\) compared to parametric approaches.\nDisadvantage: do not reduce the problem of estimating \\(f\\) to a small number of parameters \\(\\to\\) a very large number of observations is needed.\n\n\n\n\n\n\n\n\nParametric fit\n\n\n\n\n\n\nNon-parametric fit"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#estimating-f-as-part-of-supervised-learning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#estimating-f-as-part-of-supervised-learning",
    "title": "Data Science and Data Analytics",
    "section": "Estimating \\(f\\) as part of supervised learning",
    "text": "Estimating \\(f\\) as part of supervised learning\n\nFitting a model for the purposes of prediction or inference based on labeled training data \\((x_i, y_i)\\) characterizes a class of machine learning problems called supervised learning.\nFor these problems, there is an associated output \\(y_i\\) for each input \\(x_i\\). These labels act as a “supervisor” guiding the learning process, similar to a student learning under a teacher’s guidance."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-vs.-classification-problems",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-vs.-classification-problems",
    "title": "Data Science and Data Analytics",
    "section": "Regression vs. classification problems",
    "text": "Regression vs. classification problems\n\nSupervised learning problems can be further sub-categorized into regression and classification problems depending on the type of response variable:\n\nProblems with a quantitative response are called regression problems, e.g. modeling sales on the basis of the three media budgets.\nProblems with a categorical response are called classification problems, e.g. modeling consumer credit default based on economic variables. Depending on the number of categories, these problems are either binary (e.g. yes/no) or multiclass (e.g. Adelie/Chinstrap/Gentoo).\n\nWhile we select supervised learning methods on the basis of whether the response is quantitative or categorical, the type of the predictors is less important. Typically, methods can be applied with both quantitative and categorical predictors."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning",
    "title": "Data Science and Data Analytics",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nBy contrast, unsupervised learning describes the somewhat more challenging situation in which we only observe a vector of measurements \\(x_i\\), but no associated response \\(y_i\\).\nWe cannot fit a model because there is no response variable to predict.\nWhat sort of analysis can we do then?\n\nClustering: Assigning observations into relatively distinct groups, e.g. market segmentation: grouping customers into clusters based on socio-demographic characteristics.\nOutlier detection: Identifying data points that significantly deviate from the norm within a data set, potentially revealing errors, unusual events, or important discoveries.\n…"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-clustering",
    "href": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-clustering",
    "title": "Data Science and Data Analytics",
    "section": "Unsupervised learning – Clustering",
    "text": "Unsupervised learning – Clustering"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-overview-with-examples",
    "href": "slides/05_The_Data_Science_Workflow_III.html#machine-learning-overview-with-examples",
    "title": "Data Science and Data Analytics",
    "section": "Machine learning – Overview with examples",
    "text": "Machine learning – Overview with examples"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-measuring-the-quality-of-fit",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-measuring-the-quality-of-fit",
    "title": "Data Science and Data Analytics",
    "section": "Regression – Measuring the quality of fit",
    "text": "Regression – Measuring the quality of fit\n\nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to quantify how far the predicted response values are from the true response values.\nFor regression problems, the most commonly-used measure is the mean squared error (MSE): \\[\\text{MSE}_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\]\n\nThe MSE will be small if the predicted responses are very close to the true responses.\nThe MSE will be large if the predicted responses and the true responses differ substantially."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test MSE",
    "text": "Training vs. test MSE\n\nAs we use our \\(n\\) training observations to compute the MSE on the previous slide, we call this the training MSE.\nHowever, in general, we do not really care about how well the method works on the training data. Rather, we are interested in the prediction accuracy on previously unseen test data.\nConsider the following examples:\n\nWhen fitting a model to predict a stock price, we do not really care about how well it does on historical stock prices. Instead, we care about how well it predicts tomorrow’s stock price.\nWhen estimating a model to predict whether someone has diabetes based on clinical measurements (e.g. weight, blood pressure, age, …), we want to use this model to predict diabetes risk for future patients, not for the ones used for training."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-1",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test MSE",
    "text": "Training vs. test MSE\n\nMathematically speaking, suppose we have a set of \\(n'\\) previously unseen test observations \\((x_i', y_i')\\) that were not used to train the machine learning method. Then we can compute the test MSE as: \\[\\text{MSE}_{\\text{test}} = \\frac{1}{n'} \\sum_{i=1}^{n'} (y_i' - \\hat{f}(x_i'))^2\\]\nWe can use this quantity to compare different methods: we want to select the method for which it is as small as possible.\nNow, how do we go about minimizing test MSE on unseen data? Can’t we just select the method that minimizes training MSE? Turns out this is not a good idea at all…"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-2",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test MSE",
    "text": "Training vs. test MSE\n\n\nSuppose we observe the data points in the left panel, which come from the true \\(f\\) shown in black. We estimate it using three different methods with increasing levels of flexibility (low: orange, medium: blue, high: green).\nThe right panel shows the corresponding training and test MSEs."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-mse-3",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test MSE",
    "text": "Training vs. test MSE\n\nClearly, the best fit is achieved by the blue curve, the orange (linear) one is too inflexible, the green one is too flexible (too wiggly).\nThe comparison of train and test MSE as a function of flexibility reveals an interesting pattern:\n\nThe training MSE is monotonically decreasing: the more flexible the method, the better we fit our training data.\nHowever, the all-important test MSE exhibits a U-shape: it initially decreases with increasing flexibility, but then levels off and eventually increases again.\n\nThis is a fundamental property of statistical learning that holds regardless of data set and statistical method:\nAs model flexibility increases, the training MSE will decrease, but the test MSE may not."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting",
    "href": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting",
    "title": "Data Science and Data Analytics",
    "section": "Over- and underfitting",
    "text": "Over- and underfitting\n\nThe green fit has small training MSE, but high test MSE. We call this overfitting:\n\nWith its high flexibility, this method is fitting patterns in the training data that are just caused by random chance rather than by true properties of \\(f\\).\nThe test MSE will then be very large because the supposed patterns that the method found in the training data simply do not exist in the test data.\n\nConversely, the orange (linear) fit has large training MSE and large test MSE. We call this underfitting:\n\nWith its low flexibility, namely its assumption of linearity, this method is not able to fit the non-linearity of the true \\(f\\) well, even on the training data.\nBy increasing flexibility from here, we are able to use more information contained in \\(X\\) to improve the fit.\n\nConsequently, the blue fit with medium-level flexibility is close to optimal."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting-1",
    "title": "Data Science and Data Analytics",
    "section": "Over- and underfitting",
    "text": "Over- and underfitting\nLet’s see two more examples.\n\n\n\n\nHere, the true \\(f\\) is approximately linear.\nThe U-shape in test MSE is still there, but because the truth is close to linear, the test MSE only decreases slightly before increasing again.\n\n\n\n\nHere, the true \\(f\\) is highly non-linear.\nThe MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off",
    "href": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off",
    "title": "Data Science and Data Analytics",
    "section": "The bias-variance trade-off",
    "text": "The bias-variance trade-off\n\nThe U-shape observed in test MSE curves turns out to be the result of two competing properties of statistical learning methods: \\[\\text{Expected test MSE} = \\text{Var}(\\hat{f}) + \\text{Bias}(\\hat{f})^2 + \\text{Var}(\\epsilon)\\]\nSo we want methods that simultaneously achieve low variance and low bias. What is meant by these terms in this context?\n\nVariance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. In general, more flexible statistical methods have higher variance.\nBias refers to the error that is introduced by approximating a complicated real-life problem by a much simpler model. In general, more flexible statistical methods result in less bias."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off-1",
    "title": "Data Science and Data Analytics",
    "section": "The bias-variance trade-off",
    "text": "The bias-variance trade-off\nBelow, we decompose the three MSE curves from earlier into their three constituent parts: the variance of \\(\\hat{f}\\), its squared bias and \\(\\text{Var}(\\epsilon)\\), the irreducible error (represented by the dashed line):"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#the-bias-variance-trade-off-2",
    "title": "Data Science and Data Analytics",
    "section": "The bias-variance trade-off",
    "text": "The bias-variance trade-off\n\nThese plots reveal the following insights:\n\nAs the method’s flexibility increases, the variance increases and the bias decreases.\nThe relative rate of change of these two quantities determine whether the test MSE (as the sum of the two plus \\(\\text{Var}(\\epsilon)\\)) increases or decreases.\nThis is what we call the bias-variance trade-off: bias and variance need to be balanced optimally to minimize test MSE.\nThis trade-off is different for every data set and method applied to it.\n\nDue to the particularities of different data sets, there is no free lunch in statistical learning: no one method optimally trades off bias and variance on all possible data sets."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-measuring-the-quality-of-fit",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-measuring-the-quality-of-fit",
    "title": "Data Science and Data Analytics",
    "section": "Classification – Measuring the quality of fit",
    "text": "Classification – Measuring the quality of fit\n\nWith MSE, our discussion of model performance has so far focused on the regression setting. But many concepts we encountered easily transfer over to the classification setting.\nOne thing that does change is the metric for measuring the quality of fit. With \\(y_1, \\ldots, y_n\\) now categorical, the most common approach for quantifying model accuracy is the training error rate \\(ER\\): \\[ER_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n I(y_i \\neq \\hat{y}_i),\\] where \\(\\hat{y}_i\\) is the predicted class label for the \\(i\\)th observation using \\(\\hat{f}\\) and \\(I(y_i \\neq \\hat{y}_i)\\) is an indicator variable that equals 1 if \\(\\hat{y}_i \\neq y_i\\) and 0 if \\(\\hat{y}_i = y_i\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-error-rate",
    "href": "slides/05_The_Data_Science_Workflow_III.html#training-vs.-test-error-rate",
    "title": "Data Science and Data Analytics",
    "section": "Training vs. test error rate",
    "text": "Training vs. test error rate\n\nAs in the regression context, we are mostly interested in the prediction accuracy on previously unseen test data.\nTherefore, if we have a test set of \\(n'\\) observations \\((x_i',y_i')\\) that were not used to train our method, we can compute the test error rate as: \\[ER_{\\text{test}} = \\frac{1}{n'} \\sum_{i=1}^n I(y_i' \\neq \\hat{y}_i')\\]\nAgain, we can use this quantity to compare different classification methods: we want to select the method for which the test error rate is as small as possible.\nLet’s consider an example…"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-example",
    "title": "Data Science and Data Analytics",
    "section": "Classification example",
    "text": "Classification example\nSuppose we wanted to predict whether an individual will default on his or her credit card payment on the basis of annual income (\\(X_1\\)) and monthly credit card balance (\\(X_2\\)). Then we might use the \\(x\\) and the \\(y\\) axis to display the two predictors and colour to indicate default (orange) or no default (blue), like so:\n\n\n\n\n\n\n\n\n\nOrange and blue circles indicate training observations.\nFor each value of \\(X_1\\) and \\(X_2\\), there is a different probability of the response being orange or blue.\nThe orange shaded region reflects the set of points for which \\(\\Pr(Y = \\text{orange}|X) &gt; 0.5\\).\nThe purple line indicates the true decision boundary."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-example-1",
    "title": "Data Science and Data Analytics",
    "section": "Classification example",
    "text": "Classification example\n\nThe displayed data set is based on simulation, so that we know the true decision boundary.\nTypically, this is not the case of course and we only observe training data points. The goal then is to come as close as possible to the (unknown) true decision boundary.\nNote two parallels to the regression setting:\n\nEven if we knew the true decision boundary, we would commit errors. This is because there is some irreducible error.\nIn choosing a classification method, we also have to trade off bias and variance to avoid over-/underfitting."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting-regression-vs.-classification",
    "href": "slides/05_The_Data_Science_Workflow_III.html#over--and-underfitting-regression-vs.-classification",
    "title": "Data Science and Data Analytics",
    "section": "Over- and underfitting – Regression vs. classification",
    "text": "Over- and underfitting – Regression vs. classification"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data",
    "href": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data",
    "title": "Data Science and Data Analytics",
    "section": "How to get test data",
    "text": "How to get test data\n\nTo assess when we are likely to overfit, we need some way of measuring the MSE (regression) or ER (classification) on unseen test data. How do we do that?\nAnswer: use the training data in a smart way!\nThere are many different approaches, two prominent ones are:\n\nTrain/test split: split the data randomly into a train and test sample and compute error on the test sample.\nK-fold cross-validation: split the entire data randomly into \\(K\\) folds. Fit the model using the \\(K-1\\) folds and evaluate the model using the remaining fold. Repeat this process until every fold has served as the test set."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data-traintest-split",
    "href": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data-traintest-split",
    "title": "Data Science and Data Analytics",
    "section": "How to get test data – Train/test split",
    "text": "How to get test data – Train/test split"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data-k-fold-cross-validation",
    "href": "slides/05_The_Data_Science_Workflow_III.html#how-to-get-test-data-k-fold-cross-validation",
    "title": "Data Science and Data Analytics",
    "section": "How to get test data – K-fold cross validation",
    "text": "How to get test data – K-fold cross validation"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression",
    "title": "Data Science and Data Analytics",
    "section": "Introduction to linear regression",
    "text": "Introduction to linear regression\n\nLet’s finally get to building our first machine learning model!\nOne of the most widely used tools for predicting a quantitative variable is linear regression.\nAs the name suggests, linear regression assumes that the dependence of a response variable \\(Y\\) on predictors \\(X_1, \\ldots, X_p\\) is linear.\nEven if true relationships are rarely linear, this simple approach is extremely useful both conceptually and practically.\nLet’s start with the simplest case: predicting a quantitative variable \\(Y\\) on the basis of a single predictor \\(X\\). We assume therefore: \\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon,\\] where \\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters or coefficients."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "Introduction to linear regression",
    "text": "Introduction to linear regression\n\nIn the case of simple linear regression, there are only two coefficients:\n\nThe slope coefficient \\(\\beta_1\\). It specifies how much \\(Y\\) changes on average for a given change in \\(X\\).\nThe intercept \\(\\beta_0\\). It specifies the value that \\(Y\\) takes on average, if \\(X = 0\\).\n\nGiven some estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we can predict future values of \\(Y\\) by using the regression line: \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x,\\] where \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of \\(X\\) being equal to \\(x\\).\nFor the purposes of illustration, let’s come back to the example with sales as a function of advertising budgets."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-linear-regression-2",
    "title": "Data Science and Data Analytics",
    "section": "Introduction to linear regression",
    "text": "Introduction to linear regression\nSuppose we are only interested in the relationship between the social_media budget and sales. Our goal is to obtain estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), such that \\(y_i \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\) for \\(i = 1, \\ldots, n\\). There are multiple ways to do this."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-through-ols",
    "href": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-through-ols",
    "title": "Data Science and Data Analytics",
    "section": "Estimating coefficients through OLS",
    "text": "Estimating coefficients through OLS\n\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\) be the fitted value for \\(Y\\) based on the \\(i\\)th observation for \\(X\\). Then \\(e_i = y_i - \\hat{y}_i\\) represents the \\(i\\)th residual.\nOne possibility to choose the estimates for the coefficients is to minimize the residual sum of squares (RSS): \\[RSS = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2.\\]\nThe values of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize this expression are called ordinary least squares (OLS) estimators and are given by: \\[\\hat{\\beta}_1 = \\frac{\\frac{1}{n} \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\frac{1}{n} \\sum_i (x_i - \\bar{x})^2}, \\quad \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x},\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) denote the means of the variables."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-example",
    "title": "Data Science and Data Analytics",
    "section": "Estimating coefficients – Example",
    "text": "Estimating coefficients – Example\n\nEstimating the parameters for the simple linear regression where social_media is \\(X\\) and sales is \\(Y\\), we get: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot X = 7.0326 + 0.0475 \\cdot X\\]\nInterpretation of the coefficients:\n\n\\(\\hat{\\beta}_0 = 7.0326\\): when no money is invested in social media advertising, the model expects average sales of 7.0326 units.\n\\(\\hat{\\beta}_1 = 0.0475\\): for every additional 100 dollars spent on social media advertising, the expected sales increase by around \\(100 \\cdot 0.0475 \\approx 4.75\\) units on average."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#estimating-coefficients-example-1",
    "title": "Data Science and Data Analytics",
    "section": "Estimating coefficients – Example",
    "text": "Estimating coefficients – Example"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit",
    "href": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit",
    "title": "Data Science and Data Analytics",
    "section": "Assessing the goodness-of-fit",
    "text": "Assessing the goodness-of-fit\n\nHow well a regression line fits the data is described by its goodness-of-fit. It is typically assessed by the coefficient of determination \\(R^2\\).\nThe \\(R^2\\) measures the proportion of explained variance: \\[R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2},\\] where \\(TSS\\) is the total sum of squares as defined by the expression in the denominator.\nThe value of the \\(R^2\\) lies between 0 and 1:\n\n\\(R^2 = 1\\): 100% of the variance of \\(Y\\) can be explained by \\(X\\).\n\\(R^2 = 0\\): 0% of the variance of \\(Y\\) can be explained by \\(X\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-1",
    "title": "Data Science and Data Analytics",
    "section": "Assessing the goodness-of-fit",
    "text": "Assessing the goodness-of-fit"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#statistical-properties-of-ols-estimators",
    "href": "slides/05_The_Data_Science_Workflow_III.html#statistical-properties-of-ols-estimators",
    "title": "Data Science and Data Analytics",
    "section": "Statistical properties of OLS estimators",
    "text": "Statistical properties of OLS estimators\n\nUnder certain assumptions on the underlying true regression model, we can derive sampling distributions of the OLS estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), i.e. how these estimators will be distributed when computing them over several data sets.\nWe can use these sampling distributions use for testing hypotheses like \\[H_0: \\beta_1 = 0, \\quad \\quad H_1: \\beta_1 \\neq 0\\]\n… or to compute two-sided confidence intervals like \\[[\\hat{\\beta}_1 - t_{1-\\alpha/2;n-2} \\cdot \\hat{\\sigma}_{\\hat{\\beta}_1}; \\hat{\\beta}_1 + t_{1-\\alpha/2;n-2} \\cdot \\hat{\\sigma}_{\\hat{\\beta}_1}],\\] where \\(t_{1-\\alpha/2;n-2}\\) is the \\((1-\\alpha/2)\\)-quantile of a \\(t\\)-distribution with \\(n-2\\) degrees of freedom and \\(\\hat{\\sigma}_{\\hat{\\beta}_1}\\) is the standard error of the estimator of the slope coefficient \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\nHowever, this is a data science course and not a statistics course, so we will not compute these quantities by hand. Instead, we want to use R for all this.\nTo estimate a linear regression model in R, we need two things to pass into the function lm (which stands for “linear model”):\n\nA data.frame holding the variables we want included in the model.\nA formula that specifies the model. In simple linear regression, this will take the form of response ~ predictor.\n\nSo, in our advertising example, we would do the following:\n\nadvertising &lt;- read.csv(\"data/advertising.csv\")\nadvertising &lt;- advertising[, -c(1, 2, 7)]\n\nsimple_reg &lt;- lm(sales ~ social_media, data = advertising)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\nWe can then have a look at the result using the summary function:\n\nsimple_reg_summary &lt;- summary(simple_reg)\nsimple_reg_summary\n\n\nCall:\nlm(formula = sales ~ social_media, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.3860 -1.9545 -0.1913  2.0671  7.2124 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.032594   0.457843   15.36   &lt;2e-16 ***\nsocial_media 0.047537   0.002691   17.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.259 on 198 degrees of freedom\nMultiple R-squared:  0.6119,    Adjusted R-squared:  0.6099 \nF-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nThis output contains a lot of useful information. First of all:\n\nCall: summary of the estimated regression model\nResiduals: a five-point-summary of the residuals \\(e_i\\)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\n\nThen, it displays a table called Coefficients with six columns:\n\n1st column: name of the variable pertaining to the corresponding coefficient (here only (Intercept) and social_media).\n2nd column (Estimate): OLS estimate of the coefficient, so \\(\\hat{\\beta}_i\\) for \\(i = 0, 1\\).\n3rd column (Std. Error): Estimate of the coefficient standard errors, so \\(\\hat{\\sigma}_{\\hat{\\beta}_i}\\).\n4th column (t value): Test statistic \\(t\\) for the two-sided hypothesis test against 0, i.e. to test the null hypothesis \\(H_0: \\beta_i = 0\\) against the alternative \\(H_1: \\beta_i \\neq 0\\).\n5th column (Pr(&gt;|t|)): \\(p\\)-value of the corresponding hypothesis test.\n6th column: “stars of significance”\n\nFinally, additional information is displayed:\n\nResidual standard error: Estimate of the square root of \\(\\text{Var}(\\epsilon)\\).\nMultiple R-squared: \\(R^2\\)\nAdjusted R-squared / F-statistic: see example with multiple predictors."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-accessor-functions",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-accessor-functions",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R – Accessor functions",
    "text": "Linear Regression in R – Accessor functions\n\nFrom the created model object simple_reg, we can extract:\n\nThe coefficient estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) using the function coef:\n\n\ncoef(simple_reg)\n\n (Intercept) social_media \n  7.03259355   0.04753664 \n\n\n\nThe fitted values \\(\\hat{y}_i\\) using the function fitted:\n\n\nhead(fitted(simple_reg))\n\n        1         2         3         4         5         6 \n17.970775  9.147974  7.850224 14.234395 15.627218  7.446162 \n\n\n\nThe residuals \\(e_i = y_i - \\hat{y}_i\\) using the function residuals:\n\n\nhead(residuals(simple_reg))\n\n         1          2          3          4          5          6 \n 4.1292255  1.2520260  1.4497762  4.2656054 -2.7272181 -0.2461623 \n\n\n\nThe residual standard error using the function sigma:\n\n\nsigma(simple_reg)\n\n[1] 3.258656"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-accessor-functions-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-accessor-functions-1",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R – Accessor functions",
    "text": "Linear Regression in R – Accessor functions\n\n\nConfidence intervals for all regression coefficients at level \\(1 - \\alpha\\) using the function confint and its level argument:\n\n\nconfint(simple_reg, level = 0.95)\n\n                  2.5 %     97.5 %\n(Intercept)  6.12971927 7.93546783\nsocial_media 0.04223072 0.05284256\n\n\n\n\n\nAdditionally, we can extract the following quantities from the created model summary object simple_reg_summary:\n\nThe coefficient estimates, their standard errors, \\(t\\) and \\(p\\) values using the function coef:\n\n\ncoef(simple_reg_summary)\n\n               Estimate  Std. Error  t value    Pr(&gt;|t|)\n(Intercept)  7.03259355 0.457842940 15.36028 1.40630e-35\nsocial_media 0.04753664 0.002690607 17.66763 1.46739e-42\n\n\n\nThe \\(R^2\\) by accessing the r.squared field of the corresponding list:\n\n\nsimple_reg_summary$r.squared\n\n[1] 0.6118751"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-prediction",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-regression-in-r-prediction",
    "title": "Data Science and Data Analytics",
    "section": "Linear Regression in R – Prediction",
    "text": "Linear Regression in R – Prediction\n\nIf we are interested in prediction rather than inference, we need a way to predict new data points. In R, this functionality is provided by the predict function.\nIt requires the estimated model we want to use for prediction and the values of the features for which predictions should be generated in a data.frame.\nSuppose, we want to predict sales for social_media budgets of 100 and 200:\n\nnew_data &lt;- data.frame(social_media = c(100, 200))\npredict(simple_reg, newdata = new_data)\n\n       1        2 \n11.78626 16.53992 \n\n\nNote that the names of the features (here only social_media) in newdata have to be identical to their names in the training data:\n\nnew_data &lt;- data.frame(social_med = c(100, 200))\npredict(simple_reg, newdata = new_data)\n\nError in eval(predvars, data, env): Objekt 'social_media' nicht gefunden"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r",
    "title": "Data Science and Data Analytics",
    "section": "K-fold cross-validation in R",
    "text": "K-fold cross-validation in R\n\nNow, we have all we need to estimate the test MSE of our model using K-fold cross validation! Let’s say for \\(K = 5\\).\nLet’s go step by step: first, we randomly assign each observation to one of the 5 folds. The randomness is once again introduced via the function sample:\n\nn &lt;- nrow(advertising)\nK &lt;- 5\n\nadvertising$fold &lt;- sample(rep(1:K, length.out = n))\nhead(advertising, 10)\n\n   social_media radio newspaper sales fold\n1         230.1  37.8      69.2  22.1    1\n2          44.5  39.3      45.1  10.4    1\n3          17.2  45.9      69.3   9.3    5\n4         151.5  41.3      58.5  18.5    5\n5         180.8  10.8      58.4  12.9    3\n6           8.7  48.9      75.0   7.2    4\n7          57.5  32.8      23.5  11.8    1\n8         120.2  19.6      11.6  13.2    3\n9           8.6   2.1       1.0   4.8    3\n10        199.8   2.6      21.2  10.6    2"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "K-fold cross-validation in R",
    "text": "K-fold cross-validation in R\n\nNext, we write a loop, in which for every fold \\(k = 1, ..., 5\\):\n\nWe fit our simple linear regression model on all data except the \\(k\\)th fold.\nUsing this model, we predict sales on the \\(k\\)th fold.\nSince we know the true sales for all observations, we can then compute the MSE on the \\(k\\)th fold.\nSave the MSE of the \\(k\\)th fold in a vector.\n\n\nmses_linear &lt;- numeric(K)\n\nfor(k in 1:K){\n  m &lt;- lm(sales ~ social_media, data = subset(advertising, fold != k))\n  preds &lt;- predict(m, newdata = subset(advertising, fold == k))\n  mse &lt;- mean((preds - advertising$sales[advertising$fold == k])^2)\n  mses_linear[k] &lt;- mse\n}\nmses_linear\n\n[1]  7.03702 10.98168 12.21465 10.85549 12.54239"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-fold-cross-validation-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "K-fold cross-validation in R",
    "text": "K-fold cross-validation in R\n\nFinally, we can compute the cross-validated MSE by calculating the mean of the MSEs across the 5 folds:\n\nmean(mses_linear)\n\n[1] 10.72624\n\n\nSince the MSE lives on a squared scale (due to the squaring in its computation), we can bring the scale back to the original linear scale by taking the square root. The result is called the root mean squared error (RMSE):\n\nsqrt(mean(mses_linear))\n\n[1] 3.275095\n\n\nIn and of itself, these numbers do not tell us too much yet. However, we will be comparing them against the cross-validated (R)MSE of other methods, to see which one is the best."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#incorporating-non-linearities",
    "href": "slides/05_The_Data_Science_Workflow_III.html#incorporating-non-linearities",
    "title": "Data Science and Data Analytics",
    "section": "Incorporating non-linearities",
    "text": "Incorporating non-linearities\n\nThe term linear in “linear regression” actually refers only to linearity in parameters \\(\\beta_0\\) and \\(\\beta_1\\).\nWe can therefore easily incorporate non-linearities by appropriately defining the dependent variable \\(Y\\) and the independent variable \\(X\\). For example:\n\n\\(\\log(Y) = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\)\n\\(\\log(Y) = \\beta_0 + \\beta_1 \\cdot \\log(X) + \\epsilon\\)\n\\(Y = \\beta_0 + \\beta_1 \\cdot \\sqrt{X} + \\epsilon\\)\n\nLet’s estimate each of these models in our advertising example and assess their test MSEs using 5-fold cross validation again. First, we add the log and square root of the relevant variable to our data set:\n\nadvertising$log_sales &lt;- log(advertising$sales)\nadvertising$log_social_media &lt;- log(advertising$social_media)\nadvertising$sqrt_social_media &lt;- sqrt(advertising$social_media)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#incorporating-non-linearities-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#incorporating-non-linearities-1",
    "title": "Data Science and Data Analytics",
    "section": "Incorporating non-linearities",
    "text": "Incorporating non-linearities\n\nNow, let’s assess the test MSE of each of the three above models using 5-fold cross validation:\n\nmses_nonlinear &lt;- matrix(0, K, 3)\n\nfor(k in 1:K){\n  m1 &lt;- lm(log_sales ~ social_media, data = subset(advertising, fold != k))\n  m2 &lt;- lm(log_sales ~ log_social_media, data = subset(advertising, fold != k))\n  m3 &lt;- lm(sales ~ sqrt_social_media, data = subset(advertising, fold != k))\n\n  preds1 &lt;- exp(predict(m1, newdata = subset(advertising, fold == k)))\n  preds2 &lt;- exp(predict(m2, newdata = subset(advertising, fold == k)))\n  preds3 &lt;- predict(m3, newdata = subset(advertising, fold == k))\n\n  mses_nonlinear[k, 1] &lt;- mean((preds1 - advertising$sales[advertising$fold == k])^2)\n  mses_nonlinear[k, 2] &lt;- mean((preds2 - advertising$sales[advertising$fold == k])^2)\n  mses_nonlinear[k, 3] &lt;- mean((preds3 - advertising$sales[advertising$fold == k])^2)\n}\ncolMeans(mses_nonlinear)\n\n[1] 11.83003 10.71977 10.39436\n\n\nThe model \\(Y = \\beta_0 + \\beta_1 \\cdot \\sqrt{X}\\) gives the lowest test MSE out of the three and lower than the linear model as well, albeit not by much."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nTo improve our model even further, we probably have to include additional features into it. For instance, sales will probably also depend on the radio and newspaper advertising budget.\nIn the context of the linear model, we can include additional variables to create a multiple linear regression of the form \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\\]\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed (i.e. ceteris paribus).\nAs before, the regression coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are unknown and need to be estimated from the training data."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-ols-estimation",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-ols-estimation",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression – OLS estimation",
    "text": "Multiple linear regression – OLS estimation\n\nAs in the simple linear regression case, we choose the parameters to minimize the residual sum of squares (RSS): \\[RSS = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\ldots + \\hat{\\beta}_px_{pi}))^2\\]\nThe values of \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize this expression are complicated formulas of the data that are most easily represented using matrix algebra. These formulas do not really matter to us, as R computes them for us anyway.\nEstimating parameters for the multiple linear regression where sales is \\(Y\\), social_media is \\(X_1\\), radio is \\(X_2\\) and newspaper is \\(X_3\\), we get: \\[\\hat{Y} = 2.9389 + 0.0458 \\cdot X_1 + 0.1885 \\cdot X_2 - 0.0010 \\cdot X_3\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-ols-estimation-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-ols-estimation-1",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression – OLS estimation",
    "text": "Multiple linear regression – OLS estimation\n\nLet’s again go through the interpretation of each of those coefficients:\n\n\\(\\hat{\\beta}_0 = 2.9389\\): when no money is invested in advertising through any of the three channels, the model expects average sales of 2.9389 units.\n\\(\\hat{\\beta}_1 = 0.0458\\): for every additional 100 dollars spent on social media advertising ceteris paribus, the expected sales increase by around \\(100 \\cdot 0.0458 \\approx 4.58\\) units on average.\n\\(\\hat{\\beta}_2 = 0.1885\\): for every additional 100 dollars spent on radio advertising ceteris paribus, the expected sales increase by around \\(100 \\cdot 0.1885 \\approx 18.85\\) units on average.\n\\(\\hat{\\beta}_3 = -0.0010\\): for every additional 100 dollars spent on newspaper advertising ceteris paribus, the expected sales decrease by around \\(100 \\cdot 0.0010 \\approx 0.10\\) units on average."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-2",
    "title": "Data Science and Data Analytics",
    "section": "Assessing the goodness-of-fit",
    "text": "Assessing the goodness-of-fit\n\nThe \\(R^2\\) is still computed and interpreted in the same way as before: \\[R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\nIt now measures the proportion of the variance of \\(Y\\) explained jointly by the predictors \\(X_1, \\ldots, X_p\\). Alternatively, one can say that it measures the proportion of the variance of \\(Y\\) explained by the model.\nWhen adding additional variables to the model, the \\(R^2\\) never decreases and usually increases. This makes it a poor tool for deciding whether an additional variable should be added to the model."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#assessing-the-goodness-of-fit-3",
    "title": "Data Science and Data Analytics",
    "section": "Assessing the goodness-of-fit",
    "text": "Assessing the goodness-of-fit\n\nAn alternative to the \\(R^2\\) that is the so-called adjusted \\(R^2\\): \\[R_{\\text{adj}}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\\]\nIt no longer has the interpretation of the \\(R^2\\), but it is more useful for comparing models. Consider the following example:\n\nThe simple linear regression of sales on social_media had an adjusted \\(R^2\\) of 0.6099 (see corresponding R output from earlier).\nThe multiple linear regression of sales on all three advertising budgets has an adjusted \\(R^2\\) of 0.8956 (see upcoming R output).\nTherefore, the adjusted \\(R^2\\) strongly suggests that the bigger model is superior to the smaller model."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#statistical-properties-of-ols-estimators-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#statistical-properties-of-ols-estimators-1",
    "title": "Data Science and Data Analytics",
    "section": "Statistical properties of OLS estimators",
    "text": "Statistical properties of OLS estimators\n\nUsing the standard assumptions of the multiple linear regression model, we can again derive sampling distributions of the OLS estimators \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\) and thus construct \\(t\\)-tests for hypotheses like \\[H_0: \\beta_j = 0, \\quad \\quad H_1: \\beta_j \\neq 0\\]\nAdditionally, we can test the null hypothesis that none of the coefficients are useful in predicting the response, i.e. \\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0, \\quad \\quad H_1: \\beta_j \\neq 0 \\text{ für min. ein } j,\\] using the \\(F\\)-statistic \\[F = \\frac{n-p-1}{p} \\cdot \\frac{TSS - RSS}{RSS} \\sim F_{p, n-p-1}\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression in R",
    "text": "Multiple linear regression in R\n\nHowever, again, this course focuses on how to use R to estimate these types of models and compute these types of statistics.\nTo estimate a multiple linear regression in R, we simply combine the predictors on the right side of the ~ using a + sign, i.e. response ~ predictor1 + predictor2 + ... + predictorp\nSo, to estimate the model with all three predictors in our advertising example, we would run the following line of code:\n\nmultiple_reg &lt;- lm(sales ~ social_media + radio + newspaper, data = advertising)\n\nFortunately, many of the other functionalities we have seen in simple linear regression neatly generalize to the multiple linear regression case!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-linear-regression-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "Multiple linear regression in R",
    "text": "Multiple linear regression in R\n\nAgain, we inspect the result using the summary function:\n\nsummary(multiple_reg)\n\n\nCall:\nlm(formula = sales ~ social_media + radio + newspaper, data = advertising)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.8277 -0.8908  0.2418  1.1893  2.8292 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.938889   0.311908   9.422   &lt;2e-16 ***\nsocial_media  0.045765   0.001395  32.809   &lt;2e-16 ***\nradio         0.188530   0.008611  21.893   &lt;2e-16 ***\nnewspaper    -0.001037   0.005871  -0.177     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.686 on 196 degrees of freedom\nMultiple R-squared:  0.8972,    Adjusted R-squared:  0.8956 \nF-statistic: 570.3 on 3 and 196 DF,  p-value: &lt; 2.2e-16\n\n\n\nAll predictors except for newspaper have a significant influence on sales on the 0.1% significance level.\nAround 89.7% of the variance of sales can be explained with this model."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#comparing-model-performance",
    "href": "slides/05_The_Data_Science_Workflow_III.html#comparing-model-performance",
    "title": "Data Science and Data Analytics",
    "section": "Comparing model performance",
    "text": "Comparing model performance\nBut how does its test MSE hold up to the models estimated so far? Let’s investigate this question using our cross-validation routine again:\n\n\n\nmses_big &lt;- numeric(K)\n\nfor(k in 1:K){\n  m &lt;- lm(sales ~ social_media + radio + newspaper,\n          data = subset(advertising, fold != k))\n  preds &lt;- predict(m, newdata = subset(advertising, fold == k))\n  mse &lt;- mean((preds - advertising$sales[advertising$fold == k])^2)\n  mses_big[k] &lt;- mse\n}\n\nresults &lt;- data.frame(model_id = factor(1:5),\n                      model_names = model_names,\n                      test_mse = c(mean(mses_linear),\n                                   colMeans(mses_nonlinear),\n                                   mean(mses_big)))\nknitr::kable(results[,-1], col.names = c(\"Model\", \"Test MSE\"),\n             digits = 2, row.names = TRUE)\n\n\n\n\n\n\nModel\nTest MSE\n\n\n\n\n1\n\\(Y \\sim X_1\\)\n10.73\n\n\n2\n\\(\\log(Y) \\sim X_1\\)\n11.83\n\n\n3\n\\(\\log(Y) \\sim \\log(X_1)\\)\n10.72\n\n\n4\n\\(Y \\sim \\sqrt{X_1}\\)\n10.39\n\n\n5\n\\(Y \\sim X_1 + X_2 + X_3\\)\n3.06\n\n\n\n\n\nUnsurprisingly, the model that uses all three advertising budgets as predictors is much better than any of the models that just uses the social media budget!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#comparing-model-performance-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#comparing-model-performance-1",
    "title": "Data Science and Data Analytics",
    "section": "Comparing model performance",
    "text": "Comparing model performance\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(results, aes(x = model_id, y = test_mse, fill = model_id)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Model ID\", y = \"Test MSE\",\n       title = \"Test MSE of the linear models estimated\",\n       subtitle = \"Test MSE determined via 5-fold cross validation\",\n       caption = \"Source: advertising data\") +\n  guides(fill = \"none\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#categorical-predictors",
    "href": "slides/05_The_Data_Science_Workflow_III.html#categorical-predictors",
    "title": "Data Science and Data Analytics",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nSo far, all the predictors we have looked at have been quantitative variables. In practice, we often have to deal with categorical predictors as well.\nTo see an example, let’s consider the Credit data set from the ISLR package. It contains socio-economic information on 400 credit card customers.\nThe data set contains seven quantitative variables:\n\nlibrary(ISLR)\nhead(Credit[, c(\"Income\", \"Limit\", \"Rating\", \"Cards\", \"Age\", \"Education\", \"Balance\")])\n\n   Income Limit Rating Cards Age Education Balance\n1  14.891  3606    283     2  34        11     333\n2 106.025  6645    483     3  82        15     903\n3 104.593  7075    514     4  71        11     580\n4 148.924  9504    681     3  36        11     964\n5  55.882  4897    357     2  68        16     331\n6  80.180  8047    569     4  77        10    1151\n\n\nThe goal here is to build a predictive model for balance, the average credit card balance of a given customer."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#categorical-predictors-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#categorical-predictors-1",
    "title": "Data Science and Data Analytics",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nHowever, aside from the quantitative variables, the data set also contains four qualitative (categorical) variables:\n\nhead(Credit[, c(\"Gender\", \"Student\", \"Married\", \"Ethnicity\")])\n\n  Gender Student Married Ethnicity\n1   Male      No     Yes Caucasian\n2 Female     Yes     Yes     Asian\n3   Male      No      No     Asian\n4 Female      No      No     Asian\n5   Male      No     Yes Caucasian\n6   Male      No      No Caucasian\n\n\nSo how can we include the information in these qualitative predictors to benefit our linear model for balance?\nThe answer lies in dummy variables, i.e. variables that only take on the values 0 or 1. For a categorical variable with \\(K\\) levels, we create \\(K-1\\) dummy variables. Let’s see an example of how this works."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables",
    "text": "Dummy variables\n\nThe variable Ethnicity takes on \\(K = 3\\) different values:\n\ntable(Credit$Ethnicity)\n\n\nAfrican American            Asian        Caucasian \n              99              102              199 \n\n\nSo, we can encode the information in the Ethnicity variable in \\(K - 1 = 2\\) dummy variables, called \\(d_1\\) and \\(d_2\\). For example, one could be used to indicate Asian ethnicity and the other to indicate Caucasian: \\[d_{1i} = \\begin{cases}\n1 & \\text{if person } i \\text{ is of Asian ethnicity} \\\\\n0 & \\text{otherwise}\n\\end{cases}\\] \\[d_{2i} = \\begin{cases}\n1 & \\text{if person } i \\text{ is of Caucasian ethnicity} \\\\\n0 & \\text{otherwise}\n\\end{cases}\\] Note that a third dummy variable is not necessary."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-1",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables",
    "text": "Dummy variables\n\nWe could manually add these dummy variables to the data set:\n\nCredit$Asian &lt;- ifelse(Credit$Ethnicity == \"Asian\", 1, 0)\nCredit$Caucasian &lt;- ifelse(Credit$Ethnicity == \"Caucasian\", 1, 0)\nhead(Credit[,c(\"Ethnicity\", \"Asian\", \"Caucasian\")], 8)\n\n         Ethnicity Asian Caucasian\n1        Caucasian     0         1\n2            Asian     1         0\n3            Asian     1         0\n4            Asian     1         0\n5        Caucasian     0         1\n6        Caucasian     0         1\n7 African American     0         0\n8            Asian     1         0\n\n\nWith the categorical data now numerically encoded, we can estimate the following model: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 d_{1i} + \\hat{\\beta}_2 d_{2i},\\] where \\(y_i\\) is the \\(i\\)th observation of the balance variable."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-2",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\nNote that since \\(d_{1i}\\) and \\(d_{2i}\\) can only take two possible values (0 or 1), this model can only predict three distinct values: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 d_{1i} + \\hat{\\beta}_2 d_{2i} = \\begin{cases}\n\\hat{\\beta}_0 + \\hat{\\beta}_1 & \\text{if } i \\text{ is Asian} \\\\\n\\hat{\\beta}_0 + \\hat{\\beta}_2 & \\text{if } i \\text{ is Caucasian} \\\\\n\\hat{\\beta}_0 & \\text{if } i \\text{ is African American} \\\\\n\\end{cases}\\]\nTherefore, we can interpret the regression coefficients as follows:\n\n\\(\\hat{\\beta}_0\\) is the average credit card balance of people of African American ethnicity. This category constitutes the so-called reference category.\n\\(\\hat{\\beta}_1\\) is the difference in average credit card balance between people of Asian ethnicity and the reference category (African American).\n\\(\\hat{\\beta}_2\\) is the difference in average credit card balance between people of Caucasian ethnicity and the reference category (African American)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables in R",
    "text": "Dummy variables in R\n\nLet’s estimate this model with R. Fortunately, as long as the categorical variables are properly encoded as factors, we do not have to create the dummy variables ourselves. Let’s check:\n\nclass(Credit$Ethnicity)\n\n[1] \"factor\"\n\nlevels(Credit$Ethnicity)\n\n[1] \"African American\" \"Asian\"            \"Caucasian\"       \n\n\nThis looks good! Note that R will always automatically use the first factor level as the reference category when estimating a linear model with factors:\n\ncredit_model1 &lt;- lm(Balance ~ Ethnicity, data = Credit)\n\nNote that with the proper factor encoding of Ethnicity, we do not need to make use of the dummy variables Asian and Caucasian we manually added to the data set."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables in R",
    "text": "Dummy variables in R\n\nLet’s have a look at the created model object:\n\nsummary(credit_model1)\n\n\nCall:\nlm(formula = Balance ~ Ethnicity, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-531.00 -457.08  -63.25  339.25 1480.50 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          531.00      46.32  11.464   &lt;2e-16 ***\nEthnicityAsian       -18.69      65.02  -0.287    0.774    \nEthnicityCaucasian   -12.50      56.68  -0.221    0.826    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 460.9 on 397 degrees of freedom\nMultiple R-squared:  0.0002188, Adjusted R-squared:  -0.004818 \nF-statistic: 0.04344 on 2 and 397 DF,  p-value: 0.9575\n\n\nThis shows that on average…\n\npeople of African American ethnicity have a balance of \\(531\\).\npeople of Asian ethnicity have a balance of \\(531-18.69=512.31\\).\npeople of Caucasian ethnicity have a balance of \\(531-12.50=518.50\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#dummy-variables-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "Dummy variables in R",
    "text": "Dummy variables in R\n\nHowever, as the regression output shows, these differences in average credit card balances by ethnicity are clearly not significant.\nNote that the group averages inferred from the regression coefficients genuinely correspond to the group averages:\n\nround(tapply(Credit$Balance, Credit$Ethnicity, mean), 2)\n\nAfrican American            Asian        Caucasian \n          531.00           512.31           518.50 \n\n\nBased on this approach of dummy variable encoding, we can include any number of categorical variables into the regression equation.\nAs an example, the next slide displays the regression output from using all available predictors (quantitative and qualitative) to model credit card balance in the usual additive fashion."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#quantitative-and-qualitative-predictors",
    "href": "slides/05_The_Data_Science_Workflow_III.html#quantitative-and-qualitative-predictors",
    "title": "Data Science and Data Analytics",
    "section": "Quantitative and qualitative predictors",
    "text": "Quantitative and qualitative predictors\n\ncredit_model2 &lt;- lm(Balance ~ Income + Limit + Rating + Cards + Age + Education +\n                              Gender + Student + Married + Ethnicity,\n                    data = Credit)\nsummary(credit_model2)\n\n\nCall:\nlm(formula = Balance ~ Income + Limit + Rating + Cards + Age + \n    Education + Gender + Student + Married + Ethnicity, data = Credit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-161.64  -77.70  -13.49   53.98  318.20 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        -479.20787   35.77394 -13.395  &lt; 2e-16 ***\nIncome               -7.80310    0.23423 -33.314  &lt; 2e-16 ***\nLimit                 0.19091    0.03278   5.824 1.21e-08 ***\nRating                1.13653    0.49089   2.315   0.0211 *  \nCards                17.72448    4.34103   4.083 5.40e-05 ***\nAge                  -0.61391    0.29399  -2.088   0.0374 *  \nEducation            -1.09886    1.59795  -0.688   0.4921    \nGenderFemale        -10.65325    9.91400  -1.075   0.2832    \nStudentYes          425.74736   16.72258  25.459  &lt; 2e-16 ***\nMarriedYes           -8.53390   10.36287  -0.824   0.4107    \nEthnicityAsian       16.80418   14.11906   1.190   0.2347    \nEthnicityCaucasian   10.10703   12.20992   0.828   0.4083    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 98.79 on 388 degrees of freedom\nMultiple R-squared:  0.9551,    Adjusted R-squared:  0.9538 \nF-statistic: 750.3 on 11 and 388 DF,  p-value: &lt; 2.2e-16\n\n\nWith over 95% variance explained, this is a very good model for credit card balance. However, is it reasonable to leave all of the predictors in the model? Or are we better off removing some of them?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#variable-selection",
    "href": "slides/05_The_Data_Science_Workflow_III.html#variable-selection",
    "title": "Data Science and Data Analytics",
    "section": "Variable selection",
    "text": "Variable selection\n\nThe task of determining which predictors are associated with the response and are thus to be included in the model, is referred to as variable selection.\nWe can decide which variables to select on the basis of statistics like\n\ncross-validated MSE (the lower the better).\nadjusted \\(R^2\\) (the higher the better).\nAkaike information criterion (AIC, the lower the better)\n\nMost direct approach: best subset regression: compute OLS fit for every subset of variables and choose the best according to some metric:\n\nUnfortunately, there are a total of \\(2^p\\) models to consider.\nFor \\(p = 2\\), that’s 4 models, but for \\(p = 30\\) that is 1,073,741,824 models to estimate! This is not practical.\n\nInstead, we need a more efficient approach to select a smaller set of models."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#forward-selection",
    "href": "slides/05_The_Data_Science_Workflow_III.html#forward-selection",
    "title": "Data Science and Data Analytics",
    "section": "Forward selection",
    "text": "Forward selection\n\nOne approach to this task is forward selection (using AIC):\n\nBegin with a null model, i.e. a model that contains no predictors.\nFit \\(p\\) simple linear regressions (one for each predictor individually) and add to the null model the variable that results in the lowest AIC.\nAdd to that model the variable that results in the lowest AIC for the new two-variable model.\nContinue until some stopping rule is satisfied, e.g. no model with a smaller AIC can be fitted.\n\nIn R, forward selection can be achieved with the help of the step function. It requires the null model (as a lower bound) and the full model with all variables (as an upper bound) as input."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#forward-selection-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#forward-selection-1",
    "title": "Data Science and Data Analytics",
    "section": "Forward selection",
    "text": "Forward selection\n\nm0 &lt;- lm(Balance ~ 1, data = Credit)\ncredit_model_fw &lt;- step(m0, scope = Balance ~ Income + Limit + Rating + Cards + Age +\n                                    Education + Gender + Student + Married + Ethnicity,\n                        direction = \"forward\")\n\nStart:  AIC=4905.56\nBalance ~ 1\n\n            Df Sum of Sq      RSS    AIC\n+ Rating     1  62904790 21435122 4359.6\n+ Limit      1  62624255 21715657 4364.8\n+ Income     1  18131167 66208745 4810.7\n+ Student    1   5658372 78681540 4879.8\n+ Cards      1    630416 83709496 4904.6\n&lt;none&gt;                   84339912 4905.6\n+ Gender     1     38892 84301020 4907.4\n+ Education  1      5481 84334431 4907.5\n+ Married    1      2715 84337197 4907.5\n+ Age        1       284 84339628 4907.6\n+ Ethnicity  2     18454 84321458 4909.5\n\nStep:  AIC=4359.63\nBalance ~ Rating\n\n            Df Sum of Sq      RSS    AIC\n+ Income     1  10902581 10532541 4077.4\n+ Student    1   5735163 15699959 4237.1\n+ Age        1    649110 20786012 4349.3\n+ Cards      1    138580 21296542 4359.0\n+ Married    1    118209 21316913 4359.4\n&lt;none&gt;                   21435122 4359.6\n+ Education  1     27243 21407879 4361.1\n+ Gender     1     16065 21419057 4361.3\n+ Limit      1      7960 21427162 4361.5\n+ Ethnicity  2     51100 21384022 4362.7\n\nStep:  AIC=4077.41\nBalance ~ Rating + Income\n\n            Df Sum of Sq      RSS    AIC\n+ Student    1   6305322  4227219 3714.2\n+ Married    1     95068 10437473 4075.8\n+ Limit      1     94545 10437996 4075.8\n+ Age        1     90286 10442255 4076.0\n&lt;none&gt;                   10532541 4077.4\n+ Education  1     20819 10511722 4078.6\n+ Ethnicity  2     67040 10465501 4078.9\n+ Cards      1      2094 10530447 4079.3\n+ Gender     1       948 10531593 4079.4\n\nStep:  AIC=3714.24\nBalance ~ Rating + Income + Student\n\n            Df Sum of Sq     RSS    AIC\n+ Limit      1    194718 4032502 3697.4\n+ Age        1     44620 4182600 3712.0\n&lt;none&gt;                   4227219 3714.2\n+ Married    1     13083 4214137 3715.0\n+ Gender     1     12168 4215051 3715.1\n+ Cards      1     10608 4216611 3715.2\n+ Education  1      1400 4225820 3716.1\n+ Ethnicity  2     22322 4204897 3716.1\n\nStep:  AIC=3697.37\nBalance ~ Rating + Income + Student + Limit\n\n            Df Sum of Sq     RSS    AIC\n+ Cards      1    166410 3866091 3682.5\n+ Age        1     37952 3994549 3695.6\n&lt;none&gt;                   4032502 3697.4\n+ Gender     1     13345 4019157 3698.0\n+ Married    1      6660 4025842 3698.7\n+ Education  1      5795 4026707 3698.8\n+ Ethnicity  2     17704 4014797 3699.6\n\nStep:  AIC=3682.52\nBalance ~ Rating + Income + Student + Limit + Cards\n\n            Df Sum of Sq     RSS    AIC\n+ Age        1     44472 3821620 3679.9\n&lt;none&gt;                   3866091 3682.5\n+ Gender     1     11350 3854741 3683.3\n+ Education  1      5672 3860419 3683.9\n+ Married    1      3121 3862970 3684.2\n+ Ethnicity  2     14756 3851335 3685.0\n\nStep:  AIC=3679.89\nBalance ~ Rating + Income + Student + Limit + Cards + Age\n\n            Df Sum of Sq     RSS    AIC\n&lt;none&gt;                   3821620 3679.9\n+ Gender     1   10860.9 3810759 3680.7\n+ Married    1    5450.6 3816169 3681.3\n+ Education  1    5241.7 3816378 3681.3\n+ Ethnicity  2   11517.3 3810102 3682.7\n\n\nThe final model found with forward selection contains predictors Rating, Income, Student, Limit, Cards and Age. It has an AIC of 3679.9. The object credit_model_fw can be used like any other lm object."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#backward-selection",
    "href": "slides/05_The_Data_Science_Workflow_III.html#backward-selection",
    "title": "Data Science and Data Analytics",
    "section": "Backward selection",
    "text": "Backward selection\n\nAnother approach to the task of variable selection is backward selection:\n\nBegin with a full model, i.e. a model that contains all available predictors.\nFit \\(p\\) individual linear regressions, removing one variable from the full model at a time. Choose the model that has the lowest AIC.\nRepeat with the resulting model and continue until some stopping rule is satisfied, e.g. no model with a smaller AIC can be fitted.\n\nIn R, backward selection can also be achieved with the help of the step function. It only requires the full model (as an upper bound) and the direction backward as input."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#backward-selection-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#backward-selection-1",
    "title": "Data Science and Data Analytics",
    "section": "Backward selection",
    "text": "Backward selection\n\ncredit_model_bw &lt;- step(credit_model2, direction = \"backward\")\n\nStart:  AIC=3686.22\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student + Married + Ethnicity\n\n            Df Sum of Sq      RSS    AIC\n- Ethnicity  2     14084  3800814 3683.7\n- Education  1      4615  3791345 3684.7\n- Married    1      6619  3793349 3684.9\n- Gender     1     11269  3798000 3685.4\n&lt;none&gt;                    3786730 3686.2\n- Age        1     42558  3829288 3688.7\n- Rating     1     52314  3839044 3689.7\n- Cards      1    162702  3949432 3701.0\n- Limit      1    331050  4117780 3717.7\n- Student    1   6326012 10112742 4077.1\n- Income     1  10831162 14617892 4224.5\n\nStep:  AIC=3683.7\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student + Married\n\n            Df Sum of Sq      RSS    AIC\n- Married    1      4545  3805359 3682.2\n- Education  1      4757  3805572 3682.2\n- Gender     1     10760  3811574 3682.8\n&lt;none&gt;                    3800814 3683.7\n- Age        1     45650  3846464 3686.5\n- Rating     1     49473  3850287 3686.9\n- Cards      1    166806  3967621 3698.9\n- Limit      1    340250  4141064 3716.0\n- Student    1   6372573 10173387 4075.5\n- Income     1  10838891 14639705 4221.1\n\nStep:  AIC=3682.18\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student\n\n            Df Sum of Sq      RSS    AIC\n- Education  1      5399  3810759 3680.7\n- Gender     1     11019  3816378 3681.3\n&lt;none&gt;                    3805359 3682.2\n- Age        1     43545  3848904 3684.7\n- Rating     1     46929  3852289 3685.1\n- Cards      1    170729  3976088 3697.7\n- Limit      1    352112  4157472 3715.6\n- Student    1   6461978 10267338 4077.2\n- Income     1  10860901 14666261 4219.8\n\nStep:  AIC=3680.75\nBalance ~ Income + Limit + Rating + Cards + Age + Gender + Student\n\n          Df Sum of Sq      RSS    AIC\n- Gender   1     10861  3821620 3679.9\n&lt;none&gt;                  3810759 3680.7\n- Age      1     43983  3854741 3683.3\n- Rating   1     49520  3860279 3683.9\n- Cards    1    170898  3981656 3696.3\n- Limit    1    347654  4158413 3713.7\n- Student  1   6472072 10282831 4075.8\n- Income   1  10855780 14666539 4217.8\n\nStep:  AIC=3679.89\nBalance ~ Income + Limit + Rating + Cards + Age + Student\n\n          Df Sum of Sq      RSS    AIC\n&lt;none&gt;                  3821620 3679.9\n- Age      1     44472  3866091 3682.5\n- Rating   1     49263  3870883 3683.0\n- Cards    1    172930  3994549 3695.6\n- Limit    1    347898  4169518 3712.7\n- Student  1   6462599 10284218 4073.9\n- Income   1  10845018 14666638 4215.8\n\n\nThe final model found with backward selection also contains predictors Rating, Income, Student, Limit, Cards and Age. With an AIC of 3679.9, it is the same model that was found with forward selection. The object credit_model_bw can be used like any other lm object."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) regression",
    "text": "K-nearest neighbours (KNN) regression\n\nParametric methods like linear regression make strong assumptions about the form of \\(f(X)\\) (namely linearity in all predictors).\nBy contrast, non-parametric methods do not assume a parametric form for \\(f(X)\\), thereby providing a more flexible alternative for regression tasks.\nOne of the simplest non-parametric regression methods is K-nearest neighbours (KNN) regression, which works as follows:\n\nGiven a value for \\(K\\) and a prediction point \\(x_0\\), we first identify the \\(K\\) training observations that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nWe then estimate \\(f(x_0)\\) using the average of all the training responses in \\(\\mathcal{N}_0\\), i.e. formally we have \\[\\hat{f}(x_0) = \\frac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-2",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) regression",
    "text": "K-nearest neighbours (KNN) regression\n\nPlots of \\(\\hat{f}(X)\\) using KNN regression on a two-dimensional data set with 64 observations (orange dots). Left: \\(K = 1\\) results in a rough step function fit. Right: \\(K = 9\\) produces a much smoother fit."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-regression-3",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) regression",
    "text": "K-nearest neighbours (KNN) regression\n\nThe number of neighbours \\(K\\) is what we call a hyperparameter. It changes the exact behaviour of the algorithm: a small value for \\(K\\) provides lots of flexibility (low bias, high variance), while a large value for \\(K\\) provide a smoother, less variable fit (high bias, low variance).\nThe optimal value for \\(K\\) will be different in every data set and can be determined via cross-validation, for instance.\nSo, how will the performance of KNN and linear regression compare? That depends on the true form of \\(f\\):\n\nIf \\(f\\) is close to linear, then linear regression will generally be better.\nIf \\(f\\) is highly non-linear, then KNN regression will generally be better.\nAs we do not know the true \\(f\\), we have to judge which one is better based on estimates of test MSE, individually for every data set under investigation."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-vs.-linear-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-vs.-linear-regression",
    "title": "Data Science and Data Analytics",
    "section": "KNN vs. linear regression",
    "text": "KNN vs. linear regression\n\nPlots of \\(\\hat{f}(X)\\) using KNN regression on a one-dimensional data set with 50 observations. Clearly, when the true relationship (black line) is linear, KNN regression is sub-optimal, regardless of whether \\(K = 1\\) (left) or \\(K = 9\\) (right)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-vs.-linear-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-vs.-linear-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "KNN vs. linear regression",
    "text": "KNN vs. linear regression\n\nFor a non-linear relationship between \\(X\\) and \\(Y\\), however, KNN outperforms linear regression. On the left, we see KNN fits for \\(K = 1\\) (blue) and \\(K = 9\\) (red) compared to the true relationship (black). On the right, we see that the test set MSE for linear regression (horizontal black line) is significantly higher than the test set MSE for KNN with different values of \\(K\\) (displayed as \\(1/K\\))."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#caveats-of-knn-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#caveats-of-knn-regression",
    "title": "Data Science and Data Analytics",
    "section": "Caveats of KNN regression",
    "text": "Caveats of KNN regression\n\nFirst caveat of KNN regression: The curse of dimensionality:\n\nAs we saw, with small \\(p\\), KNN typically performs slightly worse than linear regression when \\(f\\) is linear, but much better if \\(f\\) is non-linear.\nHowever, when \\(p\\) becomes larger (with \\(n\\) constant), KNN performance typically deteriorates.\nThis is because, in high dimensions (e.g. \\(p = 50\\)), the \\(K\\) observations that are nearest to \\(x_0\\) may still be very far away from \\(x_0\\), leading to a very poor prediction of \\(f(x_0)\\).\n\nAs a general rule therefore: parametric methods will tend to outperform non-parametric approaches when the number of observations per predictor (i.e. \\(n/p\\)) is small."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#caveats-of-knn-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#caveats-of-knn-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "Caveats of KNN regression",
    "text": "Caveats of KNN regression\n\nSecond caveat of KNN regression: feature scaling:\n\nSince KNN predicts a test observation by identifying the training observations that are nearest to it, the scale of the variables matter.\nIf all features are measured on the same scale (like in the advertising example), there is nothing to worry about.\nHowever, consider the features salary (in USD) and age (in years): for the KNN, a difference of 1000 USD in salary is enormous compared to a difference of 50 years in age.\nConsequently, all the predictions will be driven by salary rather than age.\n\nAs a general rule therefore: when feature scales differ, perform variable standardization before doing KNN regression."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r",
    "title": "Data Science and Data Analytics",
    "section": "KNN regression in R",
    "text": "KNN regression in R\nIn R, KNN regression can be achieved with the help of the FNN package. It provides a function called knn.reg, which requires a data frame of features, a vector of responses and a value for \\(K\\) to estimate a KNN model. Let’s apply that to our advertising example, say for \\(K = 3\\):\n\nlibrary(FNN)\n\nknn_ad &lt;- knn.reg(train = advertising[, 1:3], y = advertising$sales, k = 3)\nknn_ad\n\nPRESS =  373.3867 \nR2-Predict =  0.9310732 \n\n\n\nAs there are no coefficients to interpret or inferential statistics to analyse, the pure model fit is not that interesting.\nInstead, the value of KNN regression lies mostly in prediction.\nLet’s therefore see how KNN regression with different values of \\(K\\) performs in cross-validation compared to the best-performing linear regression model from earlier."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "KNN regression in R",
    "text": "KNN regression in R\nWith the knn.reg function, we can do fitting and predicting in one function call. This requires passing the prediction features as the test argument to the function. We use this functionality in our existing cross-validation scheme:\n\nn_neighbours &lt;- 1:10 # number of neighbours considered\nmses_knn &lt;- matrix(NA, K, length(n_neighbours))\ncolnames(mses_knn) &lt;- sprintf(\"K = %d\", n_neighbours)\n\nfor(k in 1:K){\n  preds &lt;- lapply(n_neighbours, function(x) knn.reg(train = subset(advertising, fold != k, 1:3),\n                                                    test = subset(advertising, fold == k, 1:3),\n                                                    y = advertising$sales[advertising$fold != k],\n                                                    k = x)$pred)\n  mses_knn[k, ] &lt;- colMeans((do.call(cbind, preds) - advertising$sales[advertising$fold == k])^2)\n}\nround(colMeans(mses_knn), 2)\n\n K = 1  K = 2  K = 3  K = 4  K = 5  K = 6  K = 7  K = 8  K = 9 K = 10 \n  2.33   1.94   2.27   2.37   2.49   2.68   2.92   3.24   3.61   3.73 \n\n\nThe best test MSE is achieved by setting \\(K = 2\\). The resulting model has a test MSE of 1.94, which is also considerably better than the best-performing linear model, which had test MSE 3.06."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-regression-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "KNN regression in R",
    "text": "KNN regression in R"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#an-overview-of-classification",
    "href": "slides/05_The_Data_Science_Workflow_III.html#an-overview-of-classification",
    "title": "Data Science and Data Analytics",
    "section": "An overview of classification",
    "text": "An overview of classification\n\nSo far, we have dealt with situations in which the response variable \\(Y\\) was quantitative. However, often it is qualitative / categorical.\nPredicting a qualitative response is also referred to as classifying that observation, as it involves assigning the observation to a class / category.\nA classifier is an algorithm that maps input data (i.e. the predictors) to a category. Some of them first predict a probability for each of the categories. In this sense, they behave like regression methods.\nIn this first part, we will discuss the following two simple classifiers:\n\nLogistic regression\nKNN classification"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers",
    "href": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers",
    "title": "Data Science and Data Analytics",
    "section": "Evaluation of classifiers",
    "text": "Evaluation of classifiers\n\nIn the introduction, we already saw that a common way of evaluating classifiers is the error rate, i.e. the proportion of misclassified observations.\nHowever, there are many more ways to evaluate classifiers. One typically looks at the joint distribution of observed response and predicted response.\nFor binary classification, we can illustrate this distribution with the help of a so-called confusion matrix:\n\n\n\n\n\n\n\n\n\nobserved/true = 0\nobserved/true = 1\n\n\n\n\npredicted = 0\ntrue negatives (TN)\nfalse negatives (FN)\n\n\npredicted = 1\nfalse positives (FP)\ntrue positives (TP)\n\n\n\nFor an extended version of this table, see here."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-1",
    "title": "Data Science and Data Analytics",
    "section": "Evaluation of classifiers",
    "text": "Evaluation of classifiers"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-2",
    "title": "Data Science and Data Analytics",
    "section": "Evaluation of classifiers",
    "text": "Evaluation of classifiers\n\nFrom this confusion matrix, we can compute several metrics:\n\nAccuracy: percentage of correctly classified observations: \\[accuracy = (TN + TP) / (TN + TP + FN + FP)\\]\nRecall: percentage of correctly classified positives: \\[recall = TP/(FN + TP)\\]\nPrecision: percentage of correctly classified predicted positives: \\[precision = TP/(FP + TP)\\]\nF1 score: harmonic mean of precision and recall \\[F1 = 2 \\cdot recall \\cdot precision/(recall + precision)\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#evaluation-of-classifiers-3",
    "title": "Data Science and Data Analytics",
    "section": "Evaluation of classifiers",
    "text": "Evaluation of classifiers\n\nThe choice of the evaluation measure is application- and data-dependent. One typically evaluates the relative costs of false negatives and false positives.\nConsider the following example:\n\nSay, we have built a classifier that predicts whether someone has cancer based on an MRI image.\nFalse positive: telling someone they have cancer even though they do not.\nFalse negative: telling someone they don’t have cancer, although they do.\nIf the first is considered more problematic, optimize for precision.\nIf the second is considered more problematic, optimize for recall.\nIf both are important, optimize for F1 or accuracy.\n\nHowever, the sole use of accuracy can also be misleading…"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#using-accuracy-for-evaluation",
    "href": "slides/05_The_Data_Science_Workflow_III.html#using-accuracy-for-evaluation",
    "title": "Data Science and Data Analytics",
    "section": "Using accuracy for evaluation",
    "text": "Using accuracy for evaluation\n\nSuppose we have a data set of 100 credit card customers, of which 5 have defaulted on their credit card debt. We want to build a classifier that tells us whether someone will default or not.\nAfter training, the classifier produced the following confusion matrix:\n\n\n\n\n\n\n\n\n\n\nobserved/true = 0\nobserved/true = 1\nTotal\n\n\n\n\npredicted = 0\n94\n4\n98\n\n\npredicted = 1\n1\n1\n2\n\n\nTotal\n95\n5\n100\n\n\n\n\nOur classifier achieves 95% accuracy (5% error rate) on the training data!\nBut recall is only 1/5, only one default was correctly identified.\nPrecision is only 1/2, only one of the two default predictions was correct.\n\nAccuracy can be misleading for unbalanced class distributions!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#click-data-set",
    "href": "slides/05_The_Data_Science_Workflow_III.html#click-data-set",
    "title": "Data Science and Data Analytics",
    "section": "Click data set",
    "text": "Click data set\n\nAdvertising companies want to target advertisements to users based on a user’s likelihood to click.\nFor this purpose, we are using a data set from Kaggle, which contains information about the behaviour of 1000 users.\nA total of 9 variables are available for each user:\n\nThe binary variable clicked_on_ad indicates whether the user clicked the ad or not. This will be our response variable.\nPersonal information such as age, sex, time spent on the website.\nOther information, such as average annual income of the area where the user resides, city and country.\n\n\nclick &lt;- read.csv(\"data/click.csv\")"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#simple-logistic-regression-model",
    "href": "slides/05_The_Data_Science_Workflow_III.html#simple-logistic-regression-model",
    "title": "Data Science and Data Analytics",
    "section": "Simple logistic regression model",
    "text": "Simple logistic regression model\nLet’s visualize the relationship between area_income and the response clicked_on_ad in a scatter plot (with a slight jitter):\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(click, aes(x = area_income, y = clicked_on_ad)) +\n  geom_jitter(height = 0.02) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#simple-logistic-regression-model-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#simple-logistic-regression-model-1",
    "title": "Data Science and Data Analytics",
    "section": "Simple logistic regression model",
    "text": "Simple logistic regression model\n\nNow, of course, we could simply lay a linear regression line into this scatter plot. This would amount to estimating a regression for the probability of a click (\\(Y = 1\\)) using area income as the explanatory variable \\(X\\): \\[p_{\\text{click}} = P(Y=1|X) = \\beta_0 + \\beta_1X + \\epsilon\\]\nHowever, this would not be appropriate as the possible values lie between \\(-\\infty\\) and \\(\\infty\\), but a probability should be between 0 and 1.\nSo, we need to transform the right-hand side in order to ensure that it lies between 0 and 1. In logistic regression, this transformation is given by the so-called logistic function: \\[p_{\\text{click}} = P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-vs.-logistic-regression-model",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-vs.-logistic-regression-model",
    "title": "Data Science and Data Analytics",
    "section": "Linear vs. logistic regression model",
    "text": "Linear vs. logistic regression model"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#interpretation-of-simple-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#interpretation-of-simple-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Interpretation of simple logistic regression",
    "text": "Interpretation of simple logistic regression\n\nNote that with some re-arrangement of the terms, the simple logistic regression model can also be written as \\[\\log\\left( \\frac{p_{\\text{click}}}{1 - p_{\\text{click}}} \\right) = \\beta_0 + \\beta_1X\\]\nThe left-hand side of this equation are the so-called log odds. In logistic regression, we model the log odds of the success probability as a linear function of the predictors. This implies:\n\nIncreasing \\(X\\) by one unit changes the log odds by \\(\\beta_1\\).\nEquivalently, it multiplies the odds by \\(e^{\\beta_1}\\).\nHowever, because the relationship between \\(p_{\\text{click}}\\) and \\(X\\) is not a straight line, \\(\\beta_1\\) does not correspond to the change in \\(p_{\\text{click}}\\) associated with a one-unit increase in \\(X\\). This is a difference to linear regression."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\nGiven multiple covariates \\(X_1, X_2, \\ldots, X_p\\), we can easily generalize the simple logistic regression model to the following: \\[p = P(Y=1|X_1, \\ldots, X_p) = \\frac{e^{\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p}}\\]\nAgain, we model log odds as a linear function of the predictors: \\[\\log\\left( \\frac{p}{1 - p} \\right) = \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p\\]\n\n\\(\\beta_0\\) gives the baseline log odds for \\(Y=1\\) when \\(X_1 = X_2 = \\ldots = X_p = 0\\).\nA one unit increase in covariate \\(X_j\\) changes the log odds for \\(Y = 1\\) by \\(\\beta_j\\), keeping all other covariates constant (ceteris paribus)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#multiple-logistic-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#multiple-logistic-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\nIn logistic regression, the linear combination of the covariates \\(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p\\) is called the linear predictor.\nIt has the following interpretation:\n\nIf \\(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p = 0\\), then \\(P(Y=1|X_1, \\ldots, X_p) = 0.5\\), i.e. \\(Y=1\\) and \\(Y=0\\) are equally likely.\nIf \\(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p &gt; 0\\), then \\(P(Y=1|X_1, \\ldots, X_p) &gt; 0.5\\), i.e. \\(Y=1\\) is more likely.\nIf \\(\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p &lt; 0\\), then \\(P(Y=1|X_1, \\ldots, X_p) &lt; 0.5\\), i.e. \\(Y=0\\) is more likely.\n\nIn practice, the coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are unknown and must be estimated on the basis of available training data.\nEstimation no longer happens with OLS, but with maximum likelihood (ML), a very common procedure in statistics."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#statistical-inference-in-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#statistical-inference-in-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Statistical inference in logistic regression",
    "text": "Statistical inference in logistic regression\n\nBroadly speaking, the idea of ML estimation is to choose parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) such that the probability of observing the data we observed is maximized. The details are beyond the scope of this course.\nFortunately, R has efficient algorithms for ML estimation of logistic regression models, which we will soon see how to use.\nBesides estimating the regression coefficients, ML estimation allows us to perform inferential statistics akin to the linear regression case:\n\nHypothesis tests for \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\).\nConfidence intervals for regression coefficients.\nGoodness-of-fit measures similar to the \\(R^2\\).\n\nHowever, when using logistic regression as a classification algorithm in machine learning, we are mostly interested in prediction, not inference."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#making-predictions",
    "href": "slides/05_The_Data_Science_Workflow_III.html#making-predictions",
    "title": "Data Science and Data Analytics",
    "section": "Making predictions",
    "text": "Making predictions\n\nOnce the coefficients have been estimated, we can compute the probability for \\(Y=1\\) for every combination of values of the predictors.\nWe get \\(\\hat{p}(x_1, \\ldots, x_p) = \\hat{P}(Y=1|X_1 = x_1, \\ldots, X_p = x_p)\\) by simply plugging in the coefficient estimates \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\) into the logistic regression formula: \\[\\hat{p}(x_1, \\ldots, x_p) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p}}\\]\nThe prediction of the conditional response is straightforward: \\[\\hat{Y}|(X_1 = x_1, \\ldots, X_p = x_p) = \\begin{cases}\n0 & \\text{ if } \\hat{p}(x_1, \\ldots, x_p) &lt; 0.5 \\\\\n1 & \\text{ if } \\hat{p}(x_1, \\ldots, x_p) \\geq 0.5\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nLet’s finally see an example of how to do all this in R!\nTo estimate a logistic regression in R, we need three things to pass into the function glm (which stands for “generalized linear model”):\n\nA data.frame holding the variables we want included in the model, especially the binary response as a factor or numeric.\nA formula that specifies the model, similar to the lm case.\nA family that specifies the type of generalized linear model. For logistic regression, this is always equal to binomial().\n\nSo, if we wanted to use area_income and age as predictors for the response clicked_on_ad, we would do the following:\n\nlogreg &lt;- glm(clicked_on_ad ~ area_income + age, data = click, family = binomial())"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nWe can then have a look at the result using the summary function:\n\nsummary(logreg)\n\n\nCall:\nglm(formula = clicked_on_ad ~ area_income + age, family = binomial(), \n    data = click)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  9.160e-02  5.399e-01    0.17    0.865    \narea_income -1.033e-04  8.207e-06  -12.59   &lt;2e-16 ***\nage          1.626e-01  1.261e-02   12.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1386.3  on 999  degrees of freedom\nResidual deviance:  880.0  on 997  degrees of freedom\nAIC: 886\n\nNumber of Fisher Scoring iterations: 5\n\n\nWhile the output looks somewhat different to the linear regression case, the all-important coefficients matrix with Estimate, Std. Error, z value and Pr(&gt;|z|) looks virtually identical."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nUsing \\(X_1\\) for area_income (in thousand USD) and \\(X_2\\) for age, the estimated model for the click probability \\(p\\) thus has the following form: \\[\\log\\left( \\frac{p}{1 - p} \\right) = 0.0916 - 0.1033 X_1 + 0.1626 X_2\\]\nInterpretation:\n\nCeteris paribus, increasing the area_income by 1000 USD decreases the log odds for clicking on the ad by 0.1033 on average.\nCeteris paribus, increasing the age by one year increases the log odds for clicking on the ad by 0.1626 on average.\n\nThe regression output also shows that both of these variables are highly significant with \\(p\\)-values very close to zero. Both area_income and age seem to be important for predicting whether or not a user will click on the ad."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-3",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R",
    "text": "Logistic regression in R\n\nIn general, there is a high degree of consistency in how we can operate on glm objects compared to lm objects. For example, many accessor functions work in the same way as before:\n\nhead(residuals(logreg))\n\n         1          2          3          4          5          6 \n-0.9342594 -0.5191394 -0.5391085 -0.8425931 -0.5408458 -0.4287287 \n\nhead(fitted(logreg))\n\n        1         2         3         4         5         6 \n0.3536540 0.1260681 0.1352536 0.2988136 0.1360644 0.0878074 \n\ncoef(logreg)\n\n  (Intercept)   area_income           age \n 0.0915995255 -0.0001032967  0.1626462670 \n\nconfint(logreg)\n\n                    2.5 %        97.5 %\n(Intercept) -0.9649226490  1.154141e+00\narea_income -0.0001198858 -8.768127e-05\nage          0.1386819426  1.881543e-01\n\n\nDue to the different scales involved (probabilities vs. log odds), some of these functions come with additional options, as we shall now see for prediction."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-prediction",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-prediction",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R – Prediction",
    "text": "Logistic regression in R – Prediction\n\nUsing our model, what do we estimate the probability that a 29-year old living in an area with an average annual income of 50,000 USD clicks on the ad to be?\nLet’s simply plug in to our equation for the log odds: \\[\\log\\left( \\frac{p}{1 - p} \\right) = 0.0916 - 0.1033 \\cdot 50 + 0.1626 \\cdot 29 = -0.358\\]\nClearly, that is a log odds ratio and not a probability. To turn this into a probability, we need to plug into the other formula for logistic regression: \\[p = \\frac{e^{0.0916 - 0.1033 \\cdot 50 + 0.1626 \\cdot 29}}{1 + e^{0.0916 - 0.1033 \\cdot 50 + 0.1626 \\cdot 29}} = \\frac{e^{-0.358}}{1 + e^{-0.358}} \\approx 0.41\\]\nWe predict that this person will click on the ad with a probability of 41%!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-prediction-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-prediction-1",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R – Prediction",
    "text": "Logistic regression in R – Prediction\n\nIgnoring the rounding differences, both of these types of predictions are available through the predict function in R:\n\nWe get the linear predictor, i.e. the log odds for the click, by setting type to link (which is the default):\n\n\nnew_click &lt;- data.frame(area_income = 50000, age = 29)\npredict(logreg, newdata = new_click, type = \"link\")\n\n         1 \n-0.3564913 \n\n\n\nIn the machine learning context, we are typically only interested in the probability. This we get by setting type to response:\n\n\npredict(logreg, newdata = new_click, type = \"response\")\n\n        1 \n0.4118092 \n\n\nWith the ability to predict new observations secured, let’s evaluate the performance of our model as a classifier using 5-fold cross validation!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-evaluation",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-evaluation",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R – Evaluation",
    "text": "Logistic regression in R – Evaluation\n\nAgain, we randomly assign each observation to one of 5 folds:\n\nn &lt;- nrow(click)\nK &lt;- 5\nclick$fold &lt;- sample(rep(1:K, length.out = n))\n\nThis time, we save the class predictions on each hold-out fold together into one variable called class_preds_lr. This will then allow us to evaluate the out-of-sample performance of the classifier:\n\nclick$class_preds_lr &lt;- NA\n\nfor(k in 1:K){\n  m &lt;- glm(clicked_on_ad ~ area_income + age, data = subset(click, fold != k),\n           family = binomial())\n  prob_preds &lt;- predict(m, newdata = subset(click, fold == k), type = \"response\")\n  click$class_preds_lr[click$fold == k] &lt;- ifelse(prob_preds &gt;= 0.5, 1, 0)\n}\n\n\nWe classify an observation as a click if its probability is at least 0.5.\nWe classify an observation as no click if its probability is smaller than 0.5."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-evaluation-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#logistic-regression-in-r-evaluation-1",
    "title": "Data Science and Data Analytics",
    "section": "Logistic regression in R – Evaluation",
    "text": "Logistic regression in R – Evaluation\n\nNow, we can compute the out-of-sample confusion matrix of our classifier:\n\ntable(click$class_preds, click$clicked_on_ad,\n      dnn = c(\"click_prediction\", \"click_truth\"))\n\n                click_truth\nclick_prediction   0   1\n               0 431 123\n               1  69 377\n\n\nFrom this, we can now calculate the aforementioned performance metrics:\n\nAccuracy: \\((431 + 377)/1000 = 0.808\\). Our logistic regression model correctly predicts the click decision in 80.8% of the observations.\nRecall: \\(377/(123 + 377) = 0.754\\). Our logistic regression model correctly identifies 75.4% of all clicks on an ad.\nPrecision: \\(377/(69 + 377) = 0.845\\). When our logistic regression model predicts a click, it is correct 84.5% of the time.\nF1 score: \\(2 \\cdot 0.754 \\cdot 0.845/(0.754 + 0.845) = 0.797\\)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#further-topics-on-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#further-topics-on-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Further topics on logistic regression",
    "text": "Further topics on logistic regression\n\nAs we saw, many concepts from linear regression generalize to logistic regression. Further examples of this include:\n\nCategorical predictors: we can use dummy variables to encode categorical variables as predictors also in logistic regression.\nVariable selection: the step function for forward and backward selection works equally on glm objects.\nLinearity: logistic regression is actually linear in the sense that it models a linear decision boundary, i.e. the curve separating observations of different classes is a straight line. It satisfies the following equation (see slide 107): \\[\\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p = 0\\]\n\nIn our example with two predictors, we can nicely visualize the estimated decision boundary."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#linear-decision-boundary-in-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#linear-decision-boundary-in-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "Linear decision boundary in logistic regression",
    "text": "Linear decision boundary in logistic regression\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(click, aes(x = area_income, y = age, color = factor(clicked_on_ad))) +\n  geom_point() +\n  geom_abline(slope = -coef(logreg)[2]/coef(logreg)[3],\n              intercept = -coef(logreg)[1]/coef(logreg)[3]) +\n  labs(title = \"Linear decision boundary of logistic regression\",\n       subtitle = \"Illustration of linear decision boundary estimated via logistic regression on click data\",\n       color = \"Clicked\") +\n  theme_minimal() +\n  scale_color_colorblind()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-classification-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-classification-1",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) classification",
    "text": "K-nearest neighbours (KNN) classification\n\nSimilar to the regression context, a non-parametric alternative for performing classification is given by K-nearest neighbours (KNN) classification.\nUnlike logistic regression, KNN classification can also be used for multi-class classification, i.e. for when there are more than two classes.\nIn principle, the procedure is straightforward:\n\nGiven a value for \\(K\\) and a prediction point \\(x_0\\), we again start by identifying the \\(K\\) training observations that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nWe then estimate the conditional probability for class \\(j\\) as the fraction of points in \\(\\mathcal{N}_0\\) whose response values equal \\(j\\): \\[\\hat{P}(Y=j|X=x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_0} I(y_i = j)\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-classification-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-nearest-neighbours-knn-classification-2",
    "title": "Data Science and Data Analytics",
    "section": "K-nearest neighbours (KNN) classification",
    "text": "K-nearest neighbours (KNN) classification\nSuppose we have two classes (\\(+\\) and \\(-\\)) and two features \\(X_1\\) and \\(X_2\\). The figure below illustrates how conditional probability would be estimated for a point \\(x\\) and \\(K=1,2,3\\) in a toy example data set:\n\n\\[K = 1, \\hat{P}(+|x) = 0/1 \\quad K = 2, \\hat{P}(+|x) = 1/2 \\quad K = 3, \\hat{P}(+|x) = 2/3\\]"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-in-r",
    "title": "Data Science and Data Analytics",
    "section": "KNN classification in R",
    "text": "KNN classification in R\n\nIn R, KNN classification can also be achieved with the help of the FNN package:\n\nIt provides a function called knn, whose arguments are very similar to the knn.reg function we saw earlier.\nWith this function, we have to perform fitting and prediction in one function call. So let’s run KNN classification for different values of \\(K\\) directly in our existing cross-validation scheme.\nFirst though, we need to standardize our variables, since area_income is measured in USD and age is measured in years. To standardize a variable, we deduct the mean and divide by the standard deviation:\n\n\nclick$ai_scaled &lt;- (click$area_income - mean(click$area_income))/sd(click$area_income)\nclick$age_scaled &lt;- (click$age - mean(click$age))/sd(click$age)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "KNN classification in R",
    "text": "KNN classification in R\n\nNow, we can actually run the KNN classification for \\(K=1, \\ldots, 10\\) and predict each fold in the 5-fold cross validation scheme from earlier:\n\nn_neighbours &lt;- 1:10\n\nfor(nn in n_neighbours){\n  knn_colname &lt;- sprintf(\"class_preds_KNN%d\", nn)\n  click[[knn_colname]] &lt;- factor(NA, levels = c(0, 1))\n  for(k in 1:K){\n    preds &lt;- knn(train = click[click$fold != k, c(\"ai_scaled\", \"age_scaled\")],\n                 test = click[click$fold == k, c(\"ai_scaled\", \"age_scaled\")],\n                 cl = click$clicked_on_ad[click$fold != k],\n                 k = nn)\n    click[click$fold == k, knn_colname] &lt;- preds\n  }\n}\n\nLet’s compare the performance (in terms of classification accuracy on the test set) of these KNN models to the logistic regression model from earlier!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-vs.-logistic-regression",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-vs.-logistic-regression",
    "title": "Data Science and Data Analytics",
    "section": "KNN classification vs. logistic regression",
    "text": "KNN classification vs. logistic regression\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nres_knn &lt;- colMeans(as.matrix(click[, grep(\"_KNN\", names(click), fixed = TRUE)]) == click$clicked_on_ad)\nres_lr &lt;- mean(click$class_preds_lr == click$clicked_on_ad)\nres &lt;- data.frame(model = c(sub(\"class_preds_\", \"\", names(res_knn)), \"LR\"),\n                  accuracy = c(res_knn, res_lr))\nres$model &lt;- factor(res$model, levels = res$model)\nrownames(res) &lt;- NULL\n\nggplot(res, aes(x = model, y = accuracy)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue4\") +\n  labs(x = \"Model\", y = \"Test accuracy\",\n       title = \"Test accuracy of the KNN models and logistic regression\",\n       subtitle = \"Test accuracy via 5-fold cross validation\",\n       caption = \"Source: click data\") +\n  guides(fill = \"none\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-vs.-logistic-regression-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#knn-classification-vs.-logistic-regression-1",
    "title": "Data Science and Data Analytics",
    "section": "KNN classification vs. logistic regression",
    "text": "KNN classification vs. logistic regression\n\nIt appears that the logistic regression model (narrowly) outperforms all KNN models in terms of accuracy.\nTo get an even more detailed picture of the relative performances, one would look at precision, recall and F1 score of all presented classification approaches.\nWith its non-parametric flexibility, KNN classification can in principle model any shape of decision boundary. However, in the specific case of our click data set, it seems that the true decision boundary is close to linear, so the additional flexibility does not provide any benefit.\nTo illustrate this, consider the estimated decision boundaries for some of our KNN models on the next slide."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#decision-boundaries-in-knn-classification",
    "href": "slides/05_The_Data_Science_Workflow_III.html#decision-boundaries-in-knn-classification",
    "title": "Data Science and Data Analytics",
    "section": "Decision boundaries in KNN classification",
    "text": "Decision boundaries in KNN classification"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#tree-based-methods-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#tree-based-methods-1",
    "title": "Data Science and Data Analytics",
    "section": "Tree-based methods",
    "text": "Tree-based methods\n\nSo far, we know about the following regression methods:\n\nSimple and multiple linear regression\nLinear regression with forward and backward variable selection\nKNN regression\n\n… and the following classification methods:\n\nSimple and multiple logistic regression\nLogistic regression with forward and backward variable selection\nKNN classification\n\nNow, we will look into another class of machine learning methods called tree-based methods. In particular, we will discuss so-called decision trees that can be used for both regression and classification."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-regression-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#introduction-to-regression-trees",
    "title": "Data Science and Data Analytics",
    "section": "Introduction to regression trees",
    "text": "Introduction to regression trees\n\nWhen decision trees are used for regression tasks, they are referred to as regression trees.\nIn order to motivate regression trees, we begin with a simple example:\n\nThe Hitters data set in the ISLR package contains statistics of 322 baseball players in the American MLB from the 1986 and 1987 seasons.\nOur goal is to predict a player’s Salary based on Years (number of years in the MLB) and Hits (number of hits he made in the previous year)\nFirst, we remove observations that are missing Salary values and log transform the remaining salaries.\n\n\nHitters &lt;- Hitters[!is.na(Hitters$Salary), ]\nHitters$logSalary &lt;- log(Hitters$Salary)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#baseball-salary-data",
    "href": "slides/05_The_Data_Science_Workflow_III.html#baseball-salary-data",
    "title": "Data Science and Data Analytics",
    "section": "Baseball salary data",
    "text": "Baseball salary data\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(Hitters, aes(x = Years, y = Hits, color = logSalary)) +\n  geom_point() +\n  scale_color_gradientn(colors = rainbow(10)) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#a-simple-regression-tree-for-baseball-data",
    "href": "slides/05_The_Data_Science_Workflow_III.html#a-simple-regression-tree-for-baseball-data",
    "title": "Data Science and Data Analytics",
    "section": "A simple regression tree for baseball data",
    "text": "A simple regression tree for baseball data\n\nUsing this data set, let’s say, we wanted to find players who, based on their Hits and Years in the MLB, had similar salaries:\n\nWe could, for instance, group together all players with less than five years of experience.\nAmong those with more than five years of experience, we could again distinguish between those with less than 118 hits and those with more.\n\nWith these decision rules, we would thus have three groups in total:\n\nPlayers with Years &lt; 5\nPlayers with Years &gt;= 5 and Hits &lt; 118\nPlayers with Years &gt;= 5 and Hits &gt;= 118\n\nLet’s draw these three regions into the plot from earlier!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#a-simple-regression-tree-for-baseball-data-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#a-simple-regression-tree-for-baseball-data-1",
    "title": "Data Science and Data Analytics",
    "section": "A simple regression tree for baseball data",
    "text": "A simple regression tree for baseball data\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(Hitters) +\n  geom_point(aes(x = Years, y = Hits, color = logSalary)) +\n  geom_vline(xintercept = 5, linewidth = 1) +\n  geom_line(data = data.frame(x = c(5, 25), y = c(118, 118)),\n            mapping = aes(x = x, y = y),\n            linewidth = 1) +\n  scale_color_gradientn(colors = rainbow(10)) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-tree-illustration",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-tree-illustration",
    "title": "Data Science and Data Analytics",
    "section": "Regression tree illustration",
    "text": "Regression tree illustration\nWe have already built our first regression tree! The reason for the name is that we can illustrate our decisions on the groups in a tree structure:\n\n\nThe final three regions are known as terminal nodes or leaves of the tree.\nThe points along the tree where the predictor space is split are referred to as internal nodes."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#prediction-with-regression-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#prediction-with-regression-trees",
    "title": "Data Science and Data Analytics",
    "section": "Prediction with regression trees",
    "text": "Prediction with regression trees\n\nBut how do we use a regression tree for prediction?\nVery simple: our prediction for the log salary of a new player is the average log salary of all the players in his region!\nAs the plot shows, our regression tree can thus only predict 3 different values:\n\nA log salary of 5.107 for players with Years &lt; 5\nA log salary of 5.998 for players with Years &gt;= 5 and Hits &lt; 118\nA log salary of 6.740 for players with Years &gt;= 5 and Hits &gt;= 118\n\nInterpretation:\n\nFor a player with less than 5 years of experience, the number of hits that he made in the previous year seems to play little role in his salary.\nBut among more experienced players, the number of hits made in the previous year does (positively) affect salary."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-the-general-idea",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-the-general-idea",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – The general idea",
    "text": "Regression trees – The general idea\n\nWhile this is likely an oversimplification of the true relationship between experience, hits and salary, the regression tree has the advantage of being easy to interpret and to visualize.\nWe have now seen a basic example of a regression tree. Now, we will discuss how to build them. Roughly speaking, there are two steps:\n\nWe divide the predictor space, i.e. the set of possible values for \\(X_1, X_2, \\ldots, X_p\\), into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). In the previous example, we had \\(J = 3\\).\nFor every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\).\n\nAs we saw in the example, once we know the regions, step 2 is easy. But how do we get these regions \\(R_1, R_2, \\ldots, R_J\\)?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-constructing-regions",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-constructing-regions",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Constructing regions",
    "text": "Regression trees – Constructing regions\n\nIn theory, the regions could have any shape. However, in decision trees, we typically divide the predictor space into high-dimensional rectangles or boxes.\nThe goal is to find boxes \\(R_1, \\ldots, R_J\\) that minimize the RSS given by \\[\\sum_{j=1}^J \\sum_{i: x_i \\in R_j} (y_i - \\hat{y}_{R_j})^2,\\] where \\(\\hat{y}_{R_j}\\) is the mean response for the training observations within the \\(j\\)th box. Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes.\nInstead, an algorithm known as recursive binary splitting is used for the purpose of achieving this goal."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Recursive binary splitting",
    "text": "Regression trees – Recursive binary splitting\n\nRecursive binary splitting is based on the idea of, at every step, using a single predictor \\(X_j\\) at a node as a splitting variable.\nThis algorithm roughly works as follows:\n\nFirst, select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X|X_j &lt; s\\}\\) and \\(\\{X|X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS.\nNext, we repeat the process, choosing predictor and cutpoint so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions.\nContinue splitting optimally in this way until some stopping criterion is reached, e.g. until no region contains more than five observations."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-1",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Recursive binary splitting",
    "text": "Regression trees – Recursive binary splitting\nIn the baseball example, we first consider all possible splits on Years:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-2",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Recursive binary splitting",
    "text": "Regression trees – Recursive binary splitting\nThen, we run the same procedure for Hits to check if we get any lower RSS:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-recursive-binary-splitting-3",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees – Recursive binary splitting",
    "text": "Regression trees – Recursive binary splitting\n\nBased on these results, we can determine the first split:\n\nFor the variable Years, the lowest RSS was achieved for \\(s = 5\\). The lowest achievable RSS was 115.06.\nFor the variable Hits, the lowest RSS was achieved for \\(s = 118\\). The lowest achievable RSS was 160.97.\nTherefore, the first split will be based on Years for \\(s = 5\\). We split the predictor space into two rectangles: one where Years &lt; 5 and one where Years &gt;= 5, both times irrespective of the number of Hits.\n\nNext, we again check which split (across either of those two rectangles) gives the lowest RSS. Remember that we only split one rectangle at a time."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split",
    "href": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split",
    "title": "Data Science and Data Analytics",
    "section": "Recursive binary splitting – Second split",
    "text": "Recursive binary splitting – Second split"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-1",
    "title": "Data Science and Data Analytics",
    "section": "Recursive binary splitting – Second split",
    "text": "Recursive binary splitting – Second split"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-2",
    "title": "Data Science and Data Analytics",
    "section": "Recursive binary splitting – Second split",
    "text": "Recursive binary splitting – Second split"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#recursive-binary-splitting-second-split-3",
    "title": "Data Science and Data Analytics",
    "section": "Recursive binary splitting – Second split",
    "text": "Recursive binary splitting – Second split\n\nThe next best split is splitting the rectangle where Years &gt;= 5 into two smaller rectangles: one where Hits &lt; 118 and one where Hits &gt;= 118. This gives \\(RSS = 91.33\\).\nStopping after these two steps gave us the regression tree we saw in the introduction. However, we could continue until some stopping criterion is reached, e.g. no region contains more than five observations.\nRecursive binary splitting is known as a top-down, greedy algorithm:\n\nIt is top-down because it begins at the top of the tree (all observations in a single region) and then successively splits the predictor space, each split indicated as two new branches further down on the tree.\nIt is greedy because at each step, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#prediction-surface-in-regression-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#prediction-surface-in-regression-trees",
    "title": "Data Science and Data Analytics",
    "section": "Prediction surface in regression trees",
    "text": "Prediction surface in regression trees\n\nWith recursive binary splitting, the prediction surface of regression trees looks like a (irregular) stair case. Below is an example with five terminal nodes. Notice the difference to linear regression which would be a plane."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#pruning-a-tree",
    "href": "slides/05_The_Data_Science_Workflow_III.html#pruning-a-tree",
    "title": "Data Science and Data Analytics",
    "section": "Pruning a tree",
    "text": "Pruning a tree\n\nNow, performing recursive binary splitting, until only a few observations are left in each region is likely to overfit the data:\n\nA high number of splits produces a very complex tree, which will have low training error, but (probably) high test error.\nThus, a smaller tree might lead to lower variance and better interpretation at the cost of only a little bias.\n\nOne way to mitigate this is to grow a very large tree \\(T_0\\) and then to prune it back (cut branches of the tree) in order to obtain a subtree.\nThe way we do this in practice is through an approach called cost complexity pruning (aka weakest link pruning)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#cost-complexity-pruning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#cost-complexity-pruning",
    "title": "Data Science and Data Analytics",
    "section": "Cost complexity pruning",
    "text": "Cost complexity pruning\n\nThe idea of cost complexity pruning is to choose a subtree \\(T\\) of \\(T_0\\) that optimally trades off a subtree’s complexity against its fit to the training data.\nFor this purpose, we set a tuning parameter \\(\\alpha &gt; 0\\) and then find \\(T\\) so that \\[\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha \\cdot |T|\\] is minimized. \\(|T|\\) is the size of the tree, i.e. the number of terminal nodes.\nThe tuning parameter \\(\\alpha\\) controls the aforementioned trade-off:\n\nWhen \\(\\alpha = 0\\), then \\(T = T_0\\) because the expression just measures the training error.\nHowever, when \\(\\alpha &gt; 0\\), there is a price to pay for having a tree with many terminal nodes, so the expression will “prefer” a smaller subtree."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#cost-complexity-pruning-choosing-alpha",
    "href": "slides/05_The_Data_Science_Workflow_III.html#cost-complexity-pruning-choosing-alpha",
    "title": "Data Science and Data Analytics",
    "section": "Cost complexity pruning – Choosing \\(\\alpha\\)",
    "text": "Cost complexity pruning – Choosing \\(\\alpha\\)\n\nIn general, the higher the value of \\(\\alpha\\), the more the initial fully grown tree \\(T_0\\) will be pruned. Thus, as we increase \\(\\alpha\\) from 0, the optimal subtree will become smaller and smaller.\nSo how do we choose the optimal \\(\\alpha\\)?\n\nWe perform \\(K\\)-fold cross-validation!\nMore specifically, we assess the predictive performance of the subtrees on each of the \\(K\\) hold-out folds as a function of \\(\\alpha\\) and then pick the \\(\\alpha\\) that minimizes the cross-validated test MSE.\n\nHaving determined the optimal \\(\\alpha\\) in this way, we can then apply it to obtain the optimally pruned tree on the entire training data set.\nFortunately, this process is implemented in a highly optimized R package called rpart that does most of this work for us. Let’s have a look at it!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-fitting-and-visualization",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-fitting-and-visualization",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Fitting and visualization",
    "text": "Regression trees in R – Fitting and visualization\n\nThe main function in the rpart package is also called rpart. Besides fitting a regression tree, it performs the cross-validation analysis for \\(\\alpha\\) automatically in the background.\nTo use it, we need to pass it a model formula and data (like in lm) as well as the minimum \\(\\alpha\\) value, which is referred to as cp (complexity parameter) in the package (0.01 by default).\nUsing all the variables in the baseball data, we can thus fit a “full” tree like this:\n\nlibrary(rpart)\n\nrt_full &lt;- rpart(logSalary ~ . - Salary, data = Hitters,\n                 control = list(cp = 0))\n\nThe output from print(rt) is not very user-friendly so we will visualize the tree using the rpart.plot package (result on the next slide):\n\nlibrary(rpart.plot)\nrpart.plot(rt_full)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-visualization",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-visualization",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Visualization",
    "text": "Regression trees in R – Visualization"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nAs expected, this is quite a large tree! So how do we go about optimally pruning it?\nAs indicated before, rpart has done the necessary cross-validation (10-fold by default) exercise during fitting. We can look at the result by calling\n\n\nprintcp(rt_full)\n\n\nRegression tree:\nrpart(formula = logSalary ~ . - Salary, data = Hitters, control = list(cp = 0))\n\nVariables actually used in tree construction:\n [1] AtBat   CAtBat  CHits   CHmRun  CRBI    CRuns   Hits    PutOuts Walks  \n[10] Years  \n\nRoot node error: 207.15/263 = 0.78766\n\nn= 263 \n\n          CP nsplit rel error  xerror     xstd\n1  0.5689379      0   1.00000 1.00594 0.065303\n2  0.0612877      1   0.43106 0.46493 0.052709\n3  0.0577844      2   0.36977 0.44229 0.055377\n4  0.0307862      3   0.31199 0.36508 0.061041\n5  0.0219449      4   0.28120 0.34821 0.059012\n6  0.0130968      5   0.25926 0.35770 0.060186\n7  0.0117008      6   0.24616 0.36255 0.060416\n8  0.0106994      7   0.23446 0.35751 0.059687\n9  0.0082164      8   0.22376 0.35799 0.060102\n10 0.0054925      9   0.21555 0.34999 0.058923\n11 0.0052416     10   0.21005 0.34842 0.059175\n12 0.0048933     14   0.18886 0.35715 0.062130\n13 0.0043587     15   0.18397 0.35914 0.062019\n14 0.0043174     17   0.17525 0.36168 0.062240\n15 0.0032817     18   0.17094 0.36331 0.062404\n16 0.0029170     19   0.16766 0.36551 0.064355\n17 0.0021653     20   0.16474 0.36653 0.064503\n18 0.0019773     21   0.16257 0.36330 0.064039\n19 0.0000000     22   0.16060 0.36789 0.068038"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-1",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nThe main output of that function is a table with five columns:\n\nCP: The complexity parameter, a scaled version of \\(\\alpha\\). In each row, it is the smallest penalty parameter, for which the number of splits is still equal to nsplit, e.g. 0.06129 is the smallest value for which there is only one split.\nnsplit: The number of splits induced by the value of CP.\nrel error: Scaled training RSS. More precisely, it is \\(RSS/TSS\\) in-sample. Will always decrease as the number of splits (i.e. the size of the tree) increases.\nxerror: Scaled test RSS. More precisely, it is \\(RSS/TSS\\) out-of-sample (i.e. averaged across the hold-out folds). Primary metric for assessing optimal number of splits.\nxstd: Standard deviation of xerror over the different folds."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-2",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nWith this table, we can now plot the scaled RSS (in train and test sample) on the y-axis against the number of splits on the x-axis and then choose the number of splits optimally.\nIn choosing the number of splits, we want to balance to conflicting goals:\n\nChoose as large a tree as necessary to deliver a good fit.\nAmong those with similar goodness-of-fit, choose the smallest tree possible.\n\nA good, yet inevitably somewhat subjective heuristic is therefore: Choose the number of splits where the scaled test RSS starts to flatten out.\nLet’s create the plot to see where that point would be in our example (Note: we do not have to use ggplot2 to create this plot, a quick way to do it is to simply run the function rsq.rpart on our tree)!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-3",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_error &lt;- data.frame(nsplit = rep(rt_full$cptable[, 2], 2),\n                       rss_scaled = as.vector(rt_full$cptable[, 3:4]),\n                       type = rep(c(\"train\", \"test\"), each = nrow(rt_full$cptable)))\n\nggplot(df_error, aes(x = nsplit, y = rss_scaled, group = type, color = type)) +\n  geom_vline(xintercept = 4, linetype = 2, linewidth = 0.25) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Number of splits\", y = \"Scaled RSS (RSS/TSS)\", color = \"Sample\",\n       title = \"Using cost complexity pruning to choose the optimal number of splits\",\n       subtitle = \"Scaled train RSS and scaled test RSS (from 10-fold cross-validation) as a function of the number of splits\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-4",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-pruning-4",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Pruning",
    "text": "Regression trees in R – Pruning\n\nSeems like a good value for the number of splits in the baseball example is 4. Increasing the number of splits to any point beyond that does not deliver any (meaningful) reduction in the scaled test RSS.\nNow, looking back to the table, we have to find the complexity parameter cp corresponding to nsplit = 4:\n\nrt_full$cptable[rt_full$cptable[,\"nsplit\"] == 4, ]\n\n        CP     nsplit  rel error     xerror       xstd \n0.02194488 4.00000000 0.28120373 0.34820532 0.05901179 \n\n\nIt is 0.02194488. To finally optimally prune the tree, we now have to apply the prune function to our full tree rt_full together with the optimal cp parameter:\n\nrt_pruned &lt;- prune(rt_full, cp = 0.02194488)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-optimally-pruned-tree",
    "href": "slides/05_The_Data_Science_Workflow_III.html#regression-trees-in-r-optimally-pruned-tree",
    "title": "Data Science and Data Analytics",
    "section": "Regression trees in R – Optimally pruned tree",
    "text": "Regression trees in R – Optimally pruned tree\n\n# Plotting pruned tree with some extra information:\nrpart.plot(rt_pruned, extra = 101, digits = 4)\n\n\nThis is now a much more parsimonious model!"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees",
    "text": "Classification trees\n\nWhen decision trees are used for classification tasks, they are referred to as classification trees.\nAlmost all of the logic regarding recursive binary splitting, tree growing and pruning carries over from regression trees, but there are two crucial differences:\n\nIn regression trees, the predicted response is the mean response of the training observations belonging to the same terminal node. In classification trees, we use the most commonly occurring class.\nIn regression trees, we use RSS as a criterion for making the binary splits. In classification trees, we use the so-called Gini index instead.\n\nLet’s grow a tree on the click data set from earlier (with predictors area_income and age) to dive deeper into these differences."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Example",
    "text": "Classification trees – Example"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example-1",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Example",
    "text": "Classification trees – Example\n\nThe grown classification tree has five internal nodes and six terminal nodes (leaves). From left to right, we label the six resulting rectangular regions by \\(R_1, R_2, R_3, R_4, R_5\\) and \\(R_6\\).\nThe predicted class in each leaf is the one having the majority among the observations in there: green for click and blue for no click with the darkness indicating the purity of the node.\nEach node displays the following pieces of information:\n\nThe majority class at the top (0 or 1 for no click or click).\nThe number of observations falling into each of these classes in that node.\nThe percentage of all observations in the node.\n\nThe rectangular regions and resulting decision boundary can be highlighted in the two-dimensional scatter plot from earlier."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-example-2",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Example",
    "text": "Classification trees – Example"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-gini-index",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-gini-index",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Gini index",
    "text": "Classification trees – Gini index\n\nClassification trees are still built with recursive binary splitting, only the criterion changes as RSS cannot be used for classification.\nInstead, at each step, we choose the split that minimizes the Gini index \\[G = \\sum_{k=1}^K \\hat{p}_{mk} (1 - \\hat{p}_{mk}),\\] where \\(K\\) is the number of classes and \\(\\hat{p}_{mk}\\) is the proportion of training observations in the mth region that are from the kth class.\nThe Gini index is a measure of node purity:\n\nIt is low when most observations belong to the same class.\nIt is high when they are evenly distributed among classes."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Recursive binary splitting",
    "text": "Classification trees – Recursive binary splitting\nLet’s consider all possible splits on the variable area_income:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting-1",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Recursive binary splitting",
    "text": "Classification trees – Recursive binary splitting\nThen, we run the same procedure for age to check if we get any lower Gini:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-recursive-binary-splitting-2",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees – Recursive binary splitting",
    "text": "Classification trees – Recursive binary splitting\n\nBased on these results, we can determine the first split:\n\nFor the variable area_income, the lowest Gini was achieved for \\(s = 50337.93\\). The lowest achievable Gini index was 0.3983.\nFor the variable age, the lowest Gini was achieved for \\(s = 38\\). The lowest achievable Gini index was 0.3947.\nTherefore, the first split will be based on age for \\(s = 38\\). We split the predictor space into two rectangles: one where age &lt; 38 and one where age &gt;= 38, both times irrespective of the area_income.\n\nThis of course again corresponds to the result we already saw. In recursive binary splitting, we would now go on splitting like this until some stopping criterion is reached."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-fitting",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-fitting",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Fitting",
    "text": "Classification trees in R – Fitting\n\nIn R, not much changes when we want to fit a classification tree compared to a regression tree. As long as the response is converted to a factor, the rpart function will automatically fit a classification tree.\nSo, let’s make sure the response in the click data set is a factor:\n\nclick$clicked_on_ad &lt;- factor(click$clicked_on_ad)\n\nNow, we can grow a “full” tree again in the same way as before in the click data. We want to use predictors age, area_income, male and daily_time_spent_on_site:\n\nct_full &lt;- rpart(clicked_on_ad ~ age + area_income + male + daily_time_spent_on_site,\n                 data = click, control = list(cp = 0))\n\nWe can again visualize the resulting classification tree using rpart.plot (result on the next slide):\n\nrpart.plot(ct_full)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-visualization",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-visualization",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Visualization",
    "text": "Classification trees in R – Visualization"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-pruning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-pruning",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Pruning",
    "text": "Classification trees in R – Pruning\nAll functionality for pruning also carries over from regression trees. The metric used in the cptable is classification error rate:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_error &lt;- data.frame(nsplit = rep(ct_full$cptable[, 2], 2),\n                       error_rate = as.vector(ct_full$cptable[, 3:4]),\n                       type = rep(c(\"train\", \"test\"), each = nrow(ct_full$cptable)))\n\nggplot(df_error, aes(x = nsplit, y = error_rate, group = type, color = type)) +\n  geom_vline(xintercept = 2, linetype = 2, linewidth = 0.25) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Number of splits\", y = \"Error rate\", color = \"Sample\",\n       title = \"Using cost complexity pruning to choose the optimal number of splits\",\n       subtitle = \"Classification error rate for train and test data (from 10-fold cross-validation) as a function of the number of splits\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-pruning-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-pruning-1",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Pruning",
    "text": "Classification trees in R – Pruning\n\nSeems like a good value for the number of splits in the click example is 2. Increasing the number of splits to any point beyond that does not deliver any (meaningful) reduction in error rate.\nNow, looking at the cptable, we find the corresponding value of cp\n\nct_full$cptable[ct_full$cptable[,\"nsplit\"] == 2, ]\n\n       CP    nsplit rel error    xerror      xstd \n0.0130000 2.0000000 0.1960000 0.2320000 0.0202528 \n\n\nIt is 0.013. To finally optimally prune the tree, we again simply apply the prune function to the full tree ct_full together with the optimal cp parameter:\n\nct_pruned &lt;- prune(ct_full, cp = 0.013)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-optimal-pruning",
    "href": "slides/05_The_Data_Science_Workflow_III.html#classification-trees-in-r-optimal-pruning",
    "title": "Data Science and Data Analytics",
    "section": "Classification trees in R – Optimal pruning",
    "text": "Classification trees in R – Optimal pruning\n\n# Plotting pruned tree with some extra information:\nrpart.plot(ct_pruned, extra = 101)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#final-assessment-of-decision-trees",
    "href": "slides/05_The_Data_Science_Workflow_III.html#final-assessment-of-decision-trees",
    "title": "Data Science and Data Analytics",
    "section": "Final assessment of decision trees",
    "text": "Final assessment of decision trees\n\nWhether used for regression or classification, decision trees have many advantages:\n\nVery easy to explain intuitively. Even easier than linear or logistic regression.\nCan be nicely visualized and easily interpreted as long as they are only moderately large, even by non-experts.\nCan handle qualitative predictors without the need for dummy variables.\n\nHowever, there are some downsides of course:\n\nDue to their simplistic nature, decision trees typically do not have the same level of predictive accuracy as other methods.\nTrees struggle with linearity, e.g. linear decision boundaries (see next slide)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#final-assessment-of-decision-trees-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#final-assessment-of-decision-trees-1",
    "title": "Data Science and Data Analytics",
    "section": "Final assessment of decision trees",
    "text": "Final assessment of decision trees"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-1",
    "title": "Data Science and Data Analytics",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nSo far, this course has focused on supervised learning methods, such as regression and classification.\nIn that setting, we observe both a set of features \\(X_1, \\ldots, X_p\\) for each observation and a response variable \\(Y\\). The goal then is to predict \\(Y\\) using \\(X_1, \\ldots, X_p\\).\nNow, we will instead focus on unsupervised learning, where we only have set a of features \\(X_1, \\ldots, X_p\\) measured on \\(n\\) observations.\nWe are not interested in prediction because we do not have an associated response variable \\(Y\\). Rather, the goal is to discover interesting things about the measurements on \\(X_1, \\ldots, X_p\\).\nWe will focus on one particular such interesting thing, namely: can we find subgroups / clusters in the data?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-clustering-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#unsupervised-learning-clustering-2",
    "title": "Data Science and Data Analytics",
    "section": "Unsupervised learning – Clustering",
    "text": "Unsupervised learning – Clustering\n\nClustering refers to a broad set of techniques for finding subgroups or clusters in a data set. Using clustering, we ideally group the observations such that…\n\nobservations within each group are quite similar to each other and\nobservations in different groups are quite different from each other.\n\nTo make this concrete, we must define what it means for two or more observations to be “similar” or “different”. This is a domain-specific consideration that depends on the data being studied.\nConsider the following application of clustering for market segmentation:\n\nSay we have access to a large number of measurements of (potential) customers, e.g. income, occupation, past purchase behaviour, etc.\nWe can perform clustering to identify subgroups of people who may be more receptive to a particular form of advertising."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#what-is-a-cluster",
    "href": "slides/05_The_Data_Science_Workflow_III.html#what-is-a-cluster",
    "title": "Data Science and Data Analytics",
    "section": "What is a cluster?",
    "text": "What is a cluster?"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#what-is-a-cluster-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#what-is-a-cluster-1",
    "title": "Data Science and Data Analytics",
    "section": "What is a cluster?",
    "text": "What is a cluster?\nClearly, the notion of a cluster can be ambiguous. There is subjective judgement required in deciding what constitutes a cluster:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#clustering-methods",
    "href": "slides/05_The_Data_Science_Workflow_III.html#clustering-methods",
    "title": "Data Science and Data Analytics",
    "section": "Clustering methods",
    "text": "Clustering methods\n\nSince clustering is popular in many fields, there exist a great number of clustering methods. Some of the most popular ones are:\n\nK-means clustering\nHierarchical clustering\nDBSCAN\nModel-based clustering\n…\n\nAs with supervised learning, there is no free lunch: clustering methods perform differently on different data sets.\nAs an introduction to clustering, we will only be looking more deeply into the first of these methods: K-means clustering."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering",
    "text": "K-means clustering\n\nGiven a number of clusters \\(K\\), K-means clustering divides the observations into \\(K\\) distinct, non-overlapping clusters, i.e. each data point is in exactly one subset.\nThe idea is pretty simple:\n\nEach cluster is associated with a so-called centroid. It is essentially a cluster’s midpoint, calculated as the vector of feature means for all the observations in that cluster.\nEach data point is assigned to the cluster with the closest centroid, where “close” is usually defined based on Euclidean distance, i.e. \\[d(x, y) = \\sqrt{(x_1 - y_1)^2 + \\ldots + (x_p - y_p)^2}\\]\n\nThe algorithm is best understood when illustrating it with an example."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nIn the following data set, we have \\(p = 2\\) features named X1 and X2. Based on visual inspection, choosing \\(K = 3\\) for clustering seems appropriate."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-1",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nStep 1: generate \\(K\\) random centroids in the range of the data. We plot the three generated random initial cluster centroids as stars into the plot below:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-2",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nStep 2: Assign each observation to the centroid that it is closest to. Below, we colour each point in accordance with the colour of its assigned centroid:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-3",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nStep 3: Now, re-compute the centroid of each cluster as the mean (in X1 and X2, respectively) of all observations in a given cluster:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-4",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-4",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example\nStep 4: Finally, iterate the assignment of observations to the cluster with the closest centroid and the re-computation of centroids until the clusters are stable:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-5",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-example-5",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering – Example",
    "text": "K-means clustering – Example"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#notes-on-k-means-clustering",
    "href": "slides/05_The_Data_Science_Workflow_III.html#notes-on-k-means-clustering",
    "title": "Data Science and Data Analytics",
    "section": "Notes on K-means clustering",
    "text": "Notes on K-means clustering\n\nDue to the randomness of the initial cluster centroids, K-means clustering is non-deterministic, i.e. the final clustering outcome can be different every time the algorithm is run:\n\nTo mitigate this, we typically run the algorithm on multiple (random) starting values and see which run gives the best result.\nThe clusters labels (1, 2, 3) are also random, so they can be different for different runs even when the same observations are clustered together.\n\nIn general, K-means clustering…\n\nis a simple, easy-to-understand algorithm.\nworks well also in higher dimensions.\ntends to build spherical clusters of equal size."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#notes-on-k-means-clustering-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#notes-on-k-means-clustering-1",
    "title": "Data Science and Data Analytics",
    "section": "Notes on K-means clustering",
    "text": "Notes on K-means clustering\nWith these properties, K-means clustering can be expected to perform well on some data sets, whilst performing badly on others:"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R\n\nLet’s see how to run this algorithm on an actual real-world data set in R!\nWe use the mall_customers data set from Kaggle. This data set contains information on 200 customers of a large mall.\nBesides information on the age, sex and annual income (in thousand USD) of these customers, the data also contains a spending score variable based on customer behaviour and purchasing data:\n\nmall &lt;- read.csv(\"data/mall_customers.csv\")\nnames(mall) &lt;- c(\"customer_id\", \"sex\", \"age\", \"annual_income\", \"spending_score\")\nhead(mall)\n\n  customer_id    sex age annual_income spending_score\n1           1   Male  19            15             39\n2           2   Male  21            15             81\n3           3 Female  20            16              6\n4           4 Female  23            16             77\n5           5 Female  31            17             40\n6           6 Female  22            17             76\n\n\nUsing K-means clustering, we want to perform market segmentation based on the variables annual_income and spending_score."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-1",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R\nLet’s start by plotting these two variables in a scatter plot. Based on this plot, a choice of \\(K = 5\\) seems reasonable (more on how to choose \\(K\\) later)."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-2",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R\n\nImportant: before we apply K-means clustering, we should always scale / standardize our variables!\nThis is because – as with KNN regression or classification – K-means clustering revolves around computing distances between observations, which is highly driven by the scale on which the features are measured.\nTo scale our observations, we can either manually subtract the mean and divide by the standard deviation, or use the R function scale:\n\nmall_scaled &lt;- scale(mall[, c(\"annual_income\", \"spending_score\")])\n\nNow, to apply K-means clustering in R, we use the function kmeans. Besides the (scaled) data, it requires the number of clusters \\(K\\) (argument centers) and the number of random starting points (argument nstart):\n\nkm_mall &lt;- kmeans(mall_scaled, centers = 5, nstart = 10)"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-3",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-3",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R\n\nAmong other things, the resulting kmeans object holds the coordinates of the centroids (centers) and the clustering vector (cluster), which gives the cluster to which each of the observations was assigned.\nLet’s start by looking at the sizes of the clusters:\n\ntable(km_mall$cluster)\n\n\n 1  2  3  4  5 \n22 39 81 35 23 \n\n\nNext, we want to highlight the detected clusters in the original scatter plot using colour (result on the next slide):\n\nmall$Cluster &lt;- factor(km_mall$cluster)\n\nggplot(mall, aes(x = annual_income, y = spending_score, color = Cluster)) +\n  geom_point() +\n  labs(title = \"Annual income (k$) and spending score (1-100) of mall customers\",\n       x = \"Annual income (k$)\", y = \"Spending score (1-100)\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-4",
    "href": "slides/05_The_Data_Science_Workflow_III.html#k-means-clustering-in-r-4",
    "title": "Data Science and Data Analytics",
    "section": "K-means clustering in R",
    "text": "K-means clustering in R"
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k",
    "href": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k",
    "title": "Data Science and Data Analytics",
    "section": "Choosing the number of clusters \\(K\\)",
    "text": "Choosing the number of clusters \\(K\\)\n\nSo far, we have determined a suitable number of clusters \\(K\\) by inspecting the data. While this may work when there are 2 or 3 features, it certainly no longer works in higher dimensions.\nTherefore, we need some other metric to assess clustering quality. One commonly used metric is the within-cluster sum of squares (WCSS): \\[WCSS = \\sum_{k=1}^K \\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2\\]\nThe WCSS measures how much the observations within a given cluster differ from each other:\n\nIt is small when the observations a very tightly packed around the centroid.\nIt is large when they are widely spread around the centroid."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k-1",
    "href": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k-1",
    "title": "Data Science and Data Analytics",
    "section": "Choosing the number of clusters \\(K\\)",
    "text": "Choosing the number of clusters \\(K\\)\n\nTherefore, we want a clustering with as small a \\(WCSS\\) as possible. In fact, K-means clustering is designed to minimize \\(WCSS\\).\nTypically (although not necessarily), the \\(WCSS\\) decreases as we increase the number of clusters.\nConsequently, one way of choosing \\(K\\) is a heuristic called the elbow method:\n\nRun K-means with different values of \\(K\\) and record the \\(WCSS\\) for each.\nPlot the number of clusters against the \\(WCSS\\) and find the “elbow” in the plot, i.e. the point where the reduction in \\(WCSS\\) for an increase in \\(K\\) becomes very small. Choose \\(K\\) accordingly.\n\nWhile this heuristic is easy to understand, it is not always unambiguous and does require some subjective judgement."
  },
  {
    "objectID": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k-2",
    "href": "slides/05_The_Data_Science_Workflow_III.html#choosing-the-number-of-clusters-k-2",
    "title": "Data Science and Data Analytics",
    "section": "Choosing the number of clusters \\(K\\)",
    "text": "Choosing the number of clusters \\(K\\)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn_clusters &lt;- 1:10\nwcss &lt;- sapply(n_clusters, function(k) kmeans(mall_scaled, centers = k, nstart = 10)$tot.withinss)\nwcss &lt;- data.frame(k = n_clusters, wcss = wcss)\n\nggplot(wcss, aes(x = k, y = wcss)) + \n  geom_line() + \n  geom_point() +\n  geom_vline(xintercept = 5, linetype = 2) + \n  scale_x_continuous(breaks = n_clusters) + \n  labs(title = \"Choosing K in the mall_customers data\",\n       subtitle = \"The elbow in the WCSS plot is at K = 5\",\n       x = \"K\", y = \"WCSS\") + \n  theme_minimal()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#the-data-science-workflow-visualize",
    "href": "slides/04_The_Data_Science_Workflow_II.html#the-data-science-workflow-visualize",
    "title": "Data Science and Data Analytics",
    "section": "The Data Science workflow – Visualize",
    "text": "The Data Science workflow – Visualize"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data",
    "href": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data",
    "title": "Data Science and Data Analytics",
    "section": "Why even look at data?",
    "text": "Why even look at data?\n\nGiven the multitude of ways we can describe data numerically (using descriptive statistics), why do we even bother to present data visually?\nThe answer is that descriptive statistics can never tell us the full story about our data. Consider the following artificial data set from Anscombe (1973):\n\n\n\n\n\n\nIt is a collection of four data sets, each with 11 observations for variables x and y. Here is the first 2 rows of each:\n\n\n   data_id  x     y\n1        1 10  8.04\n2        1  8  6.95\n12       2 10  9.14\n13       2  8  8.14\n24       3  8  6.77\n25       3 13 12.74\n36       4  8  7.71\n37       4  8  8.84\n\n\n\nAll these four data sets have: same mean and SD in x and y, and same correlation between them:\n\n\n  data_id mean_x  sd_x mean_y  sd_y   cor\n1       1      9 3.317  7.501 2.032 0.816\n2       2      9 3.317  7.501 2.032 0.816\n3       3      9 3.317  7.500 2.030 0.816\n4       4      9 3.317  7.501 2.031 0.817\n\n\nSo, seems like these data sets should look pretty similar, right?"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data-1",
    "title": "Data Science and Data Analytics",
    "section": "Why even look at data?",
    "text": "Why even look at data?"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#why-even-look-at-data-2",
    "title": "Data Science and Data Analytics",
    "section": "Why even look at data?",
    "text": "Why even look at data?\n\nWhile this is an extreme, manufactured example, it does prove a point: visualizing data offers insight that the study of pure statistics does not.\nBut that does not mean looking at data is all one needs to do:\n\nReal data sets are often complicated and messy, displaying them graphically presents problems of its own.\nThe core problem of visualization is how to map relations between observations to visual representations in the plot, like shape, lines, colour, size, etc.\nHow to do this optimally for each given data set is subject to considerable debate. There is no simple recipe to follow.\n\nFortunately, however, there is software that provides a lot of support in our data visualization endeavours."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#data-visualization-in-r",
    "href": "slides/04_The_Data_Science_Workflow_II.html#data-visualization-in-r",
    "title": "Data Science and Data Analytics",
    "section": "Data visualization in R",
    "text": "Data visualization in R\n\n\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.”\nJohn Tukey (Statistician, 1915 - 2000)\n\n\n\n\nThere are many tools for visualizing data. Of course, we use R 😉\nEven with R, there are many approaches / systems / packages for creating data visualizations. By far the most common one is the package ggplot2.\nThe gg in ggplot2 stands for the grammar of graphics, which is a coherent system for describing and building graphs developed by statistician Leland Wilkinson in his aptly named 2005 book.\nAs we will see in great detail, this system constructs plots in layers that are built on top of one another in an additive fashion."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#the-grammar-of-graphics",
    "href": "slides/04_The_Data_Science_Workflow_II.html#the-grammar-of-graphics",
    "title": "Data Science and Data Analytics",
    "section": "The grammar of graphics",
    "text": "The grammar of graphics\n\nThe grammar is a set of rules for producing graphics from data, taking pieces of data and mapping them to geometric objects (like points and lines) that have aesthetic attributes (like position, colour and size), together with further rules for transforming the data if needed, adjusting scales, adapting coordinate system and themes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: a grammar limits the structure of what you can say, but it does not automatically make what you say meaningful, i.e. your code just being “grammatically” correct does not make the resulting plot sensible."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins",
    "href": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins",
    "title": "Data Science and Data Analytics",
    "section": "An introductory example – Palmer penguins",
    "text": "An introductory example – Palmer penguins\nLet’s see an example of how this works with ggplot2 in practice.\n\n\nThe data set we will use for our first ggplot is the palmerpenguins data set that can be obtained from CRAN in a package that bears exactly that name.\nIt contains size measurements, clutch observations, and blood isotope ratios for three penguin species observed on three islands in the Palmer Archipelago, Antarctica over a study period of three years."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins-1",
    "title": "Data Science and Data Analytics",
    "section": "An introductory example – Palmer penguins",
    "text": "An introductory example – Palmer penguins\nMake sure the required packages palmerpenguins and ggplot2 are installed. Then we can load them and start by inspecting the data set:\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\npenguins &lt;- as.data.frame(penguins)\nhead(penguins)\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n6  Adelie Torgersen           39.3          20.6               190        3650\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n6   male 2007\n\n\n\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-palmer-penguins-2",
    "title": "Data Science and Data Analytics",
    "section": "An introductory example – Palmer penguins",
    "text": "An introductory example – Palmer penguins\nAs zoologists, we might be interested in the following questions about the different types of Palmer penguins:\n\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers?\nWhat does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Non-linear?\nDoes the relationship vary by the species of the penguin? How about by the island where the penguin lives?\n\nLet’s answer all of these questions using a single visualization."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-ultimate-goal",
    "href": "slides/04_The_Data_Science_Workflow_II.html#an-introductory-example-ultimate-goal",
    "title": "Data Science and Data Analytics",
    "section": "An introductory example – Ultimate goal",
    "text": "An introductory example – Ultimate goal\nUltimately, we want to create the following plot:"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#first-step-telling-ggplot-about-our-data-set",
    "href": "slides/04_The_Data_Science_Workflow_II.html#first-step-telling-ggplot-about-our-data-set",
    "title": "Data Science and Data Analytics",
    "section": "First step – Telling ggplot about our data set",
    "text": "First step – Telling ggplot about our data set\nLet’s recreate this plot step-by-step.\n\nWith ggplot2, we begin a plot with the function ggplot. Its first argument is the dataset to use in the graph.\n\n\n\nggplot(data = penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor now, we have not told ggplot how to visualize the data, so we only have an empty canvas. Now, we will “paint” onto this canvas in layers."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#second-step-specifying-aesthetic-mappings",
    "href": "slides/04_The_Data_Science_Workflow_II.html#second-step-specifying-aesthetic-mappings",
    "title": "Data Science and Data Analytics",
    "section": "Second step – Specifying aesthetic mappings",
    "text": "Second step – Specifying aesthetic mappings\n\nThe second argument of ggplot is called mapping. It defines how variables in our data set are mapped to visual properties (aesthetics) of our plot. This argument is always defined in the aes function, and the x and y arguments of aes specify which variables to map to the x- and y-axes. We want flipper length on the x and body mass on the y axis.\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, our empty canvas has more structure: x- and y-axis have a range, ticks and labels. But the penguins themselves are not yet on the plot."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#third-step-additively-layer-on-geoms",
    "href": "slides/04_The_Data_Science_Workflow_II.html#third-step-additively-layer-on-geoms",
    "title": "Data Science and Data Analytics",
    "section": "Third step – Additively layer on geoms",
    "text": "Third step – Additively layer on geoms\n\nThis is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot. To do so, we need a geom, a geometrical object used for data representation. In the example, we want our data represented by points, so we use geom_point:\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we start to have an actual scatter plot. Note that R warns us about missing data points. We will suppress this warning in the plots to come."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-note-about-geoms",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-note-about-geoms",
    "title": "Data Science and Data Analytics",
    "section": "A note about geoms",
    "text": "A note about geoms\nIn ggplot2, geometric objects are made available with functions that start with geom_. People often describe plots by the type of geom that the plot uses, for example:\n\nScatter plots use point geoms (geom_point) as we just saw.\nBar charts use bar geoms (geom_bar)\nLine charts use line geoms (geom_line)\nBoxplots use boxplot geoms (geom_boxplot)\n…\n\nWe layer a geom onto a ggplot literally in an additive way, i.e. by combining the two with a + sign. We will learn how to deal with many different geoms in this way."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers",
    "title": "Data Science and Data Analytics",
    "section": "Fourth step – Adding aesthetics and layers",
    "text": "Fourth step – Adding aesthetics and layers\n\nDoes the relationship between flipper length and body mass differ by species? To answer this, let’s represent species with different coloured points. To achieve this, we modify the aesthetic to additionally map species to colour:\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn mapping a categorical variable to the colour aesthetic, ggplot2 automatically assigns a unique colour to each factor level (i.e. each species), a process known as scaling. ggplot2 will also add a legend that explains which values correspond to which levels."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-1",
    "title": "Data Science and Data Analytics",
    "section": "Fourth step – Adding aesthetics and layers",
    "text": "Fourth step – Adding aesthetics and layers\n\nNow, let’s add one more layer, a smooth curve displaying the relationship between body mass and flipper length. Since this is a new geometric object representing our data, we will add a new geom, namely geom_smooth based on a linear model (lm):\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have successfully added lines, but this plot does not look like our ultimate goal. Instead of having only one line for the entire data set, we have separate lines for each of the three penguin species. What went wrong?"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-2",
    "title": "Data Science and Data Analytics",
    "section": "Fourth step – Adding aesthetics and layers",
    "text": "Fourth step – Adding aesthetics and layers\n\nWhen aesthetics are defined in ggplot(), at the top level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function can also take a mapping argument, which allows for aesthetics at the local level. Here, we want point colours, but not lines separated by species, so we should specify color = species locally for geom_point only:\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are getting close to our ultimate goal… Some minor details are still missing though."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fourth-step-adding-aesthetics-and-layers-3",
    "title": "Data Science and Data Analytics",
    "section": "Fourth step – Adding aesthetics and layers",
    "text": "Fourth step – Adding aesthetics and layers\n\nDue to differences in colour perception (e.g. colour blindness), it is generally not a good idea to represent information using only colours on a plot. Therefore, in addition to colour, we can also map species to the shape aesthetic:\n\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point(mapping = aes(color = species,\n                           shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the legend is automatically updated to reflect the different shapes of the points as well."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#fifth-step-adjust-scales-labels-titles",
    "href": "slides/04_The_Data_Science_Workflow_II.html#fifth-step-adjust-scales-labels-titles",
    "title": "Data Science and Data Analytics",
    "section": "Fifth step – Adjust scales, labels, titles, …",
    "text": "Fifth step – Adjust scales, labels, titles, …\n\nThe “clean-up” work involves setting appropriate labels for title, subtitle, axes, and legend using the labs function and using the colourblind safe colour palette scale_color_colorblind from the ggthemes package (an extension of ggplot2):\n\n\n\n\nlibrary(ggthemes)\n\nggplot(data = penguins,\n       mapping = aes(x = flipper_length_mm,\n                     y = body_mass_g)) +\n  geom_point(mapping = aes(color = species,\n                           shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Flipper length (mm)\",\n       y = \"Body mass (g)\",\n       title = \"Body mass and flipper length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       color = \"Species\",\n       shape = \"Species\") +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we are done, we have built the plot of our ultimate goal step-by-step from the ground up. The example highlights the core principles of how ggplot2 allows us to build nice-looking informative graphs with relatively little coding effort."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#steps-in-creating-a-plot-with-ggplot2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#steps-in-creating-a-plot-with-ggplot2",
    "title": "Data Science and Data Analytics",
    "section": "Steps in creating a plot with ggplot2",
    "text": "Steps in creating a plot with ggplot2\nConceptually, the steps we went through are the same for every plot created with ggplot2. Based on a tidy data set, we always proceed as follows:\n\nTelling ggplot about our data set.\nSpecifying aesthetic mappings, i.e. what relationships we want to see and by what we want them represented.\nAdditively layer on geoms as needed.\nAdd and adapt aesthetics and layers until all relationships are displayed.\nAdjust scales, labels, titles, …\n\n\nThese steps are always the same. We will now only learn in greater detail about how to tell ggplot what to do by going through several examples of different data sets, aesthetic mappings, geoms and more. As with anything, the key to improvement is LOTS of trial and error…"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Histogram",
    "text": "Visualizing a numerical variable – Histogram\nLet’s say we want to visualize the distribution of body mass (in grams) across all penguins in the sample using a histogram. For this, we map the variable body_mass_g to x and use geom_histogram:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-1",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Histogram",
    "text": "Visualizing a numerical variable – Histogram\nA histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. ggplot2 by default creates 30 such bins. We can control this by setting bins ourselves:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(bins = 15)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-2",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Histogram",
    "text": "Visualizing a numerical variable – Histogram\nWe do not like the colour of this histogram. Previously, we set the color aesthetic, but here we do not want to map colour to a variable. Instead we want to set it to a specific colour. This we do in the geom, not in the aes:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(color = \"deepskyblue4\", bins = 15)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-histogram-3",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Histogram",
    "text": "Visualizing a numerical variable – Histogram\nHm, this is not what we wanted… ggplot only coloured the border line of the bars. Turns out that if we want to fill the bars in a specific colour, we need to use the fill argument of the geom_histogram function:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"deepskyblue4\", bins = 15)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-density-plot",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-density-plot",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Density plot",
    "text": "Visualizing a numerical variable – Density plot\nAn alternative visualization for distributions of numerical variables is a density plot, which is a smoothed-out version of a histogram. To create one, we simply switch out our geom to geom_density:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-density-plot-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-numerical-variable-density-plot-1",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a numerical variable – Density plot",
    "text": "Visualizing a numerical variable – Density plot\nThat’s a bit boring, let’s change the colour again (both of the line itself using color and the area below the curve using fill):\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density(color = \"deepskyblue4\", fill = \"deepskyblue4\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a categorical variable – Barplot",
    "text": "Visualizing a categorical variable – Barplot\nA simple bar chart visualizes the (absolute or relative) frequencies of the levels of a categorical variable. While we could use ggplot directly on the data, it is generally preferable to compute frequencies explicitly ourselves, e.g. for species:\n\nCodePlot\n\n\n\nspecies_freq &lt;- as.data.frame(table(penguins$species))\nnames(species_freq)[1] &lt;- \"species\"\n\n# As we have already computed frequencies, we have to tell the bar geom\n# that it should use the values as they are, i.e. use the \"identity\":\nggplot(species_freq, aes(x = species, y = Freq)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue4\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot-1",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a categorical variable – Barplot",
    "text": "Visualizing a categorical variable – Barplot\nIf the variable has a nominal (and not ordinal) scale, then it is usually preferable to order the bars based on their frequency. For this, we have to reorder the factor levels for the species:\n\nCodePlot\n\n\n\nggplot(species_freq, aes(x = reorder(species, Freq, decreasing = TRUE), y = Freq)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue4\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#visualizing-a-categorical-variable-barplot-2",
    "title": "Data Science and Data Analytics",
    "section": "Visualizing a categorical variable – Barplot",
    "text": "Visualizing a categorical variable – Barplot\nOften, we want the bars to be horizontal rather than vertical. To achieve this, we simply flip the axes using coord_flip. Additionally, we add nice axis labels:\n\nCodePlot\n\n\n\nggplot(species_freq, aes(x = reorder(species, Freq, decreasing = TRUE), y = Freq)) +\n  geom_bar(stat = \"identity\", fill = \"deepskyblue4\") +\n  labs(x = \"Penguin Species\", y = \"Observed frequency\") +\n  coord_flip()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nTo visualize the relationship between a categorical and a numerical variable, we can create parallel boxplots. For example, for the distribution of body weight by species, we could try:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-1",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nAn alternative to boxplots are so-called violin plots. In their simplest form, these are rotated density plots of the numerical variable in each category mirrored to create a symmetric, “violin-like” object:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-2",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nSince density plots can also mask certain aspects of the distribution of the data, it is often advisable to additionally plot the actual data points on top of the violin plots. We can do this by layering a point geom on top:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, color = species)) +\n  geom_violin() +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-3",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nIf we have a lot of points close together, then they overlap and it becomes impossible to tell their distribution. To avoid this, we can slightly perturb the points in the x direction. This is achieved by switching to geom_jitter.\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, color = species)) +\n  geom_violin() +\n  geom_jitter()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-4",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-4",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nAnother alternative to displaying the distribution of body weights in the three penguin species is to combine their densities into one plot in the usual way, i.e. with the numerical variable on the x axis.\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g, fill = species)) +\n  geom_density()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-5",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-categorical-and-a-numerical-variable-5",
    "title": "Data Science and Data Analytics",
    "section": "A categorical and a numerical variable",
    "text": "A categorical and a numerical variable\nNow, the densities are overlapping. To avoid this, we could of course not map the fill aesthetic. A nice alternative is to set the alpha aesthetic, which controls transparency. 0 is max transparency, 1 is max opaqueness (default).\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = body_mass_g, fill = species)) +\n  geom_density(alpha = 0.5)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#two-categorical-variables",
    "href": "slides/04_The_Data_Science_Workflow_II.html#two-categorical-variables",
    "title": "Data Science and Data Analytics",
    "section": "Two categorical variables",
    "text": "Two categorical variables\nWe can use grouped or stacked bar plots to visualize the relationship between two categorical variables. For example, we might want to see how often the different species occur on the different islands.\n\nCodePlot\n\n\n\nspecies_freq &lt;- as.data.frame(table(penguins$species, penguins$island))\nnames(species_freq)[1:2] &lt;- c(\"species\", \"island\")\n\nggplot(species_freq, aes(x = island, y = Freq, fill = species)) +\n  geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#two-categorical-variables-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#two-categorical-variables-1",
    "title": "Data Science and Data Analytics",
    "section": "Two categorical variables",
    "text": "Two categorical variables\nWe can create variations of this plot by changing the position argument of geom_bar. The default is stack. Most useful for comparing distributions are relative frequency plots achieved with position = \"fill\":\n\nCodePlot\n\n\n\nggplot(species_freq, aes(x = island, y = Freq, fill = species)) +\n  geom_bar(stat = \"identity\", position = \"fill\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#two-numerical-variables",
    "href": "slides/04_The_Data_Science_Workflow_II.html#two-numerical-variables",
    "title": "Data Science and Data Analytics",
    "section": "Two numerical variables",
    "text": "Two numerical variables\nIn the introductory example, we have already seen scatter plots and smooth curves as ways to illustrate the relationship between two numerical variables. Scatter plots are an absolute staple in a data scientist’s toolbox:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables",
    "text": "Three or more variables\nThe introductory example displayed three variables: flipper_length_mm (x), body_mass_g (y) and species (color and shape). In general, we have the following options to introduce additional variables into a scatter plot:\n\nUse additional aesthetics:\n\nColour \\(\\to\\) color (categorical or numerical variables)\nShape \\(\\to\\) shape (categorical)\nSize \\(\\to\\) size (categorical or numerical variables)\nTransparency \\(\\to\\) alpha (categorical or numerical variables)\n\nSplit plot into facets, subplots that each display one subset of the data (categorical variables).\nAnimate the plot (categorical or numerical variables, especially time).\nCreate a 3D scatter plot (categorical or numerical variables)."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-1",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables",
    "text": "Three or more variables\n\nWhen using aesthetics to represent additional variables, we must be aware that different sorts of variables can be represented more or less well by different kinds of visual representations.\nWhile there are no one-size-fits-all recipes, consider the following order as a heuristic when introducing additional variables into a 2D scatter plot:\n\nFor continuous variables: size, colour, transparency.\nFor categorical variables: colour, shape.\n\nFor example, in our scatter plot of penguin flipper lengths vs. body masses, we might want to introduce categorical information on species and island. By the heuristic, we use colour for the first and shape for the second."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-colour-and-shape",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-colour-and-shape",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Colour and shape",
    "text": "Three or more variables – Colour and shape\nIn the default setting, the points are quite small, which makes it hard to distinguish the shapes. To mitigate this, we additionally set the size aesthetic to something larger, namely 3.\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-colour-and-size",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-colour-and-size",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Colour and size",
    "text": "Three or more variables – Colour and size\nLet’s say we wanted a scatter plot of penguin bill length vs. bill depth for different species. Additionally, we want information on body mass included as well. A logical choice to represent the latter would be size:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     color = species, size = body_mass_g)) +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-multiple-aesthetics",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-multiple-aesthetics",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Multiple aesthetics",
    "text": "Three or more variables – Multiple aesthetics\nIf we now really wanted to go mad, we could add the island back in via the shape aesthetic and additionally include the flipper length via the alpha aesthetic, so that longer flippers are represented by more opaque points:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     color = species, size = body_mass_g,\n                     shape = island, alpha = flipper_length_mm)) +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Faceting",
    "text": "Three or more variables – Faceting\n\nClearly, there is now too much information in this plot.\nAdding too many aesthetic mappings to a plot makes it cluttered and difficult to make sense of.\nAnother way to introduce additional categorical variables is to split your plot into several facets.\nA facet is a subplot that displays one subset of the data. For example, there could be one facet for each factor level of a categorical variable.\nWe can use facets to explore conditional relationships, like the relationship between flipper length and body weight on each of the three islands."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting-1",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Faceting",
    "text": "Three or more variables – Faceting\nTo facet a plot by a single variable, we use facet_wrap. It takes a so-called formula as its first argument, which we create with ~ followed by the name of a categorical variable.\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species)) +\n  geom_point() +\n  facet_wrap(~island)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-faceting-2",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Faceting",
    "text": "Three or more variables – Faceting\nTo facet a plot by two variables, we use facet_grid. We specify the variable to be faceted in the rows before the ~ and the variable to be faceted in the columns after it. For example, we could additionally facet sex in the rows:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species)) +\n  geom_point() +\n  facet_grid(sex~island)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Labels",
    "text": "Three or more variables – Labels\n\nFor some data sets, we can also display an additional variable in a scatter plot by creating informative labels for the points.\nOur penguin data set is not a good example to illustrate this, so we consider the elections_historic data set from the socviz package instead:\n\nlibrary(socviz)\nelections_historic &lt;- as.data.frame(elections_historic)\nhead(elections_historic[, c(\"year\", \"winner\", \"win_party\", \"ec_pct\",\n                            \"popular_pct\", \"winner_label\")])\n\n  year                 winner win_party ec_pct popular_pct  winner_label\n1 1824      John Quincy Adams     D.-R. 0.3218      0.3092    Adams 1824\n2 1828         Andrew Jackson      Dem. 0.6820      0.5593  Jackson 1828\n3 1832         Andrew Jackson      Dem. 0.7657      0.5474  Jackson 1832\n4 1836       Martin Van Buren      Dem. 0.5782      0.5079    Buren 1836\n5 1840 William Henry Harrison      Whig 0.7959      0.5287 Harrison 1840\n6 1844             James Polk      Dem. 0.6182      0.4954     Polk 1844\n\n\nThis data set contains information on US presidential elections from 1824 to 2016. We will have a look at the winner’s share of the electoral college vote and the popular vote."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-1",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Labels",
    "text": "Three or more variables – Labels\nIn a scatter plot, we want to have the popular vote share on the x axis and the electoral college vote share on the y axis. If we only map these two aesthetics, the plot looks quite uninformative…\n\nCodePlot\n\n\n\nggplot(elections_historic, aes(x = popular_pct, y = ec_pct)) +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-2",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Labels",
    "text": "Three or more variables – Labels\nWe do not know who any of these points represent. This is a perfect use case for labeling the points. For this, we can use the label aesthetic together with the text geom (there is also a label geom, which would work as well):\n\nCodePlot\n\n\n\nggplot(elections_historic, aes(x = popular_pct, y = ec_pct, label = winner_label)) +\n  geom_text() +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#three-or-more-variables-labels-3",
    "title": "Data Science and Data Analytics",
    "section": "Three or more variables – Labels",
    "text": "Three or more variables – Labels\nThat is much better already. However, now, there is a lot of overlap. To remedy this, we need an additional library called ggrepel, which provides us with a text_repel geom to avoid overlapping labels:\n\nCodePlot\n\n\n\nlibrary(ggrepel)\nggplot(elections_historic, aes(x = popular_pct, y = ec_pct, label = winner_label)) +\n  geom_text_repel() +\n  geom_point()"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#customizing-our-plots",
    "href": "slides/04_The_Data_Science_Workflow_II.html#customizing-our-plots",
    "title": "Data Science and Data Analytics",
    "section": "Customizing our plots",
    "text": "Customizing our plots\n\nSo far, we have seen how to create and extend basic plots, like bar plots, density plots, histograms, and scatter plots by tailoring aesthetic mappings and layering on suitable geoms according to our needs.\nHowever, the customization usually does not stop there. We might want to …\n\n… add informative titles, subtitles, captions and axis labels to the plot.\n… use colours and shapes different from the ones ggplot uses by default.\n… adapt the scale of the x and / or y axis.\n… change the font type and size for labels and titles.\n… set the whole plot in a different theme to suit corporate guidelines.\n… and so much more…\n\nIn ggplot2, virtually every aspect of a plot is customizable. We will only have a look at how to customize some of the most important ones."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-guides-and-themes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-guides-and-themes",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales, guides and themes",
    "text": "Understanding scales, guides and themes\n\nOutside of aesthetic mappings and geoms, there are three types of functions that do most of the heavy lifting when it comes to customizing our plot:\n\nThe family of scale_ functions\nThe guides function and\nThe theme function.\n\nDue to the sheer amount of options for customizability and the partial overlap between some of these functions, things can become confusing very quickly when working through these functions.\nLet’s start with a basic delineation of scales, guides and themes."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-guides-and-themes-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-guides-and-themes-1",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales, guides and themes",
    "text": "Understanding scales, guides and themes\n\nA rough guide on the use of these functions is the following:\n\nScales: every aesthetic mapping has a scale. If we want to adjust how that scale is marked or graduated, we use a scale_ function.\nGuides: Many scales come with a legend to help us interpret the graph. These are called guides and can be adjusted with the help of the guides function.\nThemes: graphs have other features not strictly connected to the structure of the data being displayed. These include background colour, font size, legend placement, etc. To adjust these, we use the theme function.\n\nWe will now dig deeper into each of these three domains of customization and see examples of how the corresponding functions are used to change the appearance of the data displayed."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales",
    "text": "Understanding scales\nLet’s come back to an earlier example from the penguin data set:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3)"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-1",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales",
    "text": "Understanding scales\nThis plot has four aesthetic mappings:\n\nThe variable flipper_length_mm is mapped to x.\nThe variable body_mass_g is mapped to y.\nThe variable species is mapped to color.\nThe variable island is mapped to shape.\n\n\nAnd each of these mappings has a scale:\n\nflipper_length_mm is a continuous variable, so the x scale is continuous.\nbody_mass_g is a continuous variable, so the y scale is continuous.\nspecies is an unordered categorical variable, so the color scale is discrete.\nisland is an unordered categorical variable, so the shape scale is discrete."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-scales-2",
    "title": "Data Science and Data Analytics",
    "section": "Understanding scales",
    "text": "Understanding scales\n\nScales for these mappings may have labels, axis tick marks at particular positions, or specific colours or shapes. If we want to adjust them, we use one of the scale_ functions.\nThese functions have the following general structure:\n\n\n\n\n\nSo, for example:\n\nscale_x_continuous controls x scales for continuous variables\nscale_y_continuous controls y scales for continuous variables\nscale_color_discrete controls color scales for discrete variables\nscale_shape_discrete controls shape scales for discrete variables"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#scales-in-action",
    "href": "slides/04_The_Data_Science_Workflow_II.html#scales-in-action",
    "title": "Data Science and Data Analytics",
    "section": "Scales in action",
    "text": "Scales in action\nLet’s see this in action. Say we wanted to override the default for tick marks by having a mark every 5 mm on the x axis and every 500 g on the y axis. We can use the breaks argument to control the position of the tick marks:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(170, 230, by = 5)) +\n  scale_y_continuous(breaks = seq(3000, 6000, by = 500))"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#scales-in-action-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#scales-in-action-1",
    "title": "Data Science and Data Analytics",
    "section": "Scales in action",
    "text": "Scales in action\nNow, let’s say we also wanted to change the colours and shapes used to represent species and island, respectively. For manually adapting them, we use scale_color_manual and scale_shape_manual:\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(170, 230, by = 5)) +\n  scale_y_continuous(breaks = seq(3000, 6000, by = 500)) +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\")) +\n  scale_shape_manual(values = c(17, 18, 8))"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-shapes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-shapes",
    "title": "Data Science and Data Analytics",
    "section": "A word on shapes",
    "text": "A word on shapes\n\nThe following shapes are most commonly used in R:\n\n\n\n\n\nYou can reference them by using the numbers indicated and passing them to scale_shape_manual in the order of the factor levels.\nIn the example before, we had Biscoe = 17, Dream = 18 and Torgersen = 8."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour",
    "text": "A word on colour\n\nWhenever we map color or fill as an aesthetic, ggplot2 uses default colour or fill scales that do a fine enough job for exploratory data analysis, but typically have to be tweaked for publication-ready plots.\nIn the example before, we have manually overridden the default using scale_color_manual and the names of three colours (red, blue and green).\nChoosing the right colour(s) for data visualization is actually a very complex problem that has to consider adequate representation of the underlying data as well as differences in colour perception.\nWe will only briefly skim the surface of colour choice by looking into some of the excellent options for setting colours that already exist out of the box in ggplot2."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-1",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour",
    "text": "A word on colour\n\nA collection of colours used for representing data is called a palette.\nA very popular family of such palettes available in ggplot2 are the colorbrewer palettes. For discrete variables, they come in three different types:\n\nQualitative: best suited to represent unordered categorical data, e.g. penguin species.\nSequential: best suited to represent ordered data that progresses from low to high, e.g. school grades or counts.\nDiverging: best suited to represent ordered data with a neutral midpoint and extremes diverging in both directions, e.g. Likert scales.\n\nDepending on the type of data we want represented, we can choose a palette from the corresponding family."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-qualitative-palettes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-qualitative-palettes",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – Qualitative palettes",
    "text": "A word on colour – Qualitative palettes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-sequential-palettes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-sequential-palettes",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – Sequential palettes",
    "text": "A word on colour – Sequential palettes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-diverging-palettes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-diverging-palettes",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – Diverging palettes",
    "text": "A word on colour – Diverging palettes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – How to use colorbrewer",
    "text": "A word on colour – How to use colorbrewer\n\nWe can access all of these palettes in our plots by referencing them using the names given in the functions scale_color_brewer and scale_fill_brewer, respectively.\nIn our example, we have the species variable mapped to colour, which is an unordered categorical variable. A suitable palette to represent it might therefore be Dark2, for example.\nTo do this, we would run the following code (result on the next slide):\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(170, 230, by = 5)) +\n  scale_y_continuous(breaks = seq(3000, 6000, by = 500)) +\n  scale_color_brewer(palette = \"Dark2\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-1",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – How to use colorbrewer",
    "text": "A word on colour – How to use colorbrewer"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-2",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – How to use colorbrewer",
    "text": "A word on colour – How to use colorbrewer\n\nWith these colorbrewer palettes, we can represent data from discrete variables, like categorical or count data.\nHowever, what if we want to represent a continuous variable using colour, such as penguin bill length?\nAll sequential and diverging colorbrewer palettes can also be used for continuous scales. Depending on the aesthetic, we only have to pass them into the functions scale_color_distiller or scale_fill_distiller.\nPenguin bill length does not have a neutral midpoint, so we use a sequential palette like YlOrRd to represent it (result on the next slide):\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = bill_length_mm)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = seq(170, 230, by = 5)) +\n  scale_y_continuous(breaks = seq(3000, 6000, by = 500)) +\n  scale_color_distiller(palette = \"YlOrRd\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-3",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-word-on-colour-how-to-use-colorbrewer-3",
    "title": "Data Science and Data Analytics",
    "section": "A word on colour – How to use colorbrewer",
    "text": "A word on colour – How to use colorbrewer"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-final-word-on-colour",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-final-word-on-colour",
    "title": "Data Science and Data Analytics",
    "section": "A final word on colour",
    "text": "A final word on colour\n\nThe options for colour choice are virtually endless. The goal here was merely to introduce colour scales and how to change them.\nIf you find yourself unhappy with the colour palettes provided by colorbrewer, have a look at the Palette Finder in the R Graph Gallery:"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides",
    "title": "Data Science and Data Analytics",
    "section": "Understanding guides",
    "text": "Understanding guides\nNow let’s discuss guides. For this, consider again our violin plot from earlier:\n\nThis plot contains redundant information: the species can be inferred from the axis labels, we do not need the additional legend for species."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides-1",
    "title": "Data Science and Data Analytics",
    "section": "Understanding guides",
    "text": "Understanding guides\nTo “switch off” the guide for a particular aesthetic, we can simply call the guides function with the name of the aesthetic as a named argument set to \"none\":\n\nCodePlot\n\n\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin() +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides-2",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-guides-2",
    "title": "Data Science and Data Analytics",
    "section": "Understanding guides",
    "text": "Understanding guides\n\nOf course, the guides function can do much more than simply to “switch off” legends for individual scales.\nIn fact, it can be used to customize virtually every aspect of a legend such as:\n\nwhere the title of the legend should go\nwhere the labels should be placed\nin which orders the labels should be displayed\nand much more…\n\nAs with many aspects of ggplot2, this bridge is best crossed, when you first come to it…"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-themes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-themes",
    "title": "Data Science and Data Analytics",
    "section": "Understanding themes",
    "text": "Understanding themes\n\nFinally, a very important domain of plot customization is opened up by the theme function. It allows us to customize all aspects that are not directly related to the data being displayed.\nA very common basic use of the theme function is to place the legend at a different location in the plot and to change font sizes of different labels (result on the next slide):\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g,\n                     color = species, shape = island)) +\n  geom_point(size = 3) +\n  labs(title = \"Body mass and flipper length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       caption = \"Source: palmerpenguins package\",\n       color = \"Species\",\n       shape = \"Island\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = rel(1.75)),\n        plot.subtitle = element_text(size = rel(1.5)),\n        plot.caption = element_text(size = rel(0.75)))"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#understanding-themes-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#understanding-themes-1",
    "title": "Data Science and Data Analytics",
    "section": "Understanding themes",
    "text": "Understanding themes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#using-pre-built-themes",
    "href": "slides/04_The_Data_Science_Workflow_II.html#using-pre-built-themes",
    "title": "Data Science and Data Analytics",
    "section": "Using pre-built themes",
    "text": "Using pre-built themes\n\nWhile customizing every single aspect of a plot is definitely possible in ggplot2, it is also incredibly cumbersome and usually not necessary.\nInstead, we can simply layer on a pre-built theme that changes the overall look of a plot all at once.\nThemes that come shipped with ggplot2 are: bw, classic, dark, gray (default), light, linedraw, minimal, test and void.\nThe following slide gives an overview of our plot would look like in each of these themes. If none of those suit your needs, you can have a look at the ggthemes package that has plenty more to offer."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#using-pre-built-themes-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#using-pre-built-themes-1",
    "title": "Data Science and Data Analytics",
    "section": "Using pre-built themes",
    "text": "Using pre-built themes"
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#saving-your-work",
    "href": "slides/04_The_Data_Science_Workflow_II.html#saving-your-work",
    "title": "Data Science and Data Analytics",
    "section": "Saving your work",
    "text": "Saving your work\n\nNow that we know how to create and customize our own plots, we might want to save them to include them in a presentation or send them to a colleague.\nThe easiest way to do this is to use the function ggsave. By default, it will save the most recently created plot into the file name you provide:\n\nggsave(filename = \"my_figure.png\")\n\nYou can save the plot as PDF, JPG, PNG (or several other formats) by changing the file ending accordingly.\nYou can also change the dimensions and the resolution of the plot by changing arguments width, height and dpi, respectively.\nAs always, for more details, refer to the documentation via ?ggsave."
  },
  {
    "objectID": "slides/04_The_Data_Science_Workflow_II.html#a-sneak-peek-into-what-is-possible-1",
    "href": "slides/04_The_Data_Science_Workflow_II.html#a-sneak-peek-into-what-is-possible-1",
    "title": "Data Science and Data Analytics",
    "section": "A sneak peek into what is possible",
    "text": "A sneak peek into what is possible"
  }
]