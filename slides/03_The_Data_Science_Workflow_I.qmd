---
title: "Data Science and Data Analytics"
subtitle: "The Data Science Workflow I -- Import, Tidy and Transform"
author: "Julian Amon, PhD"
date: "March 31, 2025"
date-format: long
institute: Charlotte Fresenius Privatuniversit√§t
footer: "Data Science and Data Analytics -- The Data Science Workflow I"
format:
  revealjs:
    theme:
      - default
      - slides.scss
    width: 1350
    height: 900
    slide-number: true
    logo: img/UOS_Logo.jpg
    fig-width: 14
    controls: true
    embed-resources: true
highlight-style: arrow
execute: 
  warning: true
  echo: true
editor_options: 
  chunk_output_type: console
---

# Import

```{r}
#| echo: false
#| include: false

if(!require("dslabs")) install.packages("dslabs")
if(!require("readr")) install.packages("readr")
if(!require("readxl")) install.packages("readxl")
if(!require("googlesheets4")) install.packages("googlesheets4")

Sys.setlocale("LC_TIME", "en_GB.UTF-8")
set.seed(42)
```

## The Data Science workflow -- Import

![](img/data-science-cycle.002.png){fig-align="center"}

## Importing data into R

-   In order to get our data science workflow started, we need to be able to **import** data from different sources into R.
-   While there is a huge number of different data formats, we will focus on how to import data stored in two of the most common types of formats, namely:
    -   Text files (like `csv` or `tsv`)
    -   Spreadsheets (Excel or Google Sheets)
-   Before we dive into how to get the data stored in such files into R, we need to be able to find these files on our computer in the first place...
-   For this, we first have to look into how R orients itself in the **file system** on our computer.

## The file system

-   A computer's file system consists of nested folders (*directories*). It can be visualized as tree structures, with directories branching out from the root.
-   The **root directory** contains all other directories.
-   The **working directory** is the current location in the filesystem.

![](img/paths_1.png){fig-align="center"}

## Relative and full paths

-   A **path** lists directory names leading to a file. Think of it like instructions on what folders to click on, and in what order, to find the file. We distinguish:
    -   **Full paths**: Starts from the root directory, i.e. the very top of the file system hierarchy. An example would be:
    ```{r}
    example_path <- system.file(package = "dslabs")
    example_path
    ```
    
    -   **Relative paths**: Starts from the current working directory. Imagine the current working directory would be `/home/julian`, then the **relative** path to the folder above would simply be:
    ```{r}
    #| echo: false
    
    sprintf("R%s", strsplit(system.file(package = "dslabs"), "/R")[[1]][2])
    ```

-   In R, we can use the `list.files` function to explore directories:

    ```{r}
    #| cache: true
    
    list.files(example_path)
    ```


## Relative and full paths

![](img/paths_2.png){fig-align="center"}


## The working directory

-   When referring to files in your R script, it is highly recommended that you use **relative paths**.
-   **Reason**: paths are unique to your computer, so if someone else runs your code on their computer, it will not find files whose location is described by absolute paths.
-   To determine the working directory of your current R session, you can type:
    ```{r}
    getwd()
    ```

-   To change the working directory, use the function `setwd`:
    ```{r}
    #| eval: false
    
    setwd("/path/to/your/directory")
    ```

    In RStudio, you can alternatively also select the working directory via `Session > Set Working Directory`.


## Generating path names

-   Different operating systems have different conventions when specifying paths.
-   For example, Linux and Mac use forward slashes `/`, while Windows uses backslashes `\` to separate directories.
-   The R function `file.path` combines characters to form a complete path, automatically ensuring compatibility with the respective operating system.
-   This function is useful because we often want to define paths using a variable.
-   Consider the following example: 

    ```{r}
    dir <- system.file(package = "dslabs")
    file.path(dir, "extdata", "murders.csv")
    ```

    Here the variable `dir` contains the full path for the `dslabs` package (needs to be installed!) and `extdata/murders.csv` is the relative path of a specific `csv` file in that folder.

## Importing data from text files

-   All of us know **text files**. They are easy to open, can be easily read by humans and are easily transferable.
-   When text files are used to store **tabular data**, line breaks are used to separate rows and a predefined character (the so-called **delimiter**) is used to separate columns within a row. Which one is used can depend on the file format:
    -   `.csv` (comma-separated values) typically uses comma (`,`) or semicolon (`;`).
    -   `.tsv` (tab-separated values) typically uses tab (which can be a preset number of spaces or `\t`).
    -   `.txt` ("text") can use any of the above or a simple space ( )
-   How we read text files into R depends on the delimiter used. Therefore, we need to have a look at the file to determine the delimiter. 

## Importing data from text files

This is the first couple of lines of the `murders.csv` text file from the `dslabs` package we saw referenced before. It contains the number of gun murders in each US state in the year 2010 as well as each state's population. Clearly, it uses commas (`,`) as delimiter. Also note the use of a **header** in the first row.

![](img/csv_file.png){fig-align="center"}

## Importing data from text files -- `.csv`

-   For **comma-delimited** `csv` files, R offers the function `read.csv` to run on the (full or relative) path of the file. By default, it assumes that **decimal points** are used and that a **header** giving column names is present:
    ```{r}
    args(read.csv)
    ```

-   For **semicolon-delimited** `csv` files, R offers the function `read.csv2` to run on the (full or relative) path of the file. By default, it assumes that **decimal commas** are used and that a **header** giving column names is present.
    ```{r}
    args(read.csv2)
    ```

-   Both of these functions return a `data.frame` containing the data from the file.

## Importing data from text files -- `.csv`

As `murders.csv` is comma-delimited, we use `read.csv` to read it into R:

```{r}
dir <- system.file(package = "dslabs")
murders_df <- read.csv(file.path(dir, "extdata", "murders.csv"))
head(murders_df, 6) # shows the first 6 rows of the data frame
```

. . .

Note that the categorical variables (`state`, `abb` and `region`) are imported as `character` vectors:
```{r}
unlist(lapply(murders_df, typeof))
```

For data analysis purposes, we should probably turn these into factors. But for now, we are only interested in the successful import.

## Importing data from text files -- `.tsv`

-   For **tab-delimited** `tsv` files, R offers the functions `read.delim` and `read.delim2`, again assuming the use of **decimal points** and **decimal commas**, respectively:
    ```{r}
    args(read.delim)
    args(read.delim2)
    ```

-   Otherwise, the use is identical to `read.csv`: the function requires the path to the file you want to import and returns a `data.frame` containing the (hopefully) correctly parsed data from the file.

## Importing data from text files -- `.tsv`

![](img/tsv_file.png){.absolute top="200" left="10" width="40%"}

![](img/arrow_delim.png){.absolute top="400" left="560" width="17.5%"}

![](img/read_delim.png){.absolute top="200" left="800" width="40%"}

## Importing data from text files -- `.txt`

-   In fact, all of the functions for importing data discussed so far are just interfaces to the R function `read.table`, which provides the most flexibility when importing data from text files:
    ```{r}
    args(read.table)
    ```

    (As always, to see the meaning of all of these arguments, see `?read.table`)
-   This function is mostly used directly, when importing data from generic `.txt` files, where the format is often less strictly adhered to than in `csv` or `tsv` files.

## Encoding

Now, we know how to import data into R. So we download some data set, read it into R using the correct function, but then this happens...

![](img/encoding_issue.png){.absolute top="300" left="10" width="40%"}

![](img/arrow_csv.png){.absolute top="400" left="560" width="17.5%"}

![](img/encoding_issue2.png){.absolute top="275" left="800" width="40%"}

## Encoding

-   Such issues occur because of an incorrectly identified **file encoding**.
-   Encoding refers to how the computer stores character strings as binary 0s and 1s. Examples of encoding systems are:
    -   **ASCII**: uses 7 bits to represent symbols, enough for all English keyboard characters, but not much more...
    -   **Unicode** (especially UTF-8): the de-facto standard encoding of the internet, able to represent everything from the English alphabet to German Umlaute to Chinese characters and emojis.
-   When reading a text file into R, its encoding needs to be known, otherwise the import either fails (see previous slide) or produces gibberish (e.g. German "H√∂he" $\to$ "H√É¬∂he").

## Encoding

-   RStudio typically uses **UTF-8** as its default, which works in most cases. If it does not, you can use the `guess_encoding` function of the `readr` package to get insight into the encoding.
    ```{r}
    library(readr)
    weird_filepath <- file.path("../data", "calificaciones.csv")
    as.data.frame(guess_encoding(weird_filepath))
    ```

-   The function deems the `ISO-8859-1` encoding to be the most likely encoding of the previous file. So we pass this value as the `fileEncoding` argument to `read.csv2`:
    ```{r}
    head(read.csv2(weird_filepath, sep = ",", fileEncoding = "ISO-8859-1"), 3)
    ```

-   This time, it worked! üòä


## Importing data from spreadsheets

-   Another common way of sharing tabular data is through the use of **spreadsheets**, like Excel or Google Sheets. We will see how to import data in both of these types of documents.
-   With Excel, spreadsheets typically either have a `.xls` or `.xlsx` file suffix. Note that those are **binary** file formats, i.e. unlike text files, they are not human-readable when opened with a text editor.
-   Base R does *not* have functionality to import data from Excel spreadsheets. However, the package `readxl` does. Its two main functions are:
    -   `read_xls` to read Excel spreadsheets with `.xls` ending and
    -   `read_xlsx` to read Excel spreadsheets with `.xlsx` ending.
-   These functions allow to select only certain areas of certain sheets, transform data types and much more...


## Importing data from spreadsheets

Consider the following simple example. Suppose we have the following `.xlsx`-spreadsheet of famous people that died in 2016:

![](img/spreadsheets_deaths.png){fig-align="center"}


## Importing data from spreadsheets

-   Note how we only want to import the range `A5:F15` in the sheet called `other`. We can pass these values as the corresponding arguments to the `read_xlsx` function:
    ```{r}
    library(readxl)
    args(read_xlsx)
    ```

-   Hence, to import this data, we should call:
    ```{r}
    dir <- system.file(package = "readxl")
    xlsx_filepath <- file.path(dir, "extdata", "deaths.xlsx")
    head(as.data.frame(read_xlsx(xlsx_filepath, sheet = "other", range = "A5:F15")), 5)
    ```


## Importing data from spreadsheets

-   **Google Sheets** is another widely used spreadsheet program, which is free and web-based. Just like with Excel, in Google Sheets data are organized in worksheets (also called sheets) inside of spreadsheet files.
-   Again, Base R does *not* have functionality to import data from Google Sheets spreadsheets. However, the package `googlesheets4` does. Its main function is `read_sheet`, which reads a Google Sheet from a URL or a file id.
-   Given such a URL, its use is very similar to `read_xlsx`:
    ```{r}
    #| cache: true
    
    library(googlesheets4)
    
    gs4_deauth() # used to read publicly available Google Sheets
    # Obtaining data on global life expectancy since 1800 from the Gapminder project
    # For more information, see http://gapm.io/dlex
    sheet_id <- "1RehxZjXd7_rG8v2pJYV6aY0J3LAsgUPDQnbY4dRdiSs"
    
    df_gs <- as.data.frame(read_sheet(sheet_id,
                                      sheet = "data-for-world-by-year",
                                      range = "A1:D302"))
    ```


## Importing data from spreadsheets

Now, the data has successfully been imported:

```{r}
library(knitr)
kable(head(df_gs, 8)) # the function kable creates nice tables for presentations
```


## Importing data in other formats

-   There is **a lot** of other data formats, which can be read into R with the help of other **packages**. The following provides a brief and inevitably incomplete overview:
    -   Package `haven` for data from SPSS, Stata or SAS.
    -   Package `DBI` along with a DBMS-specific backend allows you to run SQL queries on a data base and obtain the result as a `data.frame` directly in R.
    -   Package `jsonline` for importing JSON files.
    -   Package `xml2` for importing XML files.
    -   ...


# Tidy and Transform

## The Data Science workflow -- Tidy and Transform

![](img/data-science-cycle.003.png){fig-align="center"}

## Tidy and Transform -- Data wrangling

-   After we imported data into R, we want to make it **easily processable** for visualizations or model building. This process could involve:
    -   renaming columns to avoid confusion and unnecessary typing.
    -   subsetting the data to use only parts of it, i.e. filtering.
    -   handling incorrect and/or missing values.
    -   aggregating the data to compute summary statistics.
    -   reshaping the data to suit the needs of functions operating on them.
    -   adding or replacing columns.
    -   joining other data sets to enrich the information presented.
    -   and much more...
-   Jointly, we refer to these tidying and transformation tasks as **data wrangling**.

## Tidy and Transform -- Data wrangling

![](img/data_wrangling.png){fig-align="center"}

## Example data set

Let's import some data that we will be using as an example:

```{r}
flights <- read.csv(file.path("../data", "nyc13_flights.csv"))
head(flights)
```

This data was adapted from the [nycflights13](https://cran.r-project.org/web/packages/nycflights13/index.html) package. It contains information on all 166,158 domestic flights that departed from New York City (airports EWR, JFK and LGA) in the first six months of 2013. The data itself originates from the [US Bureau of Transportation Statistics](https://www.transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGJ&QO_fu146_anzr=b0-gvzr).

## Column naming

The columns of this data set have quite long and descriptive names. Column names generally should:

-   be as short as possible.
-   be as long as necessary to be descriptive enough.
-   not use spaces or special characters.
-   only contain lowercase letters.
-   use **snake_case** for multiple words.

These rules are good to keep in mind also when naming any other R object. Let's see what kind of information is contained in our example data set and how we could name the corresponding columns appropriately.

## Column naming

::: slightlysmall
Our example data set contains the following columns:
:::

::: small
-   `year`, `month`, `day`: date of departure
-   `actual.time.of.departure`, `scheduled.time.of.departure`: actual and scheduled time of departure (in format HHMM or HMM). Too long, how about `dep_time` and `sched_dep_time`?
-   `actual.time.of.arrival`, `scheduled.time.of.arrival`: actual and scheduled time of arrival (in format HHMM or HMM). Too long, how about `arr_time` and `sched_arr_time`?
-   `departure.delay`, `arrival.delay`: departure and arrival delays, in minutes. To be consistent, how about `dep_delay` and `arr_delay`?
-   `carrier`, `flight`: airline and flight number.
-   `plane.tail.number`: plane tail number. Too long, how about `tailnum`?
-   `origin`, `destination`: origin and destination airport. `origin` fine, but maybe `dest`?
-   `air.time`: amount of time spent in the air, in minutes. Change to `air_time`?
-   `distance`: distance between airports, in miles.
-   `time.hour`: scheduled date and hour of the flight. Change to `time_hour`?
:::

## Column naming

To reset the column names of a `data.frame`, we can either use `colnames` or `names`:
```{r}
head(names(flights))
names(flights) <- c("year", "month", "day", "dep_time", "sched_dep_time", "dep_delay",
                    "arr_time", "sched_arr_time", "arr_delay", "carrier", "flight",
                    "tail_num", "origin", "dest", "air_time", "distance", "time_hour")
head(flights)
```

## Checking data types

Next, we should make sure that the data type and/or class used for each variable suits the data presented in this variable. In particular:

-   Each numerical variable should of type `integer` or `double`.
-   Each categorical variable should be a `factor`.
-   Each non-categorical text-based variable should be of type `character`.
-   Each date / date-time variable should be a `Date` or a `POSIXct`, respectively.

Let's have a look in our data set:
```{r}
sapply(flights, typeof)
```

## Adapting data types

Most of the data types in our data set seem appropriate, however we should:

-   redefine `carrier`, `tail_num`, `origin` and `dest` to be a `factor`.
-   redefine `time_hour` to be a `POSIXct` date-time.

Using the function `factor`, the first part should be no problem:
```{r}
factor_vars <- c("carrier", "tail_num", "origin", "dest")
for(var in factor_vars){
  flights[[var]] <- factor(flights[[var]])
  print(head(flights[[var]]))
}
```

## Date-times

-   In the previous lecture, we have talked about **dates** in R, but we have not talked about how to represent **time**.
-   As simple as time seem to be, representing time-related information with a computer can be incredibly complex. Think about:
    -   time zones (changing in geographical composition over time),
    -   daylight saving time (DST) and its relevance in different countries,
    -   different formats for writing dates and times (e.g. 16:00 vs. 4:00 pm),
    -   ...
-   In this course, we will fortunately stick to relatively simple cases. However, data in the real world does not always behave that nicely...

## Date-times

![](img/timezones.png){fig-align="center"}

This [map](https://en.wikipedia.org/wiki/Tz_database#/media/File:Timezone-boundary-builder_release_2023d.png) partitions the world into regions where local clocks all show the same time and have done so since 1970. Talk about complexity...

## Date-time in R -- POSIXct

One way of representing date-time information in R is with the `POSIXct` class. Just as with dates, we can use **format strings** also to identify hour, minute, second, time zone, etc.

In our `flights` data set, the variable `time_hour` has a relatively simple format:
```{r}
head(flights$time_hour)
```

. . .

Additionally, we know that these are all times from the NYC time zone. In R, we can make use of a data base of time zones with the help of the function `OlsonNames`, named after the original creator Arthur David Olson. In there, there is a time zone called **America/New_York**:
```{r}
OlsonNames()[170]
```

## Date-time in R -- POSIXct

Now, we can use the appropriate format string and time zone name to turn the `time_hour` variable into a `POSIXct` date-time. For this purpose, we require the function `as.POSIXct`:
```{r}
flights$time_hour <- as.POSIXct(flights$time_hour,
                                tz = "America/New_York",
                                format = "%Y-%m-%d %H:%M:%S")
typeof(flights$time_hour)
class(flights$time_hour)
head(flights$time_hour, 2)
tail(flights$time_hour, 2)
```

Note how these `POSIXct` date-times now have `EST` (Eastern Standard Time) or `EDT` (Eastern Daylight Time) on them to indicate the time zone. `as.POSIXct` automatically applied the daylight saving time for the correct period.

## Filtering

Now that we have the correct data types, we might want to **filter** our `data.frame`. **Filtering** refers to the process of keeping rows based on certain conditions imposed on the values of the columns. For example, we might want to find all flights on 14th February that were more than one hour delayed upon arrival at their destination.

. . . 

We can filter a `data.frame` by **logical subsetting** of the rows. For this, we have to combine the conditions we want to impose by using the logical operators, `&` (**and**), `|` (**or**) and `!` (**not**). If we want to find the carriers and flight numbers of the aforementioned flights, we could do:

```{r}
head(flights[flights$month == 2 & flights$day == 14 & flights$arr_delay > 60,
             c("year", "month", "day", "carrier", "flight", "arr_delay")])
```

## Filtering

Let's see another example: suppose we want to find all flights that left from either Newark (EWR) or JFK to Los Angeles (LAX) on 14th February after 6:00 pm. We could do:

```{r}
flights[flights$month == 2 & 
          flights$day == 14 & 
          (flights$origin == "EWR" | flights$origin == "JFK") & 
          flights$dest == "LAX" & 
          flights$dep_time > 1800,
        c("year", "month", "day", "carrier", "flight", "origin", "dest", "dep_time")]
```

. . .

Note that -- since there are only three possible `origin` airports in this data set -- we could replace the third condition with

```{r}
#| eval: false

flights$origin != "LGA"
```


## Filtering

That starts to be quite a lot of typing... To reduce the number of times that we have to type the name of the `data.frame`, we can use the `subset` function:

```{r}
subset(flights,
       month == 2 & day == 14 & origin != "LGA" & dest == "LAX" & dep_time > 1800,
       c(year, month, day, carrier, flight, origin, dest, dep_time))
```

This is much more clear and compact. Note that when using `subset`, the names of the columns we want to select do not have to be specified with quotation marks `""`.


## Handling missing values

Let's see where in our data set we have **missing values** (indicated by `NA`):

```{r}
n_missing <- sapply(flights, function(x) sum(is.na(x)))
n_missing[n_missing > 0]
```

. . .

Only six variables seem to have missing values. Note how in this data set, the missing values can be **interpreted**:

-   `dep_time` and `dep_delay` $\to$ flight was **cancelled**. In these cases, `arr_time` and `arr_delay` are also `NA`.
-   Additional missing values in `arr_time` $\to$ flight was **diverted** to another destination airport.
-   Additional missing values in `arr_delay` and `air_time`. Unknown reason for missingness, hard to construct without additional information.
-   In some cases, the `tail_num` of the plane seems to be simply unknown.


## Handling missing values

Let's start handling the cancelled flights first. Depending on the circumstances, we might want to add a variable to indicate a cancelled flight, like so...

```{r}
flights$cancelled <- is.na(flights$dep_time)
head(flights[, c("year", "month", "day", "dep_time", "arr_time",
                 "carrier", "flight", "cancelled")], 5)
```

. . .

... or remove cancelled flights from the data set all together and put them into their own `data.frame`. Let's go for that option:

```{r}
cancelled_flights <- flights[is.na(flights$dep_time), ]
flights <- flights[!is.na(flights$dep_time), ]
head(cancelled_flights[, c("year", "month", "day", "dep_time", "arr_time",
                           "carrier", "flight", "cancelled")], 5)
```


## Handling missing values

Next, let's do the same also with diverted flights, which will contain all remaining flights that have `NA`s in the `arr_time` variable:

```{r}
diverted_flights <- flights[is.na(flights$arr_time), ]
flights <- flights[!is.na(flights$arr_time), ]
head(diverted_flights[, c("year", "month", "day", "dep_time", "arr_time",
                          "carrier", "flight", "origin", "dest")], 5)
```

. . .

Let's look at how many missing values are remaining after separating out cancelled and diverted flights from our data set:

```{r}
n_missing <- sapply(flights, function(x) sum(is.na(x)))
n_missing[n_missing > 0]
```

Only a small number of `NA`s are remaining in the variables `arr_delay` and `air_time`. We will deal with them when we need to...


## Descriptive statistics -- Numeric variables

Now, we are finally ready to compute some descriptive statistics. Say, we want to know the average departure and arrival delay of a (not cancelled or diverted) flight. We use the function `mean`:
```{r}
mean(flights$dep_delay)
mean(flights$arr_delay)
```

The average departure delay is around 13.6 minutes, but the average arrival delay is `NA`? Why is that?

. . .

Of course, that's exactly because of the remaining missing values in `arr_delay`. R cannot know the average of a vector of values where some values are unknown. However, most functions for descriptive statistics have an argument called `na.rm`:
```{r}
mean(flights$arr_delay, na.rm = TRUE)
```


## Descriptive statistics -- Numeric variables

Let's say we wanted to know what the maximum departure and arrival delays were. In that case, we would use the function `max` and also set its `na.rm` option to `TRUE` (strictly necessary only for `arr_delay`):
```{r}
max(flights$dep_delay, na.rm = TRUE) / 60 # divide by 60 to get hours from minutes
max(flights$arr_delay, na.rm = TRUE) / 60 # divide by 60 to get hours from minutes
```

So both maximum departure and arrival delays were over 21 hours!

. . .

Other functions for important univariate descriptive statistics of numeric variables are:

-   `range` for both `min` and `max` in one go.
-   `median` and `quantile` for quantiles.
-   `var` and `sd` for variance and standard deviation.
-   `fivenum` and `summary` for five-point summaries.


## Descriptive statistics -- Categorical variables

From categorical variables, we very often want to compute a **frequency table**. This can be achieved with the R function `table`. Let's say we want to know how many flights started from each of the three NYC airports:

```{r}
# Absolute frequencies:
table(flights$origin)
# Relative frequencies:
prop.table(table(flights$origin))
```

. . .

Or we want to know the distribution of carriers operating out of LaGuardia in %:

```{r}
round(prop.table(table(flights$carrier[flights$origin == "LGA"]))*100, 2)
```


## Descriptive statistics -- Grouping

A very common action in data wrangling is computing statistics of a variable for each level of a categorical variable. In our `flights` data, we might be interested for example in:

-   the average departure delay for each carrier,
-   the median arrival delay in each month broken down by NYC airport,
-   the fastest air time for each route,
-   ...

Such actions require us to **group** the data by the factor levels of the categorical variable and then compute statistics on the variable of interest for each of the resulting groups.


## Descriptive statistics -- Grouping

In R, this kind of action can be achieved with the functions `tapply` and `aggregate`. Let's first look at how `tapply` works for the three examples given on the previous slide:

```{r}
# Average departure delay by carrier:
tapply(flights$dep_delay, flights$carrier, mean)
# Median arrival delay for each month and airport
tapply(flights$arr_delay, list(flights$origin, flights$month), median, na.rm = TRUE)
# Fastest air time on each route
head(tapply(flights$air_time, paste0(flights$origin, "_", flights$dest), min, na.rm = TRUE))
```


## Descriptive statistics -- Grouping

So `tapply` generally works like this:

```{r}
#| eval: false

tapply(variable_of_interest, list_of_grouping_variables, aggregation_function, ...)
```

. . .

By contrast, the function `aggregate` works like this:

```{r}
#| eval: false

aggregate(df_of_interest, list_of_grouping_variables, aggregation_function, ...)
```

`aggregate` then applies the `aggregation_function` to each variable in the `df_of_interest` by group.

. . .

So, we can compute both average departure and arrival delays by carrier in one go, for example:

```{r}
head(aggregate(flights[,c("dep_delay", "arr_delay")],
               list(flights$carrier),
               mean, na.rm = TRUE))
```


## Descriptive statistics -- Grouping with `cut`

-   A frequently encountered goal is to group not based on an existing categorical variable, but on different **intervals** of a numeric variable.
-   For example, we might be interested in analyzing delay patterns based on **air time**: short-haul ($\leq$ 3 hours), medium-haul (3-6 hours) and long-haul (6-16 hours) (according to the [IATA](https://en.wikipedia.org/wiki/Flight_length#Time-based_definitions)).
-   To group by these flight length categories, we have to **cut** our `air_time` variable at these cut points. For this, we can use the function `cut`.
-   Besides specifying the cut points, `cut` offers us also to **label** the levels of the resulting `factor`.


## Descriptive statistics -- Grouping with `cut`

So to add this variable to our `data.frame`, we might do:

```{r}
flights$length_cat <- cut(flights$air_time, c(0, 180, 360, Inf),
                          labels = c("short-haul", "medium-haul", "long-haul"))

head(flights[, c("year", "month", "day", "dep_time", "arr_time",
                 "origin", "dest", "air_time", "length_cat")])
```

Now, we can analyse average departure and arrival delays by these categories using `aggregate`, for instance:

```{r}
aggregate(flights[,c("dep_delay", "arr_delay")],
          list(flights$length_cat), mean, na.rm = TRUE)
```


## Joining tables

Our `flights` data set only contains codes for carrier, air plane and airports. To properly understand this data, we need to be able to **enrich** this data set by more information, such as:

-   Full name of the carrier
-   Full name and location of the origin and destination airports
-   Type, model and size of the aircraft used
-   Weather information at the origin airport at the time of departure

Indeed, any moderately complex data science project will involve **multiple tables** that must be **joined** together in order to answer the questions that you are interested in.


## Joining tables

The [nycflights13](https://cran.r-project.org/web/packages/nycflights13/index.html) package contains four additional tables that can be joined into the `flights` table. The below graph illustrates their relation:

![](img/nycflights13_data.png){fig-align="center"}

## Joining tables

Therefore, in the `flights` data set, 

-   the variables `origin` and `dest` are foreign keys that correspond to the primary key `faa` in `airports`.
-   the variable `tail_num` is a foreign key that corresponds to the primary key `tail_num` in `planes`.
-   the variable `carrier` is a foreign key that corresponds to the primary key `carrier` in `airlines`.
-   the variables `origin` and `time_hour` constitute a **compound** foreign key that corresponds to the compound primary key constituted by `origin` and `time_hour` in `weather`.

To illustrate how we can **join** information from these tables into `flights` using keys, we start with the easiest example of `airlines`.

## Joining tables

For this, we have to read in the `airlines` table and inspect it:

```{r}
airlines <- read.csv(file.path("../data", "nyc13_airlines.csv"))
airlines
```

This is a very simple and small data set with the carrier code and name of 16 American airline companies. Now, how do we join this information into the `flights` data set?


## Joining tables -- Types of joins

-   There are many different **types of joins** that determine how two (or more) tables are brought together. We will illustrate only the most important ones with a toy example.
-   Say, we have two tables, `x` and `y`, each with a `key` column and a column containing some values:

    ![](img/joins_setup.png){fig-align="center" width="25%"}
    
    The colored key columns map background color to key value. The grey columns represent "value" columns. 
    
## Joining tables -- Inner join

-   In an **inner join**, we want to keep only the rows that have information in both tables.
-   In the example, this is only the case for data points with `key` 1 and 2. Therefore, the rows with `key` 3 and 4 do not make it to the joined table, if we join `x` and `y` on an inner join:

    ![](img/joins_inner.png){fig-align="center" width="50%"}
    
## Joining tables -- Left outer join

-   In a **left outer join**, we want to keep all rows in the **left table** (in this case `x`). Rows without a matching `key` in `y` receive `NA` for the value column of `y`:

    ![](img/joins_left.png){fig-align="center" width="50%"}
    
-   For simplicity, such joins are usually simply called **left joins**.

## Joining tables -- Right outer join

-   In a **right outer join**, we want to keep all rows in the **right table** (in this case `y`). Rows without a matching `key` in `x` receive `NA` for the value column of `x`:

    ![](img/joins_right.png){fig-align="center" width="50%"}
    
-   For simplicity, such joins are usually simply called **right joins**.

## Joining tables -- Full outer join

-   In a **full outer join**, we want to keep all the rows across both tables and fill the missing values with `NA`s:

    ![](img/joins_full.png){fig-align="center" width="50%"}
    
-   For simplicity, such joins are usually simply called **full joins**.

## Performing joins with `merge`

In R, joining happens with the help of the function `merge`, whose arguments can be quite confusing at times... Here is the breakdown of the most important ones:

-   `x` and `y` are the left and right `data.frame` to join.
-   `by` is the name of the key(s) used for joining, if the column name of this key is the same in both `x` and `y`. If not, we specify the column names in `x` via the `by.x` argument and in `y` via the `by.y` argument.
-   The arguments `all`, `all.x` and `all.y` specify the type of join:
    -   Set `all` to `FALSE` for an **inner join**.
    -   Set `all.x` to `TRUE` for a **left join**.
    -   Set `all.y` to `TRUE` for a **right join**.
    -   Set `all` to `TRUE` for a **full outer join**.


## Performing joins with `merge`

So let's finally see joins in action! We **left join** the `airlines` data set into `flights` on the key of `carrier`. So, to do this in R, we run:

```{r}
flights <- merge(flights, airlines, by = "carrier", all.x = TRUE)
head(flights[, c("year", "month", "day", "dep_time", "origin", "dest", "carrier", "name")])
```

Note two things about this successful join:

-   First, in the `airlines` data set, we found a match for every carrier in `flights`, so the left join did not produce any `NA`s. Proof:
    ```{r}
    sum(is.na(flights$name))
    ```

-   Second, `merge` changed the order of the rows of the `data.frame`.


## Tidy data

-   To finish this chapter, let's talk about **tidy data**.
-   Rather than just meaning "clean", tidy data actually refers to a specific way of organizing your data that is beneficial for the types of actions we want to perform on them. There are three interrelated rules that make data tidy:
    -   Each variable is a column; each column is a variable.
    -   Each observation is a row; each row is an observation.
    -   Each value is a cell; each cell is a single value.
-   These rules might seem pretty obvious, but actually, most real-world data sets do not meet these requirements as data is often organized to facilitate some goal other than analysis.


## Tidy data

-   To illustrate this point, we will have a look a new data set. It contains the fertility rates in Germany and South Korea in the years 1960 to 2015:

    ```{r}
    fertility <- read.csv(file.path("../data", "fertility.csv"))
    fertility[, 1:12]
    dim(fertility)
    ```

-   This data is **untidy**. Why?
    -   It contains a variable (namely the year) in **column names**, but according to the principles of tidy data, each variable should be its own column.
    -   The observations are fertility rates in two countries, so these values should be organized in **rows**.


## Tidy data

Due to its shape, such data is said to be in a **wide format** (few rows, lots of columns). We want to reshape it to **long format** (lots of rows, few columns). For this purpose, we use the R function `reshape`:

```{r}
fertility_long <- reshape(fertility, direction = "long",
                          varying = list(names(fertility)[-1]), v.names = "fertility_rate",
                          idvar = "country",
                          timevar = "year", times = 1960:2015)
rownames(fertility_long) <- NULL
head(fertility_long, 8)
```

Now, the data is **tidy**! Note that it is exactly the same underlying data, just represented in a slightly different way.


## Tidy data

The function `reshape` takes some practice to get used to. But once we know how to get our data **tidy**, there are multiple advantages to using this consistent way of organizing your data:

-   As we will see, many functions for data analysis in R cannot be used, unless the data is tidy. This applies in particular to the visualizations we will create with the `ggplot2` package.
-   Consistent data structures are easier to work with. If every data set "looks and feels" the same, you can build routines that will make your analyses more efficient and effective.
-   Having variables consistently in columns is particularly sensible in R due to its **vectorized nature**. R performs at its best when it is able to run functions on vectors of values. With a tidy data format, we can fully leverage this potential.

